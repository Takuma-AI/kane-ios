# The ChatGPT-5 Organizational Playbook

**Channel**: AI News & Strategy Daily | Nate B Jones  
**Date**: 2025-08-11  
**URL**: https://youtube.com/watch?v=dUWxN0snnW8  
**Duration**: 0:22:55  
**Views**: 8,919  
**Transcribed**: 2025-08-24  
**Source**: youtube_captions

---

## Transcript

If you work in AI transformation, if you're trying to figure out how to get AI into your business and how to get your team to use it, how to pick the right tool for the right job so you can make the most of AI and really drive the bottom line at GBT has to change your approach and it changes it in really unexpected ways. And I want to take this briefing and talk that through in detail and give you field notes that you can take to your teams to guide how you shift implementation of AI now that chat GPT5 is in your workplace. And I've got news for you. If you're a co-pilot organization, if you're a cloud organization, it is very likely that chat GPT5 is already in your workplace because people bring it in on their phones. The shadow IT problem is real. You have to assume it's already there. So, what makes Chad GPT5 special and different? Why is it worth an executive briefing just to talk through what changes in your org as a result? Number one, the way this model works is unlike any other model. This is a bunch of models bundled together, which means that your team has to learn a brand new skill. So before when chat GPT40 was out there, it was really about your team move to a reasoning model and specifically invoke it at the right time. Go to 03, right? Or if you were uh using chat GPT40 and it was the old days, ask it to think step by step. None of that really works in the same way anymore. Now, you're going to have to actually work with your team and help them figure out how to route the model into the right model category behind the scenes so that you can get the power you need for the job you want. And this matters. When I did my full write up on chat GPT5 this week, I found that chat GPT5 is both the best and the worst model performing in the workup in the test that I did. In other words, depending on how it's prompted and which model you route to, you either get a very bad response to a complex problem or an extraordinarily good one. Your team needs to double down on taste. They need to double down on understanding what constitutes a good answer to a very hard question if you're going to use it for complex work. And I think that the answer with AI is that you have to try to use it for complex work. I don't think it's acceptable as an AI transformation organization to look at a launch like chat GPT5 and say, "Ah, we're going to wait. We're going to see what chat GPT holds. We're not going to assume this is too hard. I've got news. It's not going to get easier. We're not going back to a world where you just pick the model. Your team has to level up in the way they prompt in order to take advantage of this model. Your cheat sheet, by the way, if you have one thing that you tell your team to make sure that they hear, tell them that when you have a hard problem, when the model needs to do some really in-depth thought, literally tell the model to think hard. It's it's one of those hard-coded passwords that seems to tell chat GPT reliably to invoke the thinking model. Tell them to think hard. But that's not the only tip. At the end of the day, what your teams need to succeed with chat GPT5 is they need to recognize that the leverage has shifted from picking the right model to picking the way you work with the model. And so you need to look across your teams and I've spent a lot of time in these executive briefings highlighting use cases for AI on teams. I don't want to belabor that here. There are use cases in marketing around idea generation. There's use cases in sales around how you handle really consistent language, how you handle deals, how you translate technical requirements into contracts. There's use cases in product around developing PRDs that are effective around vibe coding prototypes. So engineers can understand what you want and there's use cases in engineering all over the place around building more efficiently using coding tools. Those are just a few examples. Customer success has like voice of customer and ticket analysis. I could go on and on. The key thing you need to understand leading AI transformation is that for those use cases you have to help people see that the envelope of capability has gone up with chat GPT5. But the way you access it is trickier now. And so as an example, looking at the customer success use case, looking at the number of tickets you can assess and the patterns that you can make out of those tickets, if you invoke thinking mode, if you set up your prompt correctly, if you feed it all the tickets, it is going to do a better job of pattern recognizing, a better job of assessing overall what's in the box on those tickets than other models. And that includes cloud models. I I threw that kind of a problem at claude code did not do as good a job as Chad GPT5 and thinking and so I feel very confident saying that that overall capacity envelope has gone up handling and synthesizing really complex data including numeric data and including mixed data has gone way up and I think it's slept on because the business has a lot of that every business I I know has really messy data and chat GPT5 gives you the first really capable approach to tackling that but only if you can persuade people to very very carefully load that context window with the right prompt and the right data. And so I would encourage folks if you're working on like how can I unlock this extra capacity, get this extra pattern synthesis, maybe it's a market analysis, maybe it's a customer sentiment analysis, maybe it's looking across a lot of our behavioral data for product, whatever it is that like is that extra step of synthesis that was tough to do with straight AI before without having like a whole agentic pipeline or without building a rag system just like in the chat. My encouragement to you is to get get that data as clean as you can. Focus it on the data you need the AI to process in order to answer the question and then and that's okay. It can be a lot. This is a 400,000 token context window. It's okay that it's a lot. Put it in in a format the AI can fairly easily parse. I have tried it where you make it parse it and give it a nasty format in addition to giving it dirty data. And I will tell you it does it, but you're going to have much better results if you give it clean data in a format it understands. So take the time, get it into markdown, get it into CSV if you can, and then once you supply the data to the system, you want to very clearly specify the artifacts that will enable the AI to show it's done the work. And this is another distinctive of Chat GPT5 that I think is going to have to get wrapped into training curricula. You need to be at a point with your teams where they know the outputs that AI needs to write, build, demonstrate to show that it has done the work. In other words, with this model, with Chad GPT5, it does better if you force it to prove its work than if you tell it to do the work. So, in other words, when you're asking for the output, say, "Hey, give me the sentiment analysis. Give me the Python workbook to show how you did it. And then also give me a plain English summary of the rubric and the scoring assessment that you used for the sentiment analysis along with any personas that you developed. Something like that, right? Like basically, show me what you did. Sure, give me the executive summary and the report, but show me all the artifacts along the way as well and demand those as outputs. Why is that important? Well, go back to the original architecture of chat GPT5. It's important because this is a model that is like a skin stretched over a bunch of different machines in the background. You are basically specifying artifacts that trace to more tool calls in the background that get you more of what you want. And so when you specify the Python grader for instance, you're specifying tool use effectively around a particular kind of grading that you want done on this particular data set. When you do that across a range of artifacts, you are hard- coding or invoking specific tool calls that you can then ensure are used against the data set in the way you want. And so that's why proving it matters. That's why defining the artifact seems to especially matter with GPT5. That is going to be a big jump for teams that are used to just saying, you know what, produce this thing. And I think it's a good jump because in a sense, we're asking the AI to do more meaningful work. We're asking the AI to come back with more in-depth analysis that it really wasn't possible to do before. And so instead of thinking of AI as a text output generator, which so many teams do, we're asking AI to think more multimodally. We're asking AI to take advantage of the maths and the code that it's able to do and actually put that at the service of our teams, even if we're not in engineering. And that's why training teams to think in artifacts really helps. So that's that's the second key piece I want to call out. So we've talked a little bit here. There's there's more to come in this video. There's a lot to dive into, but I I want to just pause for a second and say as you think about this, remember this is multiple models. You need to trace the call to the right model. So make sure that you're asking the model to think hard and get to the right problem space. Make sure you give it clean data and make sure that you asking for artifacts along the way. I will also add as we sort of move forward in this discussion, it is really really important for you to be clear with your teams about the way you want certain problems addressed in GPT5. A lot of execs settle for AI to do this. I used AI to do this. That used to be kind of okay. It is now definitely not okay because the difference between bad usage of GPT5 and good usage of GPT5 is so large. You cannot just tell your team I use GPT5 for this anymore. You need to specify and say this is how I use this tool to get this result. The specificity of communication takes more work on your part. It takes more work on the part of anyone who's teaching AI at your company. But the trade-off is that if you do that work now, you are going to get more AI fluency around a tool that is even less obviously powerful than previous AI models. At least when 03 came out, it was obviously powerful. You were talking to the reasoning model all the time. Now it's GPT5. The reasoning model is one of several that's hiding back there. And you have to kind of feel for it in the dark of latent space. And why why you ask did Chad GPT do that? Because people were complaining very loudly about the fact that there were a bunch of models and we had to pick which model to use. Well, the trade-off is we don't have to pick the model anymore. But now we have to invoke the path through the model to the power that we want behind the model. And that's the trade-off. You get only one model. Super simple. Everyone's using GPT5. But now we have to talk more about how we invoke that power. There's no free lunch. That's how it works. Now, as we sort of round out this discussion and and start to think a little bit about the wider implications of GPT5 and where we're going over the next year or two and how AI transformation unfolds from here, we have been in an era characterized by model choice. We are not in that era anymore as of August. We are now in an era when the model choice has largely been made for us. And it is the model usage that is going to determine whether organizations survive or perish. Specifically, it's whether organizations are able to quickly understand how to get the most out of the model for specific use cases that tie to their data and their teams. You know, those brown bags and those socializing AI wins that you would sort of see happen and maybe they peter out after a month or two. Those really matter. Now you need to be in a place where you are rapidly socializing how to use chat GPT5 across your business. You need to be in a place where you are defining really explicitly these are use cases in the business that are new that we can now unlock because there's a larger context window because thinking mode gives us synthesis across messy data that we didn't have before. Great. Define them. Name them. You're now going to have to tell people how to prompt for them, how to prep the data for them. And if you can get it right, you're going to have something most other companies don't know how to do, especially if they're just telling people start using GPT5. At the same time, if you're using the non-reasoning version of GPT5 and it's very, very fast and it writes a little bit better, you're going to have to get more aggressive about working with people to give GPT5 non-reasoning for the simple textbased stuff, really good prompts that drive it to write in your style, that drive it to write with no hallucinations and factcheck its work, that drive it to make sure it is complete in the answer and not overpromising. Those are all things that I have seen in practice. Is it better at hallucinating than 03? Somewhat. Yeah. Is it going to benefit from you telling it explicitly what the bar for clarity, for adherence to reality, for adherence to facts, for explaining only the answer to your solution and not 16 other things is yes, it will benefit from that clarity. This is a model that I have compared to a product manager on crack. Helpfulness is off the chain. It comes back, it gives instructions, it gives overhelpful suggestions. It has been trained to be a completeness artist. Your teams will need to learn to rein it in. Your teams will need to learn to give it guard rails. So even if we're not talking about the really complex stuff and the data stuff and you're just talking about the simple non-reasoning model, your teams still need to learn to rein it in in ways that enable it to provide very useful content rapidly. Because the last thing you want is for teams to give up and walk away from it because then they don't get the value. or conversely for teams to use it and just copy paste from it and you're going to be able to tell because suddenly all of your meetings are going to look super complete with agenda items that no one pays attention to and no one does because they're all made up by AI. Be careful because this model makes up completeness that your organization may not actually have internally. Be aware. This model likes to pretend things are complete. That's part of why it's a good coding model. And that brings me to my last observation for teams and what is new with this model that you need to pay attention to as a leader. There is a new category of software that launched on August 7th. It is not it it got called vibe coding. It's not really vibe coding or it's not the same vibe coding that we've had for months. The vibe coding we've had for months is you go to lovable, you go to bolt, you go to replet and you type something in and it builds an app. It might have a backend and transactions and loginins. it's a real app or at least it's supposed to be an app and you wrestle with it and maybe eventually you launch it. This is a lower category of software not in the sense that it's less useful but in the sense that it's more casual. It is kitchen table software. It is software for personal usage and it was positioned for personal usage in the call. I've certainly been able to use it for personal usage, but I've also been able to use it for professional usage immediately and people are sleeping on that. As an example, you could ask chat gpg5, make me a gant chart for this really complicated like giant Excel spreadsheet. It is probably going to come back with an image of a gant chart that is not ex exactly what you want and you're going to swear and say this thing can't do gant charts. Have you tried it with code? Go to the model and say here's the data respond in code. Build a gant chart app that shows this in code. And I did that. I did that with the Apollo 13 mission. I built out a whole Gant chart in code. It could not do it visualizing it directly. In other words, think of code as a tool that your teams can use for project artifacts. Low low casual artifacts where you share the link and say, I built a chat GPT app for this. This is our Gant chart for the project. Right? I built a chat GPT chat GPT app for this. This is our project update for the week. That kind of thing is now software. And teams have no idea that that's there. They don't know it's there. No one taught them that in previous prompting and levelups and AI courses. And do you know why? Because that wasn't possible before. This is really the first time we've had reasonably good coding with a reasonably complete ability to represent an app. I tried some of the stuff I worked on in Claude, which is a good coding app. It could not do this natively. I'm not saying anything against Claude in the API. It's an extraordinary model for coding. But if you want native representation in the canvas, you don't want a development environment, you don't want anything else, you just want to like try it and see if it codes up a little app, JPT5 is the best thing I've seen. It is absolutely potentially transformative if you can tell your people very clearly that for small presentations, for small things to work on that represent data in interesting ways that are visual, that might be slightly interactive, you want to be able to use this app for a weekly business review. That's a classic one. You have data. You need to represent it. You need to be able to click around, look at the the metrics. You should be able to use an app for that. You should be able to tell Chad GPT5 to do it. Now, not everyone's going to do that. There's going to be a lot of people that say they want to stick to their existing templates. The advantage if you do get into the culture of building apps is that you really unlock groundswell innovation from your team. Your team will come up with ideas for apps you did not have. If you can bless it and if you can remind them that it's a good thing. If you can remind them you're supportive of this kind of kitchen table software and you want them to be able to use it to solve interesting problems. You will not imagine the 200 use cases across your business. You'll imagine three or four of them and you'll try them out and you'll let people know these are awesome little use cases. I tried one. Here it is. Like I tried a travel itinerary one. It's really fun. And I could see when I showed people that tiny travel itinerary app I made that their eyes lit up and they're like, "Oh, let me remix that. Let me try that." And Chad GPT makes that so easy. You can remix it like you're remixing music. You can go back in and say, I want it to be, you know, the travel itinerary is for a different place, right? It's going to be for when I go to the Grand Canyon and so I want you to re remake it. And it's really easy to do. The weekly business review, it's going to be for the sales or not for marketing. I want to remix that artifact. People need to get your blessing to use chat GPT in new ways because the assumption is often if I try it and if I fail, it's going to be bad. And this goes back to classics of change management, right? Like you need to bless people to fail so they can learn to succeed. Okay, wrapping all of this up, what have we learned here? Number one, rollouts for Chad GPT5 are going to be different from rollouts for anything else because of the way they've made it one model. And so when we think about it from that frame, certain implications fall out that are new and different based on previous AI rollouts for orgs. First, you have to tell people how to prompt and access the power behind the model. That's where I called out think hard. Second, it could tackle big, gnarly, data heavy problems in the chat the way it never could before. You have to be responsible for the data you put in, for making it clean. You have to be responsible for teaching people to prompt it well. And you have to be responsible for reminding people that it does that work best when you invoke those tools by demanding artifacts, by demanding proof of work, by demanding that it actually shows how it did the work. Not that you tell it how to, but that you demand the artifacts that shows that it did the work. And that's a that's a fine distinction, but it's important to emphasize because telling it how to, that doesn't matter. But saying, I want you to show me the greater you used. Well, that's actually helpful. Then moving on from sort of the data analysis piece, we talked a little bit about the importance of making sure that your team feels comfortable using AI in new ways that are unexpected because Chad GPT5 unlocks those, right? So we talked about this coding use case. It's an entirely new class of work. How can you help the team understand that? How can you socialize out these sort of coded artifacts that become essentially a new way of doing business around the office? We talked about the importance of making sure that teams feel comfortable using AI for non-reasoning tasks in Chad GPT5 and what that looks like because again non-reasoning chat GPT5 is very good. It's extremely fast. It's extremely coherent. But it's going to be up to you to convey these are the guardrails. This is the house style. This is what I want to do with hallucinations and how I handle it. This is when you can be creative and when you can't be creative because hallucinations and creativity relate. And so if it were me, I would go in Monday morning and I would look at the current AI transformation playbook you have and I would basically say let's assume that we can do 20% more with AI because of chat GPT5 and let's assume that a lot of the way we taught the org is going to have to change because the old ways of learning are gone. the old ways of like I hope you weren't still doing think step by step in your AI transformation playbook, but if you were, that's got to go. I hope you weren't doing too much of an emphasis on model selection, but that's going to have to go. Uh, I hope you were able to communicate that whatever you're teaching people is going to update as models come out because that's still true. You're going to have to update how people think about which tasks they select, how people think about how they share their work. I called that out. You're going to have to update how you think about prompt libraries and what they contain because prompt libraries might now contain not just prompts, but also the artifacts that came out of good prompts. Like maybe you're saying, I want to have a a customer sentiment analyzer and here's the prompt for it, but here's the Python autograder for it, too, and I want you to have that. Or, you know, the prompt library contains not just the prompt to build the app, but also an example of the app that you can remix. Right? There's there's more that's evolving here that we haven't seen before. Do I have it all figured out? I do not have it all figured out. I do have some strong convictions on where GPT5 is going with the organization and I wanted to share these early field notes with you so that you also get a sense of what you need to focus on as you roll out GPT5 to your orgs. Good luck. Let me know how it's going and uh I will continue to report from the field as I dig into GPT5 transformation.

---

## Timestamped Transcript

[0:00:00] If you work in AI transformation, if
[0:00:02] you're trying to figure out how to get
[0:00:04] AI into your business and how to get
[0:00:06] your team to use it, how to pick the
[0:00:09] right tool for the right job so you can
[0:00:11] make the most of AI and really drive the
[0:00:13] bottom line at GBT
[0:00:16] has to change your approach and it
[0:00:18] changes it in really unexpected ways.
[0:00:21] And I want to take this briefing and
[0:00:23] talk that through in detail and give you
[0:00:25] field notes that you can take to your
[0:00:28] teams to guide how you shift
[0:00:30] implementation of AI now that chat GPT5
[0:00:34] is in your workplace. And I've got news
[0:00:36] for you. If you're a co-pilot
[0:00:38] organization, if you're a cloud
[0:00:39] organization, it is very likely that
[0:00:41] chat GPT5 is already in your workplace
[0:00:44] because people bring it in on their
[0:00:46] phones. The shadow IT problem is real.
[0:00:49] You have to assume it's already there.
[0:00:51] So, what makes Chad GPT5 special and
[0:00:53] different? Why is it worth an executive
[0:00:56] briefing just to talk through what
[0:00:59] changes in your org as a result? Number
[0:01:02] one, the way this model works is unlike
[0:01:07] any other model. This is a bunch of
[0:01:10] models bundled together, which means
[0:01:13] that your team has to learn a brand new
[0:01:16] skill. So before when chat GPT40 was out
[0:01:20] there, it was really about your team
[0:01:22] move to a reasoning model and
[0:01:24] specifically invoke it at the right
[0:01:25] time. Go to 03, right? Or if you were uh
[0:01:28] using chat GPT40 and it was the old
[0:01:30] days, ask it to think step by step. None
[0:01:33] of that really works in the same way
[0:01:36] anymore. Now, you're going to have to
[0:01:39] actually work with your team and help
[0:01:40] them figure out how to route the model
[0:01:43] into the right model category behind the
[0:01:47] scenes so that you can get the power you
[0:01:51] need for the job you want. And this
[0:01:53] matters. When I did my full write up on
[0:01:56] chat GPT5 this week, I found that chat
[0:01:59] GPT5 is both the best and the worst
[0:02:02] model performing in the workup in the
[0:02:05] test that I did. In other words,
[0:02:07] depending on how it's prompted and which
[0:02:09] model you route to, you either get a
[0:02:11] very bad response to a complex problem
[0:02:14] or an extraordinarily good one. Your
[0:02:16] team needs to double down on taste. They
[0:02:18] need to double down on understanding
[0:02:21] what constitutes a good answer to a very
[0:02:23] hard question if you're going to use it
[0:02:25] for complex work. And I think that the
[0:02:27] answer with AI is that you have to try
[0:02:30] to use it for complex work. I don't
[0:02:32] think it's acceptable as an AI
[0:02:34] transformation organization to look at a
[0:02:37] launch like chat GPT5 and say, "Ah,
[0:02:40] we're going to wait. We're going to see
[0:02:41] what chat GPT holds. We're not going to
[0:02:44] assume this is too hard. I've got news.
[0:02:46] It's not going to get easier. We're not
[0:02:48] going back to a world where you just
[0:02:50] pick the model. Your team has to level
[0:02:52] up in the way they prompt in order to
[0:02:56] take advantage of this model. Your cheat
[0:02:58] sheet, by the way, if you have one thing
[0:03:00] that you tell your team to make sure
[0:03:03] that they hear, tell them that when you
[0:03:06] have a hard problem, when the model
[0:03:08] needs to do some really in-depth
[0:03:09] thought, literally tell the model to
[0:03:11] think hard. It's it's one of those
[0:03:13] hard-coded passwords that seems to tell
[0:03:16] chat GPT reliably to invoke the thinking
[0:03:20] model. Tell them to think hard. But
[0:03:23] that's not the only tip. At the end of
[0:03:25] the day, what your teams need to succeed
[0:03:28] with chat GPT5 is they need to recognize
[0:03:31] that the leverage has shifted from
[0:03:33] picking the right model to picking the
[0:03:36] way you work with the model. And so you
[0:03:38] need to look across your teams and I've
[0:03:40] spent a lot of time in these executive
[0:03:42] briefings highlighting use cases for AI
[0:03:45] on teams. I don't want to belabor that
[0:03:47] here. There are use cases in marketing
[0:03:50] around idea generation. There's use
[0:03:52] cases in sales around how you handle
[0:03:55] really consistent language, how you
[0:03:57] handle deals, how you translate
[0:03:59] technical requirements into contracts.
[0:04:01] There's use cases in product around
[0:04:03] developing PRDs that are effective
[0:04:05] around vibe coding prototypes. So
[0:04:07] engineers can understand what you want
[0:04:09] and there's use cases in engineering all
[0:04:11] over the place around building more
[0:04:12] efficiently using coding tools. Those
[0:04:15] are just a few examples. Customer
[0:04:16] success has like voice of customer and
[0:04:18] ticket analysis. I could go on and on.
[0:04:21] The key thing you need to understand
[0:04:23] leading AI transformation is that for
[0:04:26] those use cases you have to help people
[0:04:29] see that the envelope of capability has
[0:04:33] gone up with chat GPT5. But the way you
[0:04:37] access it is trickier now. And so as an
[0:04:39] example, looking at the customer success
[0:04:41] use case, looking at the number of
[0:04:44] tickets you can assess and the patterns
[0:04:46] that you can make out of those tickets,
[0:04:48] if you invoke thinking mode, if you set
[0:04:50] up your prompt correctly, if you feed it
[0:04:52] all the tickets, it is going to do a
[0:04:54] better job of pattern recognizing, a
[0:04:57] better job of assessing overall what's
[0:05:00] in the box on those tickets than other
[0:05:02] models. And that includes cloud models.
[0:05:04] I I threw that kind of a problem at
[0:05:06] claude code did not do as good a job as
[0:05:08] Chad GPT5 and thinking and so I feel
[0:05:10] very confident saying that that overall
[0:05:13] capacity envelope has gone up handling
[0:05:15] and synthesizing really complex data
[0:05:18] including numeric data and including
[0:05:20] mixed data has gone way up and I think
[0:05:23] it's slept on because the business has a
[0:05:25] lot of that every business I I know has
[0:05:27] really messy data and chat GPT5 gives
[0:05:29] you the first really capable approach to
[0:05:32] tackling that but only if you can
[0:05:35] persuade people to very very carefully
[0:05:38] load that context window with the right
[0:05:41] prompt and the right data. And so I
[0:05:44] would encourage folks if you're working
[0:05:46] on like how can I unlock this extra
[0:05:48] capacity, get this extra pattern
[0:05:50] synthesis, maybe it's a market analysis,
[0:05:52] maybe it's a customer sentiment
[0:05:53] analysis, maybe it's looking across a
[0:05:56] lot of our behavioral data for product,
[0:05:58] whatever it is that like is that extra
[0:06:00] step of synthesis that was tough to do
[0:06:02] with straight AI before without having
[0:06:04] like a whole agentic pipeline or without
[0:06:07] building a rag system just like in the
[0:06:09] chat. My encouragement to you is to get
[0:06:12] get that data as clean as you can. Focus
[0:06:14] it on the data you need the AI to
[0:06:16] process in order to answer the question
[0:06:19] and then and that's okay. It can be a
[0:06:21] lot. This is a 400,000 token context
[0:06:23] window. It's okay that it's a lot. Put
[0:06:25] it in in a format the AI can fairly
[0:06:29] easily parse. I have tried it where you
[0:06:32] make it parse it and give it a nasty
[0:06:34] format in addition to giving it dirty
[0:06:38] data. And I will tell you it does it,
[0:06:41] but you're going to have much better
[0:06:42] results if you give it clean data in a
[0:06:44] format it understands. So take the time,
[0:06:47] get it into markdown, get it into CSV if
[0:06:49] you can, and then once you supply the
[0:06:51] data to the system, you want to very
[0:06:54] clearly specify the artifacts that will
[0:06:58] enable the AI to show it's done the
[0:07:00] work. And this is another distinctive of
[0:07:02] Chat GPT5 that I think is going to have
[0:07:04] to get wrapped into training curricula.
[0:07:06] You need to be at a point with your
[0:07:09] teams where they know the outputs that
[0:07:13] AI needs to write, build, demonstrate to
[0:07:17] show that it has done the work. In other
[0:07:21] words, with this model, with Chad GPT5,
[0:07:24] it does better if you force it to prove
[0:07:26] its work than if you tell it to do the
[0:07:29] work. So, in other words, when you're
[0:07:30] asking for the output, say, "Hey, give
[0:07:33] me the sentiment analysis. Give me the
[0:07:35] Python workbook to show how you did it.
[0:07:38] And then also give me a plain English
[0:07:41] summary of the rubric and the scoring
[0:07:43] assessment that you used for the
[0:07:45] sentiment analysis along with any
[0:07:46] personas that you developed. Something
[0:07:48] like that, right? Like basically, show
[0:07:50] me what you did. Sure, give me the
[0:07:52] executive summary and the report, but
[0:07:54] show me all the artifacts along the way
[0:07:55] as well and demand those as outputs. Why
[0:07:59] is that important? Well, go back to the
[0:08:01] original architecture of chat GPT5. It's
[0:08:05] important because this is a model that
[0:08:08] is like a skin stretched over a bunch of
[0:08:11] different machines in the background.
[0:08:13] You are basically specifying artifacts
[0:08:16] that trace to more tool calls in the
[0:08:19] background that get you more of what you
[0:08:21] want. And so when you specify the Python
[0:08:23] grader for instance, you're specifying
[0:08:25] tool use effectively around a particular
[0:08:28] kind of grading that you want done on
[0:08:30] this particular data set. When you do
[0:08:32] that across a range of artifacts, you
[0:08:34] are hard- coding or invoking specific
[0:08:37] tool calls that you can then ensure are
[0:08:40] used against the data set in the way you
[0:08:42] want. And so that's why proving it
[0:08:44] matters. That's why defining the
[0:08:45] artifact seems to especially matter with
[0:08:47] GPT5. That is going to be a big jump for
[0:08:51] teams that are used to just saying, you
[0:08:53] know what, produce this thing. And I
[0:08:56] think it's a good jump because in a
[0:08:57] sense, we're asking the AI to do more
[0:09:00] meaningful work. We're asking the AI to
[0:09:03] come back with more in-depth analysis
[0:09:06] that it really wasn't possible to do
[0:09:08] before. And so instead of thinking of AI
[0:09:10] as a text output generator, which so
[0:09:12] many teams do, we're asking AI to think
[0:09:15] more multimodally. We're asking AI to
[0:09:18] take advantage of the maths and the code
[0:09:20] that it's able to do and actually put
[0:09:22] that at the service of our teams, even
[0:09:24] if we're not in engineering. And that's
[0:09:26] why training teams to think in artifacts
[0:09:28] really helps. So that's that's the
[0:09:30] second key piece I want to call out. So
[0:09:32] we've talked a little bit here. There's
[0:09:33] there's more to come in this video.
[0:09:35] There's a lot to dive into, but I I want
[0:09:37] to just pause for a second and say as
[0:09:39] you think about this, remember this is
[0:09:41] multiple models. You need to trace the
[0:09:44] call to the right model. So make sure
[0:09:45] that you're asking the model to think
[0:09:48] hard and get to the right problem space.
[0:09:50] Make sure you give it clean data and
[0:09:52] make sure that you asking for artifacts
[0:09:54] along the way. I will also add as we
[0:09:56] sort of move forward in this discussion,
[0:09:58] it is really really important for you to
[0:10:02] be clear with your teams about the way
[0:10:06] you want certain problems addressed in
[0:10:08] GPT5. A lot of execs settle for AI to do
[0:10:12] this. I used AI to do this. That used to
[0:10:14] be kind of okay. It is now definitely
[0:10:17] not okay because the difference between
[0:10:19] bad usage of GPT5 and good usage of GPT5
[0:10:23] is so large. You cannot just tell your
[0:10:26] team I use GPT5 for this anymore. You
[0:10:29] need to specify and say this is how I
[0:10:32] use this tool to get this result. The
[0:10:34] specificity of communication takes more
[0:10:36] work on your part. It takes more work on
[0:10:38] the part of anyone who's teaching AI at
[0:10:40] your company. But the trade-off is that
[0:10:43] if you do that work now, you are going
[0:10:46] to get more AI fluency around a tool
[0:10:49] that is even less obviously powerful
[0:10:52] than previous AI models. At least when
[0:10:54] 03 came out, it was obviously powerful.
[0:10:56] You were talking to the reasoning model
[0:10:58] all the time. Now it's GPT5. The
[0:11:00] reasoning model is one of several that's
[0:11:02] hiding back there. And you have to kind
[0:11:04] of feel for it in the dark of latent
[0:11:06] space. And why why you ask did Chad GPT
[0:11:09] do that? Because people were complaining
[0:11:11] very loudly about the fact that there
[0:11:12] were a bunch of models and we had to
[0:11:14] pick which model to use. Well, the
[0:11:16] trade-off is we don't have to pick the
[0:11:17] model anymore. But now we have to invoke
[0:11:19] the path through the model to the power
[0:11:21] that we want behind the model. And
[0:11:24] that's the trade-off. You get only one
[0:11:25] model. Super simple. Everyone's using
[0:11:27] GPT5. But now we have to talk more about
[0:11:29] how we invoke that power. There's no
[0:11:31] free lunch. That's how it works. Now, as
[0:11:33] we sort of round out this discussion and
[0:11:36] and start to think a little bit about
[0:11:37] the wider implications of GPT5 and where
[0:11:40] we're going over the next year or two
[0:11:41] and how AI transformation unfolds from
[0:11:43] here, we have been in an era
[0:11:47] characterized by model choice. We are
[0:11:49] not in that era anymore as of August. We
[0:11:53] are now in an era when the model choice
[0:11:55] has largely been made for us. And it is
[0:11:58] the model usage that is going to
[0:12:00] determine whether organizations survive
[0:12:02] or perish. Specifically, it's whether
[0:12:05] organizations are able to quickly
[0:12:07] understand how to get the most out of
[0:12:09] the model for specific use cases that
[0:12:11] tie to their data and their teams. You
[0:12:14] know, those brown bags and those
[0:12:16] socializing AI wins that you would sort
[0:12:18] of see happen and maybe they peter out
[0:12:21] after a month or two. Those really
[0:12:23] matter. Now you need to be in a place
[0:12:25] where you are rapidly socializing how to
[0:12:27] use chat GPT5 across your business. You
[0:12:31] need to be in a place where you are
[0:12:32] defining really explicitly these are use
[0:12:34] cases in the business that are new that
[0:12:36] we can now unlock because there's a
[0:12:37] larger context window because thinking
[0:12:39] mode gives us synthesis across messy
[0:12:41] data that we didn't have before. Great.
[0:12:44] Define them. Name them. You're now going
[0:12:46] to have to tell people how to prompt for
[0:12:47] them, how to prep the data for them. And
[0:12:49] if you can get it right, you're going to
[0:12:51] have something most other companies
[0:12:53] don't know how to do, especially if
[0:12:55] they're just telling people start using
[0:12:56] GPT5. At the same time, if you're using
[0:12:58] the non-reasoning version of GPT5 and
[0:13:01] it's very, very fast and it writes a
[0:13:02] little bit better, you're going to have
[0:13:04] to get more aggressive about working
[0:13:06] with people to give GPT5 non-reasoning
[0:13:08] for the simple textbased stuff, really
[0:13:10] good prompts that drive it to write in
[0:13:13] your style, that drive it to write with
[0:13:15] no hallucinations and factcheck its
[0:13:17] work, that drive it to make sure it is
[0:13:20] complete in the answer and not
[0:13:22] overpromising. Those are all things that
[0:13:24] I have seen in practice. Is it better at
[0:13:27] hallucinating than 03? Somewhat. Yeah.
[0:13:29] Is it going to benefit from you telling
[0:13:32] it explicitly what the bar for clarity,
[0:13:35] for adherence to reality, for adherence
[0:13:37] to facts, for explaining only the answer
[0:13:40] to your solution and not 16 other things
[0:13:43] is yes, it will benefit from that
[0:13:45] clarity. This is a model that I have
[0:13:47] compared to a product manager on crack.
[0:13:50] Helpfulness is off the chain. It comes
[0:13:52] back, it gives instructions, it gives
[0:13:54] overhelpful suggestions. It has been
[0:13:56] trained to be a completeness artist.
[0:13:59] Your teams will need to learn to rein it
[0:14:02] in. Your teams will need to learn to
[0:14:03] give it guard rails. So even if we're
[0:14:05] not talking about the really complex
[0:14:07] stuff and the data stuff and you're just
[0:14:08] talking about the simple non-reasoning
[0:14:10] model, your teams still need to learn to
[0:14:13] rein it in in ways that enable it to
[0:14:16] provide very useful content rapidly.
[0:14:18] Because the last thing you want is for
[0:14:20] teams to give up and walk away from it
[0:14:22] because then they don't get the value.
[0:14:24] or conversely for teams to use it and
[0:14:26] just copy paste from it and you're going
[0:14:28] to be able to tell because suddenly all
[0:14:30] of your meetings are going to look super
[0:14:31] complete with agenda items that no one
[0:14:33] pays attention to and no one does
[0:14:35] because they're all made up by AI. Be
[0:14:37] careful because this model makes up
[0:14:40] completeness that your organization may
[0:14:42] not actually have internally. Be aware.
[0:14:45] This model likes to pretend things are
[0:14:48] complete. That's part of why it's a good
[0:14:50] coding model. And that brings me to my
[0:14:52] last observation for teams and what is
[0:14:54] new with this model that you need to pay
[0:14:56] attention to as a leader. There is a new
[0:14:59] category of software that launched on
[0:15:01] August 7th. It is not it it got called
[0:15:04] vibe coding. It's not really vibe coding
[0:15:06] or it's not the same vibe coding that
[0:15:07] we've had for months. The vibe coding
[0:15:09] we've had for months is you go to
[0:15:12] lovable, you go to bolt, you go to
[0:15:13] replet and you type something in and it
[0:15:15] builds an app. It might have a backend
[0:15:16] and transactions and loginins. it's a
[0:15:18] real app or at least it's supposed to be
[0:15:19] an app and you wrestle with it and maybe
[0:15:21] eventually you launch it. This is a
[0:15:24] lower category of software not in the
[0:15:26] sense that it's less useful but in the
[0:15:27] sense that it's more casual. It is
[0:15:29] kitchen table software. It is software
[0:15:30] for personal usage and it was positioned
[0:15:33] for personal usage in the call. I've
[0:15:35] certainly been able to use it for
[0:15:36] personal usage, but I've also been able
[0:15:39] to use it for professional usage
[0:15:41] immediately and people are sleeping on
[0:15:43] that. As an example, you could ask chat
[0:15:46] gpg5, make me a gant chart for this
[0:15:48] really complicated like giant Excel
[0:15:50] spreadsheet. It is probably going to
[0:15:52] come back with an image of a gant chart
[0:15:54] that is not ex exactly what you want and
[0:15:56] you're going to swear and say this thing
[0:15:58] can't do gant charts. Have you tried it
[0:16:00] with code? Go to the model and say
[0:16:03] here's the data respond in code. Build a
[0:16:06] gant chart app that shows this in code.
[0:16:09] And I did that. I did that with the
[0:16:11] Apollo 13 mission. I built out a whole
[0:16:13] Gant chart in code. It could not do it
[0:16:15] visualizing it directly. In other words,
[0:16:17] think of code as a tool that your teams
[0:16:20] can use for project artifacts. Low low
[0:16:23] casual artifacts where you share the
[0:16:24] link and say, I built a chat GPT app for
[0:16:26] this. This is our Gant chart for the
[0:16:28] project. Right? I built a chat GPT chat
[0:16:30] GPT app for this. This is our project
[0:16:33] update for the week. That kind of thing
[0:16:36] is now software. And teams have no idea
[0:16:39] that that's there. They don't know it's
[0:16:41] there. No one taught them that in
[0:16:42] previous prompting and levelups and AI
[0:16:45] courses. And do you know why? Because
[0:16:47] that wasn't possible before. This is
[0:16:49] really the first time we've had
[0:16:50] reasonably good coding with a reasonably
[0:16:54] complete ability to represent an app. I
[0:16:57] tried some of the stuff I worked on in
[0:17:00] Claude, which is a good coding app. It
[0:17:02] could not do this natively. I'm not
[0:17:05] saying anything against Claude in the
[0:17:07] API. It's an extraordinary model for
[0:17:08] coding. But if you want native
[0:17:10] representation in the canvas, you don't
[0:17:12] want a development environment, you
[0:17:13] don't want anything else, you just want
[0:17:14] to like try it and see if it codes up a
[0:17:16] little app, JPT5 is the best thing I've
[0:17:19] seen. It is absolutely potentially
[0:17:21] transformative if you can tell your
[0:17:23] people very clearly that for small
[0:17:26] presentations, for small things to work
[0:17:28] on that represent data in interesting
[0:17:30] ways that are visual, that might be
[0:17:32] slightly interactive, you want to be
[0:17:35] able to use this app for a weekly
[0:17:37] business review. That's a classic one.
[0:17:39] You have data. You need to represent it.
[0:17:41] You need to be able to click around,
[0:17:42] look at the the metrics. You should be
[0:17:43] able to use an app for that. You should
[0:17:45] be able to tell Chad GPT5 to do it. Now,
[0:17:47] not everyone's going to do that. There's
[0:17:49] going to be a lot of people that say
[0:17:50] they want to stick to their existing
[0:17:51] templates. The advantage if you do get
[0:17:53] into the culture of building apps is
[0:17:56] that you really unlock groundswell
[0:17:58] innovation from your team. Your team
[0:18:00] will come up with ideas for apps you did
[0:18:03] not have. If you can bless it and if you
[0:18:05] can remind them that it's a good thing.
[0:18:06] If you can remind them you're supportive
[0:18:08] of this kind of kitchen table software
[0:18:10] and you want them to be able to use it
[0:18:12] to solve interesting problems. You will
[0:18:14] not imagine the 200 use cases across
[0:18:16] your business. You'll imagine three or
[0:18:17] four of them and you'll try them out and
[0:18:19] you'll let people know these are awesome
[0:18:22] little use cases. I tried one. Here it
[0:18:24] is. Like I tried a travel itinerary one.
[0:18:26] It's really fun. And I could see when I
[0:18:28] showed people that tiny travel itinerary
[0:18:30] app I made that their eyes lit up and
[0:18:33] they're like, "Oh, let me remix that.
[0:18:35] Let me try that." And Chad GPT makes
[0:18:37] that so easy. You can remix it like
[0:18:39] you're remixing music. You can go back
[0:18:41] in and say, I want it to be, you know,
[0:18:42] the travel itinerary is for a different
[0:18:43] place, right? It's going to be for when
[0:18:45] I go to the Grand Canyon and so I want
[0:18:47] you to re remake it. And it's really
[0:18:49] easy to do. The weekly business review,
[0:18:51] it's going to be for the sales or not
[0:18:53] for marketing. I want to remix that
[0:18:54] artifact. People need to get your
[0:18:57] blessing to use chat GPT in new ways
[0:19:00] because the assumption is often if I try
[0:19:02] it and if I fail, it's going to be bad.
[0:19:04] And this goes back to classics of change
[0:19:06] management, right? Like you need to
[0:19:08] bless people to fail so they can learn
[0:19:09] to succeed. Okay, wrapping all of this
[0:19:12] up, what have we learned here? Number
[0:19:14] one, rollouts for Chad GPT5 are going to
[0:19:17] be different from rollouts for anything
[0:19:18] else because of the way they've made it
[0:19:20] one model. And so when we think about it
[0:19:22] from that frame, certain implications
[0:19:24] fall out that are new and different
[0:19:26] based on previous AI rollouts for orgs.
[0:19:28] First, you have to tell people how to
[0:19:30] prompt and access the power behind the
[0:19:31] model. That's where I called out think
[0:19:33] hard. Second, it could tackle big,
[0:19:36] gnarly, data heavy problems in the chat
[0:19:38] the way it never could before. You have
[0:19:40] to be responsible for the data you put
[0:19:41] in, for making it clean. You have to be
[0:19:43] responsible for teaching people to
[0:19:44] prompt it well. And you have to be
[0:19:46] responsible for reminding people that it
[0:19:49] does that work best when you invoke
[0:19:52] those tools by demanding artifacts, by
[0:19:54] demanding proof of work, by demanding
[0:19:56] that it actually shows how it did the
[0:19:58] work. Not that you tell it how to, but
[0:20:00] that you demand the artifacts that shows
[0:20:02] that it did the work. And that's a
[0:20:03] that's a fine distinction, but it's
[0:20:05] important to emphasize because telling
[0:20:06] it how to, that doesn't matter. But
[0:20:08] saying, I want you to show me the
[0:20:09] greater you used. Well, that's actually
[0:20:11] helpful. Then moving on from sort of the
[0:20:14] data analysis piece, we talked a little
[0:20:16] bit about the importance of making sure
[0:20:20] that your team feels comfortable using
[0:20:23] AI in new ways that are unexpected
[0:20:26] because Chad GPT5 unlocks those, right?
[0:20:28] So we talked about this coding use case.
[0:20:31] It's an entirely new class of work. How
[0:20:33] can you help the team understand that?
[0:20:34] How can you socialize out these sort of
[0:20:36] coded artifacts that become essentially
[0:20:38] a new way of doing business around the
[0:20:40] office? We talked about the importance
[0:20:41] of making sure that teams feel
[0:20:44] comfortable using AI for non-reasoning
[0:20:47] tasks in Chad GPT5 and what that looks
[0:20:49] like because again non-reasoning chat
[0:20:52] GPT5 is very good. It's extremely fast.
[0:20:55] It's extremely coherent. But it's going
[0:20:57] to be up to you to convey these are the
[0:20:58] guardrails. This is the house style.
[0:21:00] This is what I want to do with
[0:21:01] hallucinations and how I handle it. This
[0:21:03] is when you can be creative and when you
[0:21:04] can't be creative because hallucinations
[0:21:06] and creativity relate. And so if it were
[0:21:09] me, I would go in Monday morning and I
[0:21:12] would look at the current AI
[0:21:15] transformation playbook you have and I
[0:21:17] would basically say let's assume that we
[0:21:19] can do 20% more with AI because of chat
[0:21:21] GPT5 and let's assume that a lot of the
[0:21:25] way we taught the org is going to have
[0:21:26] to change because the old ways of
[0:21:30] learning are gone. the old ways of like
[0:21:32] I hope you weren't still doing think
[0:21:34] step by step in your AI transformation
[0:21:36] playbook, but if you were, that's got to
[0:21:38] go. I hope you weren't doing too much of
[0:21:40] an emphasis on model selection, but
[0:21:42] that's going to have to go. Uh, I hope
[0:21:44] you were able to communicate that
[0:21:48] whatever you're teaching people is going
[0:21:50] to update as models come out because
[0:21:51] that's still true. You're going to have
[0:21:53] to update how people think about which
[0:21:56] tasks they select, how people think
[0:21:58] about how they share their work. I
[0:22:00] called that out. You're going to have to
[0:22:02] update how you think about prompt
[0:22:04] libraries and what they contain because
[0:22:06] prompt libraries might now contain not
[0:22:08] just prompts, but also the artifacts
[0:22:10] that came out of good prompts. Like
[0:22:12] maybe you're saying, I want to have a a
[0:22:14] customer sentiment analyzer and here's
[0:22:16] the prompt for it, but here's the Python
[0:22:18] autograder for it, too, and I want you
[0:22:20] to have that. Or, you know, the prompt
[0:22:22] library contains not just the prompt to
[0:22:24] build the app, but also an example of
[0:22:25] the app that you can remix. Right?
[0:22:27] There's there's more that's evolving
[0:22:28] here that we haven't seen before. Do I
[0:22:31] have it all figured out? I do not have
[0:22:33] it all figured out. I do have some
[0:22:35] strong convictions on where GPT5 is
[0:22:38] going with the organization and I wanted
[0:22:40] to share these early field notes with
[0:22:41] you so that you also get a sense of what
[0:22:45] you need to focus on as you roll out
[0:22:46] GPT5 to your orgs. Good luck. Let me
[0:22:49] know how it's going and uh I will
[0:22:51] continue to report from the field as I
[0:22:52] dig into GPT5 transformation.

---

*Extracted from YouTube Auto-generated captions*