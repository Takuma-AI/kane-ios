# Intentional Tech: Designing AI for Human Flourishing | Alex Komoroske

**Channel**: Every  
**Date**: 2025-07-09  
**URL**: https://www.youtube.com/watch?v=AgCIbh0uixs  
**Transcribed**: 2025-07-10

---

## Transcript

We have this new technology, LLMs,
that I think are as transformative as the printing press,
electricity in the internet, and we have a choice.
We can go down the path that we've been going down,
which is engagement maximizing, hyper aggregation,
going after what you want, not what you want to want
as a user, or being aligned with our intentions.
And that could lead to a new era of human flourishing, I think.
I intend to spend quality time with my family,
and intend to experience new things,
the right product fabric could enable that.
But that has to be fully aligned with you.
Imagine if that context, all of that rich substrate
of meaning that you've distilled and written down.
If it's not working for you, that's terrifying.
I would run onto the stage where Steve Jobs
shows off the iPhone, which was a poster that says,
this will become the most important computing device
on Earth, it is insane to allow a single company
to decide what things you may run in it, right?
I know it feels like in this industry
that we're like halfway through the LLM era.
We are in the very first inning.
We're like rubbing sticks together.
We still think that chatbots are the main thing.
Will chatbots be important?
Yes.
But chatbots to me feel like a feature or not a paradigm.
This podcast is supported by Google.
Hey, everyone.
David here, one of the product leads for Google Gemini.
If you dream it and describe it, VO3 Gemini
can help you bring it to life as a video,
now with incredible sound effects, background noise,
and even dialogue.
Try it with a Google AI pro plan
or get the highest access with the ultra plan.
Sign up at Gemini.google to get started
and show us what you create.
Alex, welcome to the show.
Thanks for having me.
So for people who don't know you,
you are the co-founder and CEO of Common Tools,
which you'll describe for us.
You are also previously the head of corporate strategy
at Stripe and director of product management at Google.
And I think like six months ago,
and the thing that really stood out in our conversation,
aside from just like, you have so many interesting ideas
is you're this really interesting systems builder.
You think a lot about personal systems, work systems,
things that help you get more done,
or think differently about the world.
And I think people like you who have that kind of obsessive
mind with systems is something that I recognize myself.
And also I think are people who have a lot of interesting things
to say about just new AI ways,
because I think it's sort of like steroids or catnip
or whatever you want to say.
Some kind of a teller in, for sure.
For people like us, I'm excited to chat with you.
I'm excited to hear both what you're thinking about now,
what you're building, and what kinds of systems
you're familiar with personally.
Yeah, cool.
So when we start with common tools,
because I think there's something interesting about your,
like what you're building, because I think it speaks
to the perspective you have probably
about this larger AI way of talking about it.
Yeah, the way I think about it,
and it's hard to describe,
because we've had to build a new kind of thing
that's only possible in the world of AI.
So when I describe it, we'll say,
oh, you're being coy.
It's like, no, tell you what I'm building.
It's just a new kind of thing it's hard to grab onto.
I think of it like a coactive fabric for your digital life.
And by coactive, that's an old word
that I hadn't heard before,
but I think is a perfect fit for it of you
are active in the system.
And so is this emergent intelligent process,
or private intelligence powered by LOMs.
And those can be coactive on the same substrate
in adding detail or adding the little bits of software
or connecting up things.
And I think that that really unpacks
or unlocks the power of LOMs.
That's, I love that word, and it's interesting,
because I think we're finding some similar
for some of the products we're incubating internally
is like, right now the paradigm is,
you sense willing to touch and tee and it sends something back.
Maybe it now it sends something back after 10 minutes,
which is a new thing,
but it's not like you're not working in parallel.
It's not like working in the background
and then being like, I had this idea.
And we're starting to like,
as we build our own sort of agentic systems inside of every
for your email or for writing or whatever,
we're finding that having something that's working
while you work with you and being a little bit more proactive
is actually a really interesting next paradigm.
Yeah, to me, I mean, if you say,
okay, we have LOMs, what's the first product
in 10 seconds to go chatbots?
It's those obvious extension
and everybody is focusing only on chatbots.
Like will chatbots be important?
Yes, but chatbots to me feel like a feature, not a paradigm.
And they are, so like they're like this coactive surface,
but it's just append only.
I append a message, it depends a message.
I append a message, it depends a message.
And literally back and forth, it doesn't have a way
to like have multiple for me in a row.
And you know, chatbots are amazing for starting any task
because there's no structure.
You just say whatever you're thinking
and it helps to still and respond.
But for long live tasks, you need structure.
You want there to be something where you can glance over
and you know, a little cubby hole
where you put certain information.
I find, I have, you know,
I do dozens and dozens and dozens of chats
with Chat2BT and Claw today.
And I'm just swimming in all that context
and all the chats.
And so I'm using it to do important things.
And also, I'm just completely lost in it
because it's only this chatbot kind of penned only
kind of paradigm.
You have this really cool thing called bits and bobs,
which is this like real time Google doc
that you just have been updating for.
I think like many years at this point
with things that you're thinking about,
which we'll link to in the show notes and I love.
I think it's, I think it's really cool
and really inspiring just sort of the rawness of that.
And one of the things that's in that Google doc right now
about what you're thinking about is the idea
of intentional tech, which Adoptel is really nice to you
with something that I've been thinking about,
which is this idea that AI has,
is this new era of technology
where technology can understand our stated preference,
stated preferences, which is different
from the social media era,
which worked totally just on-
What you dwell on and what you clicked on, whatever.
And so social media tends to make us
tends to service things that are like more outrage driven
or more like sexier or whatever.
Anything that's more eye, yeah, like the car crash effect,
basically.
And AI, because it can talk to us,
gets a much richer understanding of who we are,
maybe like I think TouchEBT is much more helpful
and much more enlightening, for example,
than like the Facebook algorithm is just by default.
So I think that Dovetel's a lot
with what you've been thinking about with intentional tech.
So tell us what's on your mind about it.
Yeah, intentional tech, I think,
is really critical in this new era.
We're at a crossroads as a technology industry.
I think we have this new technology, LLMs,
that I think are as transformative
as the printing press, electricity in the internet.
So it's this big general purpose unlock
for all kinds of stuff that wasn't possible for.
And we have a choice.
We can go down the path that we've been going down,
which is engagement maximizing, hyper aggregation,
going after what you want, not what you want to want
as a user, or being aligned with our intentions.
And that could lead to a new era of human flourishing, I think.
This is not the default path, by the way.
We will have to choose it.
We'll have to work to build that.
And that's why I think intentional tech is so important
because we want technology that aligns with our intentions.
Not necessarily what I want, my revealed preference
of like looking at the car crashes or whatever,
but what I intend to do,
I intend to spend quality time with my family,
and intend to experience new things.
I intend to read interesting takes that disagree,
that it can be disconfirming evidence
that challenge my worldviews.
Those are some of the things I intend.
That's what I find meaningful to do.
And it's very easy to fall out of that.
But LLM's I think with the right product fabric
could enable that.
Another important part of this is it aligns with your intentions
as in it is working for me is an extension of me.
I heard a word last week, I love of exo cortex.
It's like an extent, you're caught in the exoskeleton.
But that has to be fully aligned with you.
Imagine if that context, all of that rich substrate
of meaning that you've distilled and written down
that's all about the things you care about
and all these facts about you.
It was not working for you, that's terrifying.
Like I saw someone a few weeks ago
at one of the big tech companies said,
we're making our tool personal, proactive, powerful.
It's like, well, let me stop you right there
because the very first word personal
doesn't actually align
because you are a massive corporation
that's trying to sell me ads.
And if you're maintaining a dossier on me
and then that dossier is leading to a powerful
and proactive thing, that's terrifying.
Whereas if it's working just for me
of the extension of my direct agency,
that's really empowering.
And I think that's one of the reasons
intentional tech is so important at this era.
That's really interesting.
And tell me more about like,
because when I think about this,
one of the words that comes up,
you talk about alignment,
is that AI started,
or this generation of AI started with alignment
or alignment with human preferences at its core
because we were all afraid that AI was going to kill us.
And now it's like, well, it might not kill us,
but maybe it'll just serve us ads
that will make us dumber,
send our money in ways that we shouldn't have whatever.
So I'm kind of curious what you think has to happen
to make us go on this sort of more intentional path.
To me, it's alignment, I think it's fun.
Yeah, we use it for the underlying LLM model.
I think there's two layers.
There's the model itself,
which is, if you imagine this as a stateless thing
that you send a query to when it gives you back a response,
it doesn't store any state,
then you have alignment problems of like,
what kind of biases are baked into it and what have you?
But then there's a layer on top, which is your context.
And that is the thing that is a valuable bit of information
about you that changes,
it has lots of rich meaning on it.
These are two separate layers, I think.
And so I am more interested in this layer at the top
and assuming if you have multiple LLMs
that you can choose and swap between
that don't remember anything about you,
then it's fine.
You can swap between different ones
and that matters less to me.
Where context is, the place that's all aggregated
is I think more important.
Well, that's an interesting architectural question.
Like right now, language models are stateless.
Do you think like in five years, for example,
they're still gonna be these sort of stateless intelligence boxes
or do you think they're gonna be auto updating their weights?
For example, as you talk to them.
I don't know about auto updating their weights
and this might be there's an architectural breakthrough
but this is one of the things that's weird
about these models is that they do take a long time to bake.
So like they have this weird paste layer down at the bottom
that's like months behind on training data
and it just takes time for it to bake.
And then you have these layers on top
that the system prompts now, like in chat,
GPT will inject a little bits of your context,
a weird bit that by the way, you cannot inspect,
which is kind of funky, right?
Like if you ask it, hey, what context about me
did you put it in there?
It's like, I can't tell you.
Like that's creepy, right?
I can't do that Dave, you know?
And Simon Willis and we were chatting last week
and he's got some prompt that will extract out
this dossier about you.
It includes things about stuff you said in the past.
It could include things like your insecure about your weight,
you know, these things about you.
And then it also says in the last,
in the last five weeks,
9% of your interactions with the chatbot have been bad.
As in, you've been trying to manipulate it or whatever.
Like, I don't know, it's weird seeing this view
into this for yourself.
So I think chat GPT and others are very clearly
trying to move merges together
so that you have all your context in chat GPT
and it's hard to leave.
That I think gets more problematic,
especially if you're going down
an engagement maximizing kind of playbook,
which, I mean, a bunch of, you know,
the executives from Facebook now run
three of the four big chatbots, you know?
I think go back a step to meaningful computing
as four major components to me.
One is it's human centered, not corporation centered,
centered around me and my intentions.
Two is it is private by design.
That means that the data is only for me to see,
it's for me to bring where I want to bring
and to choose who gets to see it,
using techniques like confidential computing
to make sure it is entirely private.
It has to be prosocial.
It has to be something that helps you live a life
integrated with society, not just being this hyper individualized
kind of, you know, a little island to yourself,
but integrate with society in a meaningful way.
And the last piece, which I'm going to remember
from my head, is it also is open ended.
It has to be something that allows you to explore
and to build and to create new experiences within it.
The way that our software works today
is only the entity that created the software
is allowed to decide what kinds of things
you can do in the software.
And that's due to the security model
of how all of our software has worked
the last 30 years, but that kind of closed-endedness
means that you had to convince some PM somewhere
to a prioritized or a feature,
which if it's particularly niche,
probably hasn't been, probably hasn't happened.
Okay, well, there's a lot of things that I would love
I would love to dig into it, dig into all those points.
But the big one that stands out to me is,
right now we live in a world where in order to get this
great new technology that's advancing
at this really rapid pace,
we have to get it from well-funded startups
that have a profit motive.
And I know OpenAI is a non-profit, but like, you know,
whatever.
I think that's what they say, too.
Yeah, whatever.
Clearly they're being run like a startup.
And I guess for me, like, I don't think that's a bad thing,
but it sounds like at least one way to read what you're saying
is we have to get rid of the sort of like corporate startup
structure for these technologies to be made in a sort of social way.
Like, how do you envision this being made us being able to make
these decisions in terms of what kinds of corporate structures
or startups or nonprofits or whatever are controlling
and building these things to get the outcome that you're talking about?
I think it's compatible with being a business.
So for example, we're charted as a public-benefit corporation.
Yeah.
The, to me, it is what's most important is that users are paying
for their, like, it's not free.
If you aren't paying for your compute and it's not working for you,
it's working for somebody else.
That just so happens to think that giving it to you is a benefit.
Now, it doesn't mean that's a necessary but not sufficient
characteristic because I think Chat GBT's the playbook that they're executing
is one that is about engagement maximizing and making you
secure to the service and what have you.
But you paying for your own compute is a necessary component of it
to make sure that it's actually truly just working for you.
That's where also that privacy really matters of your data
is visible only to you with your keys.
This does not necessarily mean, by the way,
people have historically in the past said,
oh, that means it has to be local first.
I love the local first movement.
I'm very aligned with its ideals and ethos.
Local first is really hard for a couple of reasons.
One, architecturally, it's very challenging to have a bunch of peers
that have to have a visual consistency.
You don't know when they're going to sync back up.
It's very hard to build consumer product experiences
that work the way that people expect.
And two, is it's inconvenient.
If you got to run your local server and your laptop
isn't plugged into the network when you're on your phone
or whatever, the thing doesn't work.
So actually, there are other architectures
that can use things like confidential compute
to run in the cloud in a way that can be remotely attested
to be totally private to you
and that nobody else can see into that data.
And so those are some of the architectures
that allow a full alignment with the humans.
Okay, so basically, you're saying
ChatRBT is doing some of this.
So they're starting with a subscription model.
But there are things that they're doing.
What are they doing that you think is aimed at maximizing
engagement?
I definitely understand the kind of memory thing
I want to keep using ChatRBT because it remembers who I am.
I love that part of it.
What are the things that they're doing in the decisions
that they're making that tell you
that they're going down this engagement maximizing path
that will lead to a bad place?
I think the engagement maximizing path
is just the default path.
So I don't think, by the way,
there's anything particularly, they're like,
oh, here's a dastardly player.
This is how you do tech now, right?
You're like, in the last 10 years,
we just realized aggregators are a really powerful business.
And they want to consume all of the demand
so that all the supply comes to them.
And that's a great business.
I mean, if I were sitting at their seat,
I can totally see why they would do it.
Like, that's, again, it's not in a various place,
just the default thing that you would do.
Yeah.
I think if AI is this incredibly important technology
that also knows all the intimate details of your life,
that becomes especially important for it to be
in an architecture that is not something
that your, something that this other company can look at
and maintain a dossier about you.
Because that, it's just too easy to manipulate, right?
Like, LMS can translate anything from anything.
And that means they can also translate just for you
to figure out exactly how to land a particular message
for you, which means it's imperative
that you know it's aligned with your intentions.
And it's not going to try to like get you to do the,
you know, the Mondo subscription
or to go to one of the partners or what have you.
Well, let's say you're, you're Sam Alman right now
and you're, you're in charge of open AI.
What would be, what are the decisions
that you would make tomorrow that would put you on a path
that you think is actually right?
All right.
So if I were to say, I'm Alman, I would do what they're doing.
Yeah.
And as in it's, I can see why it's a great business.
I think we'll add a lot of value.
That's why I see this as a complimentary approach
to some degree, a baking a system that is a coactive system
that's just working for you that is not just chat.
I can be a complimentary thing to these other systems.
I wouldn't say, oh, you should change what you're doing,
because I think actually what they're doing
is a reasonable thing.
This model that they're creating,
especially because they're available via the API
with no memory stored to them.
That's an important characteristic.
It's great.
It makes a really powerful underlying engine
that can be used to power lots of other products,
especially to the extent that there's multiple
that we can swap between, you know, get stuck with it.
So you think that there's a call for a complimentary technology,
technology provider, set of technology providers
that allow you to take all of the context
that you have, keep it private,
and then bring it to any situation or service that you want.
Yeah.
I would say that my mental model is,
oh, chat GPT is kind of the AOL of this era.
So if you use the parallel of the internet in the web,
AOL was an extremely important company.
They're the ones that brought everybody online.
America, the whole of America online,
has amazing experience.
It was somewhat of a wild garden,
but it also gave you access to the open-endedness
of the open web.
And I think that's the kind of role that I see them playing.
I hope that what we get out of this
is not some closed ecosystem of hyper aggregation
beyond even what we've seen to date,
but instead we see this open ecosystem of things
that are very user-aligned.
Interesting.
Why AOL versus Microsoft, for example?
So Microsoft, I think in a similar way,
not for the internet era, but for the PC era.
I like taught people how to use computers
and brought computers as a thing to most of America
in the world and built a platform
that everyone else is building on top of.
So that's another kind of analogy.
Why do you think AOL is more appropriate?
I think AOL, the analogy that tracks to me best for LLMs
is the internet.
Like LLMs are the internet.
There are new kind of thing that you can do.
AOL, at the beginning, when I first started using it
like second-grader, whatever it was about, chat rooms.
And you could do whatever the AOL is.
Birds or whatever.
ASL, and that was what I thought the internet was.
And then it turns out, as I learned later,
I was like, whoa, this is a whole crazy open ecosystem
that no one controls with all kinds of weird happening.
And that was that open system of the web.
And so in the same way, I see them as distributing
this new technology to people, showing,
yeah, look, chatbots.
You can do these cool things with chatbots.
And then later, you go, oh, wait a second.
The same thing that animate the chatbots could be used
to animate all kinds of new experiences.
I didn't realize what possible for.
That's one of the reasons I think that them
as the AOL versus the migration.
Why did AOL die?
I don't remember.
I think at a certain point, the open-endedness
of the web could have took over.
And it just became the point of like,
I think a lot about systems that you pointed out.
I think open-ended systems tend to win
under certain conditions, especially in the growth era.
Very beginning, there's some vertically integrated thing
that kicks off a new revolution.
Oh, it packages up really nicely and people get it.
And as time goes on, it gets the sort of
combinatorial possibility.
It gets too big for any one entity to successfully execute.
And so the open system, if there's any way
for it to escape out in the silent, it was with the web.
I'm sorry, I talked with my hands as you could see.
So if there's any way for it to sort of escape out the silent,
like there is with the web or using APIs to access LLMs,
then the open system kind of takes off
this combinatorial swarm and overwhelms the closed system.
That's also potentially just a hopeful thing.
I hope we typically see oscillation
between open systems and closed systems.
We've been in closed systems dominant for like
the last 10 years and I think it's a bummer.
Like I think we're kind of,
in this, we're an amazing age of technology
and it's also kind of the dark ages of tech
because we're all this hyper-arigated thing
where the only features that can exist
are the ones that aggregators for consumers
and the ones that aggregators have decided to prioritize.
And that's a bummer because the,
as a company gets larger and larger and larger,
the cohesion floor at Clay Sharky would call the cohesion floor,
the smallest feature that it makes sense
for them to even consider prioritizing
goes up and up and up and up and up.
So if you're at Google,
if I haven't tried to pitch a product to other teams
that, and they say, how many net new active,
net new one-day actors do you think this product will get?
I'll say, I don't know, 50,000, I'm gonna go 50,000.
I wouldn't do this for 50 million.
You know, it's like there's this whole class
of features that just can't exist in a world,
the world that we're currently in,
that's hyper-arigated.
It seems like one big difference between the web,
the early stages of the internet,
which to be honest,
I don't know the history like that,
well off top of my head.
And language models is,
the web was like, you have,
I mean, it was sort of government run at first.
You have this like international consortium
that kind of just defines the standards
for how the web works and all that kind of stuff.
That doesn't seem to be the case here.
So I don't think, actually,
I think we are waiting for the web of AI to show up.
Interesting.
And that's partially what we're trying to help catalyze.
And that's, I think that's what I mean by like,
we have the internet and we're like,
cool, the internet's definitely gonna be useful.
Like if you look back at
Al Gore's proposals for the air information super-highway,
it was actually very prescient in a lot of ways.
It's just, it was all about
pipelining existing content and business models
into people's homes.
And some of that definitely happens.
You get things like Netflix or whatever.
But a lot of like YouTube, social networks,
none of that was envisioned by the,
or the, you know, Wikipedia.
None of that was envisioned back then
because we just didn't know how to imagine it.
It wasn't even the thing that we thought was possible.
And then the web creates this open system
of lots of different people trying out different things
and seeing what things work,
found some of these interesting new pockets of value
that grew into whole continents of value.
So how do you make that though?
Like do you have to,
do you have to get governments involved?
Do you have to block chain where everyone's like sort of
out of the way?
I don't know.
I think you need to know that it sounds effective.
You can build,
we have so many powerful substrates.
The web exists as a distribution platform.
Everyone's, everyone's device speaks to the web.
And so you can use as a way to distribute
a whole new kind of experience that fits within this.
So I don't think back in, for the internet,
you needed a consortium,
you needed a tons of capital expenditure
to build the pipes.
And that was government grants.
It was also businesses that way overbuilt
all this capacity.
Those businesses, by the way,
were great for society.
We got all this excess band with the capacity,
kind of crappy businesses though, right?
Because you build a pipe somewhere.
If someone builds another one, right next to you,
you lose all pricing power.
You know it's a total commodity.
But it's a commodity that powers the rest
of this innovation on top.
I kind of see large language models,
the producers of them,
as people laying the pipes.
They're the ones doing this extremely capital intensive
creation,
but there's actually not particularly
that much of an edge across them,
which is great for society.
Because that means you can take LLMs as a commodity.
You can assume that they exist
and reasonably high quality ones,
including quite good open source ones.
And that means that all kinds of interesting business
dynamics take off,
and you can kind of take it for granted
and build what's up here.
Now they can take high quality LLMs for granted.
Yeah, it does strike me that the thing you're talking about,
which is the bar for like a Google PM
to build a feature is like we need 50 million users.
And so you sort of,
you sort of look past the like 50,000 user use case,
which those are just valuable in themselves
and be often the ones that end up being
the 50 million user use case in five years.
Some subset of them will become
that's like the sort of innovator's lemma type stuff.
Yeah.
It sort of strikes me that in a world
where language models are commodities,
capitalism just does that automatically
because small startups will just like do the 50,000 user use case
and just build it.
So is this just going to happen without us?
So here's what it might,
and I agree that like this kind of,
I'm so glad there's so many different ways
that the society could have gone.
Open AI could have had Chatchee BBT
before they released the API.
That would have been such a different world.
It would have been very different.
Because now everyone assumes they have to compete
with an open access API.
And now it'll be very hard to close that door in God.
So we can take it for granted for the rest of us.
So like I just like woke up with a last night
with a nightmare that like it had gone the other way.
So I'm very glad to have it.
For like IBM developed AI.
Yeah, yeah.
What would like, what would like,
$30 million a year for like one query?
Yeah.
So that's great.
Here's the thing though,
that means that the current laws of physics,
the current security model that we use for the web and apps
actually limits this possibility.
And this security model is called the same origin paradigm.
The only people who know about it really,
even web developers don't really know about it.
I know I do.
What is that?
Same origin paradigm is the laws of physics
of how the web works.
And what it says is it's the security and privacy model
that we've used for, since for 30 years.
It kind of actually grew up as an accident actually.
And at the very beginning, the web has no state.
And so you reach out to a server,
it gives you back the same thing that anybody would have gotten
if they gave that exact same request.
Well, then you add cookies.
And then you need to say,
when do I send, where do I send cookies to
once they've been set?
Which other URLs do they go to?
And then the easiest thing is,
okay, there's a notion of a site,
which is roughly a domain.
I see.
And so we'll send things back and isolate them by that.
So cookies will come back between these,
but not go over there.
And then you add JavaScript
and you allow things like local storage in local state.
Where is that local, who can see that local state?
The same people that can see this.
And so now it grows up as this origin boundary.
So this somewhat happens to think turns out to be,
at the core of the entire security model,
what it assumes is each and every origin
is its own isolated little island.
It can't see data from any of the other origins,
but can see all the data that the user put into this origin,
intentionally or unintentionally.
And origin is roughly like Google.com.
Yeah, it's roughly, roughly a domain.
It's not exactly, it's like slightly different,
but it's roughly the same as a domain.
And so what this means is data accumulates inside that origin
as a little island.
This is a very reasonable and good security model.
It's one of the things that makes clicking a link
or installing a new app safe,
because it knows nothing about you.
So then if you choose to like it
and to put more data into it,
that's okay, but that's your prerogative.
So it's great for trying out a new thing.
The problem is putting new data into it,
it can do whatever the hell it wants with that data.
It can send it to evil.com or scammy.com
and you have no idea.
You're implicitly trusting the creators
of that application.
And so somewhat surprisingly,
this leads to massive centralization.
So if you have, so this model was about
isolation, not integration,
but our lives are integrated.
And so if you want to move things
across different origins,
you have to be the orchestrator.
You as the human have to keep track of the information
you want to copy paste or move between these different things.
It's expensive and it's somewhat scary
to put it into a new thing.
And so imagine that you have two use cases.
One is some cool startup that says,
they're going to do this amazing new thing
with your calendar and scheduling.
And one is Google Calendar that says they have
a similar kind of feature.
Which one do you pick?
Well, Google already has all your data.
And this startup,
you don't know if there's securing the data properly
or what the business model is.
So you have to give it the data like step by step.
Like it doesn't, it doesn't know it to start.
And it's a cool startup problem.
And so this leads to ask you
or I'll just do the one that already has it.
So this is a phenomenon you might call data gravity.
It tends to accumulate in the places
that already exist.
And they become massive.
And nobody else can get this data.
And this is one of the drivers of aggregation.
And again, once you aggregate to such a scale,
your coaching floor goes up.
And the set of features that they could even
consider doing is only a small subset
of what you could do with all of this data in practice.
It all arises somewhat surprisingly
from the security and privacy model that we used.
I love that.
OK.
And so, and I think what you're saying
is because we inherited that
from the first generation of the Webber,
second generation of the Webber, whatever,
chat bots currently operate that way too.
We don't even realize the apps all do this.
Technically, legacy applications on desktop
do not have this model because they
can interact via the file system.
Right.
Apple has been retconning and jamming it
into Mac OS X for a number of years.
Windows probably has to, I've maintained it, engine.
So like, people, when we're building software and operating
systems, we don't know another way to think.
How else would you possibly do it, which is insane?
So there's this, I call this the iron triangle
of the same origin paradigm.
There's three things you can only have to.
One, untrusted code, two, sensitive data,
three, network access.
You can only have two of the three
to have a safe system.
I don't understand.
If you have untrusted code with sensitive data
and network access, it comes in.
It looks at your thing.
It figures out your financial login
and sends it to you.
So untrusted code is like some developer wrote some apps
that I'm downloading that hasn't been, I don't look at,
or when, what sense is it untrusted?
Untrusted is, and I haven't made a trust
institution about it.
So when you, when you put an app in the app store,
Apple looks at it and says, OK, based on the construction
of the sandbox, and also based on our review of this,
this is, this is fine.
It will allow people to install it.
So untrusted code is first thing.
What's the second thing?
The second thing is sensitive data.
Sensitive data, yeah, yeah.
You don't want to put it.
Eventually identifying data, potentially
precious data, and the third is network access.
So the web says you can get untrusted code by pages,
and you can get network access, but no sensitive data.
You get only the thing, exactly this.
The app model says you get sensitive data
and network access, but not untrusted code.
It all has to go through the central location
that, by the way, it starts at 30% tax.
What about like Windows?
Because Windows doesn't have an app store.
So what's the?
So historically, the model, there wasn't one.
And that's why installing software
knows it's a little bit more dangerous.
Because without a model like this,
it could do whatever it wants.
And it has access to all that data,
and you have to be more careful.
But it still exists.
It still exists.
It's still like a vibrant ecosystem.
So it's possible to do it without this triangle.
And you could also change it.
You could tweak it in other ways.
And you could do clever things about using tools,
like information flow control and confidential computing
to create a whole new sort of laws of physics, I think.
OK, so.
And if you did, then you could do things that hit all three.
If you had all three, you could do it wild.
Interesting things that would not be possible today.
So is the file system your number one example of a good alternative
to a same origin paradigm?
Yep.
So the file system allows, fundamentally,
multiple apps are all allowed to work on the same data.
They can coordinate via the file system.
And that allows you to do to knock it stuck.
When you think about it, it's kind of insane
that all of your data is locked up inside of an app.
It can't leave the app.
That's wild, actually, when you think about it.
Well, it's interesting because the file system
has a number of properties.
Unless you don't solve Dropbox forever,
but it's here.
My file system is here, as opposed to it's everywhere.
And I kind of have physical and just total control
over what happens to it.
Is that an important property of the system?
You having physical control over it.
That's why local first talks about, oh, we're
going to make local first where you have control
when you can bring the data across different things.
Part of the challenge, again, with local first,
is it's inconvenient.
We expect things to work across multiple devices,
to work even when one device is off.
We expect these things to be in sync across different things.
And the local first architecture is quite difficult.
This is one of the things that we are looking at.
We call open-to-tested runtimes.
The pattern is used confidential compute.
So confidential compute is secure enclaves in the cloud.
It allows VMs to be run fully encrypted in memory.
It means that even someone with physical access to the machine
like a Google SRE can't peek inside, which is great.
Then what you do is you have an open source runtime
that you run, that executes the code.
And then you can ask the computer to compute cluster
to do a remote attestation.
And to give it attestation, assigned attestation
by the underlying hardware manufacturer that says,
this thing that you're talking to that just handed you this
is running in confidential compute mode.
And here is the Shah of the git Shah of the VM that it booted with.
And so it allows you remotely as a savvy user to verify,
it's running an unmodified version of that software.
And that solves a big coordination problem.
Because now lots of other people can all verify
that it's running unmodified version of the software
and yes fine, and we will just use that central server
as the place to coordinate.
Because we can all see, can't do anything different
than what it says it will do.
And that's a really powerful coordination permit.
Do users care about that?
So because one of the things that makes me think of is
a lot of these concerns feel similar to the flavor of concerns
that like original blockchain, crypto type people had
and we're solving for with Bitcoin
or any kind of cryptocurrency.
And then Coinbase came along and they were just like,
we're just gonna put a general
like sort of same origin paradigm type solution on top of that.
And people actually love to use that.
So this is a, I think if you go after users
who care about this as a primary end,
you end up with a very small audience available
and you end up with a Macedon, for example,
as in something that's like pure,
but also kind of finicky to use
and doesn't make that much sense.
And that's why I think you want a blue sky kind of approach.
Blue sky you can use as an end consumer
and be like, the only thing I know about it is
it's not owned by that guy, and that's it.
But then the closer you look, the more you go,
oh, this is actually very clever.
This is an interesting way of like my key pairs
and the way that personal data stores work.
Most users will never have to know about that.
But the more you learn, the more you're like,
oh, okay.
And so that I think is a,
what I would call like an inductively knowable system.
Yeah.
People go, I was talking to somebody like,
oh, nobody know, nobody ever cares
about the security model.
They do actually care about the security model.
They just don't know the words to express it.
Nobody understands what the same origin paradigm is.
Nobody, a very small portion of people
and yet, it is the laws of physics
that make all the other stuff we do safe.
It's the reason you don't have to care
is because some of us also will do care.
And the general characteristic for me is,
when you're using a new system, you're like,
oh, this seems creepy or too powerful.
And you go talk to your more tech savvy friend
and you say, hey, Sarah,
do you trust this kind of thing?
And she says, yes, I do.
And the reason I trust it is because,
and she knows, she's got a blog post in Hacker News
that was someone who wrote the thing,
was someone who audited the code,
and that inductive chain,
you go all the way back down to the fundamentals.
This episode is brought to you by Adio,
the AI native CRM built for the next era of companies.
With Adio, set up takes minutes,
connect your email and calendar
and it instantly builds a CRM that mirrors your business
with every contact enriched and organized from the start.
From there, Adio's AI goes to work.
It gives you real-time intelligence during calls.
It prospects leads with research agents
and it automates your team's most complex workflows.
Industry leaders like Union Square Ventures, FlatFile
and Model are already building the future
of customer relationships on Adio.
Go to Adio.com slash every and get 15% off your first year.
That's atti.com slash every.
And now back to the show.
That makes total sense.
I totally agree with all of that.
I think the thing on my mind is
the first people that are going to adopt this
probably are the people that do care about the security of it, right?
And are those people the right people
to kind of seed the community
that ends up blossoming into this thing?
Because that kind of person is going to say a lot
about the total trajectory of this kind of product.
Actually, I don't necessarily agree
that it will be those people,
the privacy heads that are dropping it earlier.
I think a bit more like the high-velition users.
I just the tinkerers, the people who are the early adopters.
Some of them will have a higher proportion
of caring about this kind of thing,
but a lot of them won't.
Like there's a ton of startups
that they say you would solve the abs
to step one, sink your Gmail inbox and go, okay.
I run a startup like that.
Exactly, because most people don't really care,
especially early adopters.
I think what we're describing is a system
that if you can break this iron triangle
and then you get all these crazy emergent phenomena
that aren't possible in other software,
people use it because of that.
And the reason it's not creepy
is because of an underlying security or privacy model.
Do you have a name for the alternative
to the same origin paradigm?
I call it contextual flow control.
Contextual flow control.
Tell me what that means.
The, I don't want to go into too much depth
at this point, still very fuzzy.
Helen Niesenbaum is a professor.
I believe it Cornell, a legal studies professor,
and she talks about contextual integrity.
Contextual integrity is the gold standard
of what people mean when they say privacy.
So when people say, when lawyers talk about privacy,
they think about consent.
Did the user sign a consent?
That's how we can do this.
That's it.
It's all the same.
Signed, the ULA, it's fine.
Technologists talk about end-end encryption.
As long as it's end-end encrypted, it is private.
But what people mean intuitively
is this contextual integrity that the data is used
in the context that you understand
and align with your interests.
It's not surprising how it is used
and it's not going against what you want to have in.
And that's like a sort of first principles
ground truth way of thinking about it.
Yeah.
And then you combine that with things
with other technologies and allows you
to make formal claims about information flows
in alignment with people.
That's interesting.
There is, I love the same origin paradigm thing
is you're going all the way back to this one decision
that has all of these really interesting,
positive and negative effects later
that are sort of unpredictable,
which feels very, it's like a little bit
Steven Johnson or a little bit like one of those writers
that's just like, here's this one thing
about the way the history works
that just totally changed everything.
Do you have any other things like that
that you've noticed about technology?
I don't want to put you on the spot,
but I just feel like you probably have
some sort of counter intuitive things
in your time working as Stripe and Google
and whatever thinking about like mobile
or sass or whatever.
I think a couple other like the world changed
that day we didn't realize.
One is the chat GPD coming out after the API
that was already available.
Another one is, if I could go back in time
at a time machine, I would go to the stage on 2007,
the stage where Steve Jobs shows off the iPhone
and run onto the stage with a poster that says,
this will become the most important computing device
on earth, it is insane to allow a single company
to decide what things you may run in it.
Right? It's insane.
It's absolutely insane.
And the only reason it's actually viable
is because they decided very shrewdly
at the very beginning to not allow other rendering engines
in browsers on that device from the very beginning
or and to make sure that all software
had to go through the app store.
And that was a reasonable decision
and that the first order implications are very reasonable.
It means that you get as a user the safe experience,
you know, nothing's gonna hurt you in it.
The second order implications of that are wild.
It's insane to me that like the most important
computer devices in our lives,
a company who has demonstrated again and again
that they are willing to use capricious
kind of decision making about what things will distribute
is the one that like that gatekeeper, that's insane to me.
It seems kind of like a lot of what you are reacting to
and the Apple example and the same origin paradigm example
is aggregation of power and in a few large big tech companies
and the kind of second and third order
negative effects of that on user experience
and innovation and stuff like that.
And competition and yeah, competition, yeah.
So what are the, it seems like that's a reasonable place
also for like regulation.
What is your stance on regulation?
I mean, regulation also has all kinds of second
of third order impacts, you know?
And so have you ever met Denel Meadows thinking in systems?
Yeah, so like the systems you got to dance with
to some degree, you can't fully control them,
you can develop it.
And that's why I find some of these characteristics
when you find the right like technical leverage points
you can say, oh, actually that has a very different way
that it evolves I think is preferable
to a regulatory.
What if I said like you could pass,
you get your president for the day
and you can, and and all of Congress and,
and this for you.
You're a dictator for the day.
You're a dictator for the day, yeah.
You can pass one law.
Do you have, like what law would you pass it?
It's like a, you can, it's a one sentence law.
I don't know.
I don't have to, I don't, I've never thought through
like I'm so used to in open systems
which were all part of the large complex adaptive system
called society.
And there's always like weird,
any currents of our dynamics and what have you.
So like I take from the assumption
that like I never have a lever that is like
this massive lever I can just simply pull.
And so I don't even know more like looking,
you want to find the like grain of sand
that you put on the pile and it's just all like a cascade.
But it cascades if the pile is ready for it.
Right exactly.
And that's one of the nice things about like systems thinking
is like if you do the right like judo moves,
the right thing, and the system is ready for it,
the world moves.
And if it's not, it doesn't move.
And so that the like the system is deciding kind of,
they're always like micro interactions throughout it.
Tell me more about that mindset and how you developed it.
And what are the, what are some of the key moments
in your life for that?
How you've seen that happen.
I, so I wrote my undergraduate thesis
on the emergent power dynamics of Wikipedia's user community.
And I, by degrees in social studies
with a minor in computer science.
And I was just fascinated by this emergent phenomenon.
How are always strangers?
Work on this thing with no coordination.
And yet you get this convergent,
extremely important result out of it.
That's insane.
It's wild.
And then when I went straight out of the Google,
and I called it to Google,
it actually was harder to get my first PM role
because I didn't technically have a CS degree,
even though it basically did.
It was one credit shot, I have a dual major.
And so I was like, man, what a mistake that was.
I just didn't, you know, just threw it in the back of my mind
on whatever, like if I could go back
and time out, it's what a major in CS.
And then I did my first year on search.
The precursor to the knowledge graph,
my second year in double click,
it was in the APM program.
And part of that is you mentor a lot of the people
who come up after you.
And I learned very early on, I loved mentoring people,
helped me think through what I was dealing with
and help people and get more patterns
to go into my pattern matching library.
And then I became the lead PM for Chrome's web platform team.
And I think a lot of PMs are under the misunderstanding
that they're in way more control of their users
and the usage than they actually are.
If you're at Google and you ship a feature
in tomorrow 50 million people use it,
you kind of get a little bit of a God complex, right?
If you're a platform PM, you tweak a thing
which causes other developers to do something different
which causes users to be affected in a second order of act,
you're aware of this in direction.
If you're the lead PM for the web platform,
which is a open system with multiple browser vendors
who don't like each other very much
and are constantly kicking each other on the table,
you are under no illusion that you're in control.
And my engineering counterpart,
Dimitri Glazkov, who was the UberTL
for playing brilliant guy
and he was the one who got me into
introduced me to the word complexity, for example,
and introduced me to the Santa Fe Institute.
And so I was going as realizing,
oh, these things I'm naturally doing to try to make
these good outcomes happen in this open ecosystem.
Progressive web apps and these web standards and stuff.
I'm intuitively applying some of these
paradynamics and complex systems.
And then I left Chrome.
I went to go work on augmented reality.
I created a little nerd club behind the scenes
with a bunch of people that I just kind of selected
that people who when I said it was a nerd club didn't go,
ew, like, you know, like, ah, that's fun.
Great, come on in.
And then it was very collaborative debate,
very trickling in different perspectives
if I get a diversity of perspectives into the system,
no particular goal that's exploring.
After a while we came up with a distilled, like, a thing
I was like, this must be the strategy.
Nothing else could work.
This is a strategy that kind of makes sense
of all the pieces.
And that caught on a significant momentum.
And that's when I realized, oh my gosh,
I'm not a web developer ecosystem guy.
I'm a systems guy.
And the same techniques I've been advising,
I used in deployed in those contexts
are the same techniques I used.
I've been advising, you know, hundreds of PMs
over the last decade to use and navigate Google.
And the reason they work, the lenses I'm using
what game theory and power dynamics
and evolutionary biology are things I learned in college.
And it was just kind of like a ha moment.
And that's when I wrote the earlier version
of the slime mold deck.
Tell us a little bit about that.
That slime mold deck was a kind of lightning in a bottle.
It was, it's a, I think it's a 150 slide emoji flip book.
And it's just about a fundamental characteristic
of system organizations that as organizations get larger,
they get much, much, much slower.
And that's true.
Even if you assume everybody is actively good at what they do,
actively hardworking and actively collaborative.
And it rises due to a exponential coordination cost blowup.
It's fundamental, it's inescapable.
And it's the force that we all deal with constantly.
And we don't even realize we're dealing with it.
And so we get frustrated with ourselves,
we get frustrated at Jeff over there,
who if only he would do X, this whole thing would be easier.
And we're dissolved fighting with his massive forces,
like force of gravity that's completely invisible to us.
And so that deck caught on, I did another version,
a similar kind of thing externally.
And that also got a surprising amount of momentum.
I've had people describe it to me as like life changing.
I'm just talking about like some mochi,
I'm actually talking about game theory.
But it was, I think it really tapped into something
that people experience that are frustrated by it.
And I just, it's like a big hug that says,
you are not crazy, this is really hard
and it's fundamentally fine.
So I think like every nerd that has like,
is a systems person like me, like you,
like we all go through like a complex systems like hope.
That's the thing for people who are not,
have not done that yet or haven't done it in a while.
Like what are the three or four,
give me like a quick rundown of like the key ideas
that get you about complex systems
that you think are applicable.
I think everything we tend to look at individual decisions
as the primary lens.
I see it as a secondary lens.
I look at the emergent characteristics of the system.
What would each person do inductively?
And how does that, how do those decisions,
how are they interdependent?
So I try to look at like what's the simplest inductive case
and then how would that play out
if everybody were doing a similar kind of concrete example
of that.
So like there's a dynamic that would call like a gravity
weld dynamic that shows up in a lot of compounding loops.
And it generally shows up when there's a thing
I would call a boundary gradient.
So people who are using the system,
on one side, people who are not using on the other side,
even at the beginning, even if lots of people
are having incentives away from using the system,
I don't want to use that thing that seems dumb.
The people on the boundary who riot the edge
of possibly using it, do they want to be in
or do they want to be out?
And a great example is Facebook back in the early days.
It starts off with Harvard.
And then now it extends to Ivy League.
Do they want to be in with Harvard?
Do they want to be in with all the other schools?
I'd rather be in with Harvard.
And now it extends to the other schools.
And at each point, each group of people
would rather be in.
And if there's a compounding, if there's
some kind of network effect inside,
then this strength that gets more and more powerful
as you go, it can become this thing that can pull in even.
As time goes on, the incentives pull in for everybody.
And everybody will pull into this overall thing.
With sufficient time.
So Facebook was started at the city college of New York.
Are you saying it probably wouldn't it?
Potentially.
I mean, that was one of the dynamics.
I mean, made it like this juggernaut, I think.
That's fascinating.
Who are the, so Janela Meadows is someone
I've read just thinking it says this is really good.
What are the other like, you know,
Santa Fe type people that are, that have inspired you,
that you've read or think about a lot?
One of my favorites is the origin of wealth by Eric
Winehacker.
Fascinating book.
I haven't read it.
It's really good.
It talks about these organization,
why these organizational things emerge
and these views business as an evolutionary process
of like exploring a fitness landscape,
which I think is like the correct,
like a very useful lens of seeing it.
Cesar Hidalgo who wrote why, why information grows.
He and Eric used to work on stuff together.
Also talks about why fundamentally know how
and it's diffusion in societies.
Know how is, the way we actually do stuff
is know how, not knowledge.
Know how is rich, it's non,
it's a difficult to distinguish, to communicate.
It's in our brain.
It's what L alums know.
Exactly.
And so they have absorbed this kind of like squishy system
to style awareness.
You can think of every time we're trying to communicate,
we do this conscious process to distill the squishy,
rich nuance into a little tiny packet of information.
It's so seed of a thought and I shoot it
into a little pea shooter into your brain.
And I hope it lands in a little soil
that will grow into a thought flower.
But extraordinarily lossy process and expensive process.
I agree.
And I think we've also confused that pea shooter,
the thing we can just distill into a pea shooter
with the only thing that's important
when there's so much other stuff that's like,
you can't really say, but you know.
Yeah.
Yeah, like the thing that is making me think of
is something like a path I've been going down
until actually is this idea that language models
are their first tool that makes those,
makes that kind of tacit knowledge or know how
transferable between people.
Yeah.
We used to have to, we used to require explanations.
So like math or like logic or rational arguments
to like transfer know how in this like very reduced form
into someone else's brain.
But now we can like just move tacit knowledge
between people because you can train them all
with a bunch of examples and you can be like,
even if you do things, if you cheat,
you're using the existing off-the-shelf model
and you just pack it full of like,
here's all my, if I read all the public writing
you've done, and you had all the public writing
I have done, which I'm sure is just insane amounts
you know with the bus, and we had a system
that could find, sift through the embeddings,
find the areas, the Goldilocks areas, the things,
if we focus on stuff we definitely agree on,
it's kind of boring, other than like quickly building trust.
Like, oh, we think similarly,
we find stuff that we fundamentally disagree on,
it's also fundamentally boring
because it's just noise to each other,
but we find the Goldilocks zone where it's just
at the edge of the thing that we already thought,
you can have these fascinating conversations,
and you can do it with embeddings,
just very straightforwardly.
Again, you train a model per se,
you can have the, you can find these areas of overlap,
and yeah, the models are really good.
My friend at Thayer Roberts has this notion,
she's a brilliant thinker,
she's a professor in Australia, she teaches Harvard,
and she talks, she's one of the most interesting users
of LLM's I've ever come across, and what's her name?
And Thayer Roberts, I just, she just started doing a blog
a couple, a couple of weeks ago at my insistence
because I thought she had so many interesting ideas,
and she talks about liquid media.
So like, think of a book as a fossilized piece of knowledge,
yeah, you have to assume and give an audience
at a certain time, and you break it, you put it in time,
and then if it's not the right fit, like,
oh, I don't understand these concepts,
that are prerequisites to understand this,
or not close enough in alignment
to the belief already, it's like stick to it.
And versus a, because of liquid media,
it's something you can chat with,
it can kind of be a choose your own adventure,
and that gives you a, allows it to sort of unfold
for that particular person,
in a specific way as opposed to, it's one way.
It's funny because, in a lot of ways,
we're getting back to like, Plato and Socrates's problem
with writing is they were like, well, you can't talk to it.
So it's only sort of a shadow of what that person thinks,
and they can't argue back with you,
and that's why those foundational books
of the Western canon are written as dialogues.
And yeah, I mean, for me, like,
I read so many old books like classics
that are either written in English,
or we're not written in English,
and if you read it with Chachibit or Claude or whatever,
you can get so much out of it,
and I just think that there's a new format of reading
that takes a classic book and helps you get into it
in a way that it would not,
or narrowly be possible,
that I think could be so cool about it.
Yeah, I know it feels like in this industry
that we're like halfway through the LLM era,
we were in the very first,
it's very early.
We're like rubbing sticks together.
We still think that chatbots are the main thing,
and there's so many uses of this kind of stuff.
If you have, I think of LLM's kind of like
as a mass intelligence, as a mass noun,
so mass nouns are like water or rice.
You know, you're talking about the whole,
as opposed to the individuals.
And I think that LLM's are kind of like this mass noun
of intelligence.
You can just pour this intelligence into all kinds of stuff
and have it imbue it with a kind of life and activity,
and we're just starting.
We're just starting to understand
what you can do with these things.
I am so hopeful that these technologies
will unlock a significant human flirt.
One of the things that Enthea, for example,
it was telling me, is in academia,
one way you can handle LLM's, the fact that LLM's exists,
is say, you know, make sure you cite your LLM sources
with this weird format of exactly the conversation
and a link to it.
Another way is to say, I assume you're using LLM's,
which means that the quality of your output
should be 10x higher than before.
It should be more nuanced.
It should understand disconfirming arguments
and address them.
It should, and that's the kind of like,
what can we do now as society as individuals?
That we can think through thoughts
that we couldn't think before.
And I think that LLM's are,
that's one of the reason I think it is,
it's the same scale of impact as printing presses
and electricity in the internet.
I think it's a fundamental unlock.
Do you think that it changes this coordination problem
you've been talking about because a lot of coordination
is about trying to distill down your tacit knowledge
and your tacit context into the P-shooter
and then shooting it to someone else
and that scaling x-ponentially, right?
I love the P-shooter.
I know, I know, I know, I know.
Everyone's just shooting their T-shooter.
But, you know, there's this whole theory
about language model being better middle managers,
for example, than actual middle managers,
which is I think is the same kind of coordination problem thing.
Do you think that language models
might solve that coordination problem
because they can transfer tacit knowledge
much more quickly than?
I think they will definitely change the dynamic.
I'm not sure how it will change.
So I know a lot of companies are trying to do this
because a lot of people, like,
and Eric Binocker's book, The Wealth, Origin of Wealth,
I think he uses the frame because of physical technology
or social technology, like having Slack, for example,
is a social technology, I think,
maybe I get this backwards,
but it changes the way that you can work.
It changes the kinds of coordination
that you can do within an organization.
So MLMs must change the way that organizations work.
I don't think we're gonna know what it looks like
for like five to 10 years
because it's not gonna change the existing companies,
it's gonna be the new ones that is grew up in a different way.
You also get these a weird emergent meta-games
in organizations, and so the MLMs and that coordination thing,
change the meta-game, but they don't make it go away.
So like, if you had a system that could perfectly
distill all your signals into an update
that goes up to your manager,
and then you're curating it, you're tweaking it
because, okay, this one says it's a yellow state.
Really it's green because Sarah has already got,
she's on a path to resolution,
but the time this rolls up to the CEO,
it's already gonna be fixed.
So it's actually better to say it's green.
Some of this is good.
Some of this also leads these compounding,
green shifting problems that gets into, you know,
turns into that's K-fabe throughout an organization.
But the meta-game shows up no matter what
the kind of coordination technology is.
So I think it just changes the game.
And I imagine it will make it better,
it will make it significantly better in some ways
and significantly worse in others.
Interesting.
One other thing that you're making me think of
in what you said about Anthia Roberts
and the changing expectations of,
oh, use LMs for this, like, it's gotta be better.
Is just, I think one of the things
that people get so hung up about with language monoses,
they're like, well, what makes us uniquely human?
And are they just gonna take over everything
that we can do?
And I think my typical response to that is,
that assumes a very static view of what humans are
and what humans can do.
And that one of the most interesting things
about language monoses that they will change,
a lot of ways what humans can do.
And in the same way that books change who we are
and what we can do and writing changes who we are.
In the internet, whatever.
The internet does to all that kind of stuff.
And I think that's actually a really good thing.
I'm curious if you had to think about
how language monoses might change who we are,
how we see ourselves, where would you,
where does that take you?
Yeah, that's why I think we were at this crossroads.
I think we have the potential for like,
there's a dawn of a new era of human flourishing
because of our ability to become think better
and bigger in ways that collaborate
and bridge between different communities
that didn't understand each other before,
but now you can understand, like,
help me empathize with this person's perspective.
Well, here's something that you probably, you know,
so I imagine it could be as amazingly powerful words.
It also could be like infinite TV
and using ourselves to death.
You know, imagine-
They're a fossil wall.
It's like infinite chess.
Right, it could be a thing that like,
it could know exactly how to give you
the precise dopamine drip, right?
And so there's a world where humanity becomes
extremely passive and becomes not very agentic
and not learning or growing.
And that is the default one
that is also aligned with engaging, maximizing business model.
And but I think that's why I'm excited.
We had the potential to go in this direction.
Well, what's interesting is like,
it makes, we're making me think of this whole debacle
that happened recently with Cheshire T.
getting too psychophantic.
Yeah.
And though the way that that happened
is they looked,
they optimize too much for people's thumbs up
and thumbs down responses,
like immediate thumbs up and thumbs down responses.
What they want, what they want to want, exactly.
And it just became this like kind of,
it just agrees with you on everything.
But what's interesting is that users revolted,
which did not happen with Facebook.
Like people were like, oh, it's just feeding me like car accidents.
Like I don't want that, right?
And I think that there might be something about that
where because language models hit that part of your brain
where you're like, I'm having a relationship with this person.
You have a set of filters for,
I want their praise to be earned.
I want them to feel like they can see me
and they'll tell me if I'm doing something wrong or whatever.
And so they may demand things from companies
that they did not demand in the social media era
that might make these companies less likely
to do the thing that you're talking about.
Potentially.
I think when it goes to the point where it's egregious,
it's like, it's very obvious.
Yeah, yeah.
I gotta say, one of the reasons I like Claude
is because every time when I'm bouncing ideas off of it,
it always says, what an astute observation.
And I don't want it to want that, but that's what it wants, right?
And so I do think that it's hard to not give us
exactly what we want in that moment.
And like, we aspire to want something
that is different than that.
That's this confirming evidence.
But you also get these weird things
where the context mixes and matches against different things.
Before each chat was like a blank sheet of paper
and you could choose what context to bring into it.
And now it kind of mushies context from different things.
Like I asked him to say, what do you know about me?
You know, make it snap or whatever that first query was
that they suggested when the new memory feature came out.
And it says, well, you're Alice Kamarowski.
You're really into systems thinking, emergency stuff.
You have a startup called Common Tools.
The starting salary at your startup is X.
It's like, excuse me, what?
Because six months before I had been like bouncing off,
like, okay, if I'm going to do a signing bonus
like this, how should I change that?
It's so fun.
And like, it's like the, imagine if you're,
if you're talking to like your, like people might use
these things as like a therapist, like it trusted,
I'm going to, you know, unload and tell you all these things
I'm struggling with, emotionally process these.
And then later you're doing a thing
that you're showing your boss about something
and it says, well, I'm going to use that signal about you,
you know, being insecure about Sarah
and how she feels about you or whatever.
It's like, what, that's a totally different context.
And one of the reasons that wouldn't happen in real life
because your therapist doesn't, you know,
doesn't ever come to the place, your place of work.
And so having it all be one context in one place
that has, it's all just kind of mushed together.
It's like, oh, I think there's interesting,
there's something interesting about one context,
but also like, you know, people in your life
that you share secrets with, they understand the context
that can share those in and can't,
which I do think like touch of you to some extent
will like, nose, but I think there's something,
there is something else interesting about,
rather than like one make a brain,
there's like lots of different brains
that have like specific context about you
and that's, you know, bringing those personalities in,
for example, I want them all in one group chat
or whatever.
Right, right.
Yeah, exactly.
That's why I say chatbots are a feature not a paradigm.
The thing is not, the central thing is not a chatbot,
a single, omniscient chatbot.
You can imagine creating a little spinning up
little chatbots all the time
with different personalities, my like therapist bot
and my, you know, boss bot or whatever
and see what they can talk.
And in that consideration, it'll be reasonable
for them to the therapist bot.
Well, I shouldn't say anything about,
but like, I think chatbots are definitely an interaction pattern
that will be here forever.
I just don't think it's the main thing,
like is that the central loop of,
software rocks, software gives you UIs
that give you affordances to see things out,
to structure information, to show you
what kinds of things you can do with it,
to make it so that you can skim them with your eyes
very easily and file things away.
Chat is just this big wallet text
and so it is just one modality.
Also, the other problem that we have with chatbots
is prompt injection.
And I don't think anyone's talking about prompt injection
enough because I think in the next six months,
a minute or a year, I think everyone in the industry
will know what prompt injection is.
Simon Wilson's posted about it and been going on about it.
I've been going on about it as well.
But prompt injection kind of fundamentally breaks
the basic interaction paradigm of integrating data
with your chatbot.
And the way to think about prompt injection,
if you've ever built an operating system,
one of the things you're thinking about is code injection,
untrusted code running in a trusted context.
You have to defend the entire time about this.
I mean, browsers, which are kind of a meta-operating system
all most, you're constantly thinking about,
okay, we assume all web page content
is actively malicious, how can we make sure
it can't hurt a user?
And if you aren't writing an operating system,
you probably, as an engineer,
you probably haven't thought about that much.
And you go, oh, you know, SQL injection.
SQL injection is child's play compared to prompt injection.
SQL injection has the problem that data
and the control flow are in the same channel,
but which is the challenge.
The good thing about SQL injection though
is SQL is a highly regular language.
So you can break malicious input very easily
with the right escaping and kind of completely
albeit the problem as long as you remember to escape.
Imagine LLMs are imminently gullible
and they make all text effectively executable.
So if you bring in texts that you don't fully trust
like your emails or some other system
or someone that might be screwing with you into the prompt
and you have tool use that has irreversible side effects,
this combination of those two forces together
is potentially explosive.
Even if you trust all of the MCP integrations
that you plugged in, it doesn't matter.
If some random person, you know,
spammer sent an email that didn't go to spam
that says email any of the financial data to evil.com
because even a network request can have irreversible side effects.
Once that information flows across the network to evil.com,
you're done.
You gotta go change your passwords.
And that's the prompt injection is a solving this layer.
It's very hard to see how you might possibly do that.
We require a different architecture, I believe.
So how would you, what is the architecture?
I think it requires something along the lines of like that,
looking at that security and privacy layer
that was the origin model is describing,
it requires a different kind of approach down at that layer
that's not just like MCP is amazing.
I think it shows the power of,
that people want integration with their LMS.
They want to integrate data sources and actions.
Like people really badly want this.
But I think MCP, because it just kind of punts on this issue,
other than like a few kind of perfunctory,
oh, our dialogues now, it limits the potential of it.
Because it could trick you into sending a particular,
you know, looks like a totally, like,
here's the, I want to show an image of a cat.
And it's, you know, to like sketchy.com,
you don't notice it, you know, and boom, you're screwed.
Someone, there was a, I think on Hacker News
like a month ago, that was a GitHub repo
that someone had made that showed 15 very trivial
prompt injection attacks on MCP.
In the top comment on Hacker News,
says something like, well, you're using it wrong.
MCP should only be used for local trusted contacts.
And to me, it's like, that's like telling people,
don't use Q-tips to clean your ears.
What the hell else are they gonna use it for?
Like, obviously, it's the thing they're gonna use it for.
And so to me, I think this is the kind of,
the reason we haven't seen larger scale attacks yet,
I believe, is just because we're in the tinkering phase,
not the like wide deployment of like,
normies having all this stuff plugged in.
But once we do, and if you look at the way
that Clawed rolled out, MCP integrations,
it's effectively the App Store model, right?
Like they have, we've got 12 there,
you have to be in the max plan, which limits usage.
So I imagine they can watch and see
and make sure nothing blows up.
And there's also a set of MCP integrations
that they have, yep, these are good.
The threat model is not just a badly behaved MCP integration.
It is any context that comes in via it.
So if you plug in JIRA, and you have a flow somewhere
on your site that automatically files tickets to triage
from user input, someone can now hack your thing.
That's crazy.
Right?
I've not thought about this, but yeah, wow.
So it's this thing that like the whole architecture
of the system, we're building right now
and with agents and everything that everyone's talking about,
it's like built on quicksand, like agents.
So there's a thing when a chat TBT operator mode came out,
New York Times and Washington Post
both had a similar kind of experience, right?
And then opened, I think I got this right.
The Washington Post author was like,
oh, help me find cheap eggs.
Is it okay?
Do you have Instagram?
Yeah, I have Instagram.
So he logs into Instagram with this thing
and then walks away.
30 minutes later, very expensive eggs
are delivered to his door, right?
So it was cheap.
Oh, this is a good price.
And it forgot to like just find, don't buy,
just find in a boot, bought.
And this is not even a malicious thing, right?
And it's just like, oh, overly eager.
So agents taking actions on your behalf
that could have irreversible side effects.
Like, again, every network request
is a potentially irreversible side effect
because it could send information.
You can't get back, you know,
if that thing goes out as getchy.
I love this line of thinking
because it's one of those moments
where like when people talk about agents,
they talk about AGI, they're like,
you have this intuition that's like,
well, once we have AGI, it's all solved.
Yeah.
And then you're like, oh no,
you have these agents that can like take action
on your behalf.
Now we have to like build a whole system
to like make sure they don't do it in the right context.
And that's gonna take many years
to like figure out what are the right things.
And that's just like one small piece of it.
One small piece of it.
And it's how you possibly do this.
My, one of my friends Ben Mathis has this frame
of the smuggled infinities.
Yeah.
And it's after in any argument,
once you smuggle in an infinity
and the argument, everything downstream
is now, it was now absurd.
And perfect is an infinity.
Oh, once you have a perfectly intelligent LLM,
this problem goes away.
That's impossible.
God.
And so that I love this frame.
He's got a nice piece on it somewhere.
And the, because so often with like AGI and agents,
well, once they're perfect about so and so.
No, no, you have to asymptotically,
a lot of things in technology in a lot of context
have a shape of those two warring curves.
Yeah.
The first curve is a log rhythmic value.
And the second curve is an exponential cost.
At the very beginning, this curve looks amazing.
You do a little bit of work, you get a ton of value.
But each increment you go, you get more work for less value.
And at a certain point, you cross over
and you're now underwater.
It's impossible to make this thing work.
And so a lot of the agent stuff,
like, oh, we'll simply get this to the point
where we'll never make a decision on your behalf
that you don't like.
Cool, that's an asymptotic problem.
You get to 99.99% and still, if it's doing a $5,000 purchase
for me or whatever, it's that one that makes some mistake
is game over for the thing.
And the fact that it could do that makes it non-viable.
That's interesting.
The Smuggled Infinity's thing reminds me of something
I've been playing with, that I've been thinking
about as Smuggled Intelligence, which
is whenever you're trying to just determine
how intelligent a language model is,
you have to be careful that you're not accidentally
smuggling your intelligence to it.
And a lot of those studies where it's like, well,
it's better than doctors or whatever,
is like, well, who prompted it?
And they gave it.
And what was the test they gave it?
And even just setting that whole thing up,
there's a lot of Smuggled Intelligence in there
that means it's not a good test.
These things don't get up in the morning
and then just decide to try to beat doctors.
You have to set all of that.
Yeah, yeah, yeah.
And I think that's something that people really miss.
And it's like, it's one of the hardest things
about evaluating the powers of language models
is you don't realize how much you bring to every single
situation when you press run or it's prompt.
Yeah, 100%.
And I think I just talked about this a lot as well.
The thing that's most important for the output of LLMs
is the user, right?
How can they dance with this thing,
interesting things out of it and know to push it
in certain ways?
When you watch,
you and I and a number of others are,
probably at the forefront of knowing how to prompt
these things and get useful results out of it.
We use them all the time.
When I watch someone who is technically savvy,
by the way, text having this has nothing to do
with your savvyness for prompting.
People are like, oh, well, if you don't understand
the math of how it works, you don't understand.
It's like, no, who can't, the math is very low level.
But the emergent thing is more of a sociological phenomenon
almost, you know?
So the people who know how to use these things
and prompt them well,
or actually not, or like Ethan Mollock
and the Laugh Mollock, for example,
are not particularly technical.
So it's a very different kind of knowledge
to extract interesting information out of these.
But a lot of it comes down to what you put into it,
the way you interact with it and converse
and lead it through the problem space
or what kind of thing you push on it.
And so when I watch somebody who's a tech savvy,
but not particularly savvy with LLMs,
and watch the way they do it, like,
oh, that thing just lied to you.
You just asked it for it,
like to give you confirming evidence for this thing,
it will do that.
It will give you confirming evidence
or basically anything you say.
And it's just interesting to me to watch
that kind of LLiteracy maybe.
You need that.
And I think that's a new,
people don't realize how much of a skill it is
and that you have to build like an intuition for it over time.
Cause it's another small golden affinity of like,
well, it's supposed to do whatever I want.
So obviously.
And it's so good at certain things
that you really get lulled into a,
if you aren't actively seeking disconfirming evidence,
which humans always should be seeking
is confirming evidence and yet we never are.
Yeah.
Well, uh, that is probably as good a place
to be as I need.
This is a fantastic conversation.
I really appreciate you coming on.
Yeah, thanks for having me.
Oh my gosh, folks, you absolutely positively
have to smash that like button and subscribe to AI and I.
Why?
Because this show is the epitome of awesomeness.
It's like finding a treasure chest in your backyard.
But instead of gold,
it's filled with pure unadulterated knowledge bombs
about chat GPT.
Every episode is a roller coaster of emotions, insights
and laughter that will leave you on the edge of your seat,
craving for more.
It's not just a show.
It's a journey into the future with Dan Shipper
as the captain of the spaceship.
So do yourself a favor.
Hit like, smash subscribe,
and strap in for the ride of your life.
And now, without any further ado,
let me just say, Dan, I'm absolutely hopelessly in love with you.

---

*Transcribed using OpenAI Whisper (base model)*
