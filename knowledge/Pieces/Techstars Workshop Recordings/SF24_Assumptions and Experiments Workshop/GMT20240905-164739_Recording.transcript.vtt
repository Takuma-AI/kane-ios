WEBVTT

1
00:00:03.850 --> 00:00:07.609
Lilliana Robinson: I'm going to double check that. You guys can see the screen before I start this time.

2
00:00:09.560 --> 00:00:11.439
Lilliana Robinson: Is everyone seeing this

3
00:00:12.570 --> 00:00:14.879
Lilliana Robinson: new tab. Okay.

4
00:00:14.990 --> 00:00:16.450
Lilliana Robinson: Dave, you're saying it.

5
00:00:17.980 --> 00:00:19.070
Lilliana Robinson: Okay, cool.

6
00:00:19.600 --> 00:00:23.199
Lilliana Robinson: So today, we're going to talk about assumptions and experimentation.

7
00:00:23.250 --> 00:00:48.060
Lilliana Robinson: So the Texters team who put this deck together, they call the assumptions hypotheses. It's the same thing. Neil and I always call them assumptions, because we really like the riskiest assumption test which I put in your Pre work packet. If you didn't click on that article. I highly recommend going after this workshop and reading it. It's like a 5 min read but riskiest assumptions are the same as

8
00:00:48.220 --> 00:01:07.899
Lilliana Robinson: hypotheses that you're making. But assumptions gives gives the implication that you are just assuming it, which is the correct implication. Hypothesis sounds a lot more official when it's all really just speculation and assumptions. So you'll see the word hypothesis in here. But I've renamed it to assumptions and experimentation.

9
00:01:08.300 --> 00:01:30.640
Lilliana Robinson: So like, I just said, before we started today is about learning a skill. So I added this in here, so that you guys understand the objective today, recognizing assumptions and designing experiments to validate them. That's all I want you to take away from this, and we can kind of chat and go through it together. And then throughout program, you will design experiments and start validating these assumptions like

10
00:01:30.640 --> 00:01:39.726
Lilliana Robinson: the whole program. That's really a lot. One of the best things you can do is come out of techstars with a lot of validated assumptions that you're sure about your business.

11
00:01:40.930 --> 00:01:43.239
Lilliana Robinson: I want to start with a warm up question

12
00:01:44.990 --> 00:02:03.709
Lilliana Robinson: based on the riskiest assumption test that you guys did in your Pre work packet. What is one hypothesis or assumption that you have about your company that you'd like to share? Does anybody have one that they're thinking of? It does not have to be your riskiest assumption? Just any assumption or hypothesis. Yeah. Agent, limit.

13
00:02:05.600 --> 00:02:25.790
Dominique (CEO) & William Stone (COO)_Agent Lunar: Yeah, one. I think it's it's probably up there with the riskiest. But we. We have an assumption that small business owners will be willing to essentially trust the AI to outsource that component of the work from either themselves or from a freelance or agency.

14
00:02:27.730 --> 00:02:32.460
Lilliana Robinson: And the whole product kind of hinges on that right? Because if they're not willing to do it.

15
00:02:32.830 --> 00:02:45.589
Lilliana Robinson: you're not really going to be able to provide them with that service, and you'd have to pivot. I think that's a great example, because we need to make sure that that's validated very soon. I'm interested. What your riskiest one is, if it's not that one. But we can talk later.

16
00:02:48.050 --> 00:02:50.050
Lilliana Robinson: Yeah, that was a great answer, though, Dave.

17
00:02:52.573 --> 00:02:53.479
Dave Scott: So, and

18
00:02:53.970 --> 00:03:04.610
Dave Scott: I didn't raise my hand. But I'm happy to give. You know our ours is ours is, if they, if we build it, they will come right. And so you're you're talking about a legacy of playing these games for 50 years in person.

19
00:03:04.720 --> 00:03:08.889
Dave Scott: and we're theoretically now saying that we're going to allow you to

20
00:03:09.050 --> 00:03:15.399
Dave Scott: play it online. And so you know, there's a huge assumption that people are going to be like, Yeah, that's cool. I'll do that.

21
00:03:16.160 --> 00:03:40.630
Lilliana Robinson: Yeah, that's a huge platform shift, right? Like assuming people who play tabletop games would just want to go on. The computer is a huge assumption I'm very interested in, like what you might have already. That kind of backs up that assumption. Or if it's just straight up, unproven, which is fine, too, most of you. By the way, most of your assumptions will be unproven because we're catching you so early. So that's like normal. You don't have to be ashamed.

22
00:03:41.150 --> 00:03:42.130
Lilliana Robinson: George.

23
00:03:44.520 --> 00:03:46.709
George Reed - Noetic: Thanks. Yeah, I I think

24
00:03:46.730 --> 00:03:49.459
George Reed - Noetic: probably the most fundamental assumption

25
00:03:49.760 --> 00:03:51.879
George Reed - Noetic: that's the most risky would be that

26
00:03:52.140 --> 00:03:57.680
George Reed - Noetic: companies actually really prefer, like less biased insight when they're conducting market research

27
00:03:57.900 --> 00:04:02.369
George Reed - Noetic: cause in my experience a lot of the time it's about sort of

28
00:04:02.410 --> 00:04:05.250
George Reed - Noetic: finding validation for preconceived ideas.

29
00:04:05.570 --> 00:04:07.409
George Reed - Noetic: And that's a big. I mean, that's a big problem. So.

30
00:04:07.410 --> 00:04:08.100
Lilliana Robinson: Yeah.

31
00:04:09.300 --> 00:04:09.910
George Reed - Noetic: the.

32
00:04:09.910 --> 00:04:18.889
Lilliana Robinson: Confirmation bias. Right? That's like, why, a lot of them hire the consultants. They don't actually want the truth. They just want the consultant to validate, already decided to do.

33
00:04:18.890 --> 00:04:19.859
George Reed - Noetic: Same thing. Yeah.

34
00:04:20.519 --> 00:04:24.689
Lilliana Robinson: That's interesting. I'm very interested in what you can come up with to test that Morgan.

35
00:04:27.019 --> 00:04:48.270
Morgan Stanton: For me, it's the assumption that like, now's the time for, you know, solving problems in women's health. I think no one ever asked like, why should we solve for cancer? Or you know Alzheimer's or something? But I think a long time people have neglected women's health, and I think now is the time to go forward, and that's what my company is based off of. So just on kind of that assumption.

36
00:04:48.550 --> 00:04:55.929
Lilliana Robinson: Tell me more about that. What would it mean for you if that is untrue? Does that mean the demise of the business, or it just would be harder like.

37
00:04:56.290 --> 00:04:57.160
Lilliana Robinson: just harder.

38
00:04:57.160 --> 00:05:19.044
Morgan Stanton: I think for a long time. It's just kind of an assumption that women's pain is just something most women can just deal with. So I think there's more of a public awareness about it now. I think investors are more aware of it now. But that you know, that's again an assumption on my part. So if it's not true, it doesn't mean that obviously the patients are still there. These women still want help. It will just be a bit harder on me for herself.

39
00:05:20.540 --> 00:05:39.199
Lilliana Robinson: That's interesting. Yeah, I haven't really heard an assumption like that before, but that's completely valid where it's like, look, this could be the time for it, and it could not be, and it could be a lot harder if it's not the time for it. But that doesn't mean that it's like your business cannot succeed right now, either way. So I would say, like, that's like a less risky assumption. But it's still totally valid.

40
00:05:40.330 --> 00:05:41.869
Lilliana Robinson: Okay, anyone else

41
00:05:44.950 --> 00:05:46.900
Lilliana Robinson: cool. Let's move on.

42
00:05:47.475 --> 00:05:50.760
Lilliana Robinson: So those were all great. Let's do an example

43
00:05:51.040 --> 00:06:00.629
Lilliana Robinson: or a couple of examples very similar to what you guys just shared. Let's pretend we have a health tech company in the dementia space targeting senior citizens, children

44
00:06:00.780 --> 00:06:06.790
Lilliana Robinson: who are making decisions for their aging parents. I know a lot of us on the call have aging parents. So this is like pretty

45
00:06:06.800 --> 00:06:14.969
Lilliana Robinson: should resonate with a lot of people. So pretend we're all working on this business. A couple of hypotheses or assumptions that we could make

46
00:06:14.990 --> 00:06:28.240
Lilliana Robinson: about this business. I believe that Facebook is the best channel to acquire people 45 to 60 years old who are my target customers. I really like this one as our very 1st example, because

47
00:06:28.250 --> 00:06:35.000
Lilliana Robinson: all of your examples, which were spot on and great, but they were much more big picture.

48
00:06:35.350 --> 00:07:04.330
Lilliana Robinson: So like, probably because you guys were giving me some of your riskiest ones, I want it to be clear that it's also extremely important to validate the smaller assumptions that you're making the assumptions where you're betting your time, your resources, your your team's efforts, all of that I want to make sure that every single thing that you're doing is not based on an unproven assumption. So something as small as trying to acquire people via Facebook, because you think 45 to 60 year olds are going to be on there.

49
00:07:04.330 --> 00:07:23.180
Lilliana Robinson: That is also an assumption. So it goes as big as are the customers even going to be interested in my platform being on a computer versus tabletop like, Dave said. All the way down to what channel should I use to target this group of people? These are all assumptions that we want to talk about today.

50
00:07:23.480 --> 00:07:31.419
Lilliana Robinson: Another one. I believe that investing in social media marketing will result in more qualified leads per month than email marketing.

51
00:07:31.600 --> 00:07:55.200
Lilliana Robinson: So obviously, we have to make prioritizations and distinctions between the different things that we do, but most of the time when we choose one thing or over the other, it starts as an assumption or hypothesis. You. You're not exactly sure when you pick one thing or another, when all of you are looking at your quarterly roadmap, you're deciding what you're going to work on this this quarter. A lot of you are choosing one thing over the other

52
00:07:55.350 --> 00:08:16.590
Lilliana Robinson: just based on a hypothesis. And we can actually get that a lot more at a lot higher of a success rate. If we start validating those hypotheses before we make those bets so like, if this person decided to start actually investing time and resources in social media marketing. I would hope that it was after they validated this assumption before moving away from email marketing.

53
00:08:17.150 --> 00:08:36.489
Lilliana Robinson: And I've seen companies not do that. So I've seen companies put tens of thousands of dollars into different campaigns or assumptions that they're making and lose it all during program which is really sad. So when I saw this workshop, I was extremely happy about it. I've added in a few things of my own from things that I've seen, because

54
00:08:36.809 --> 00:08:45.580
Lilliana Robinson: this is one of the major pitfalls of program. People usually figure it out in the second half of program. But in the 1st half of program a lot of our founders in the past have

55
00:08:45.640 --> 00:08:59.730
Lilliana Robinson: blown a lot of the capital that we've given them a lot of their time during program. We have a very finite amount of time all of these things on unproven assumptions. So I would guys to take away from this, not doing that. I don't know if you want to say anything else. No.

56
00:08:59.730 --> 00:09:03.700
Neal Sáles-Griffin: I do. I do just. I just want to double down on what Liliana just said it.

57
00:09:03.710 --> 00:09:18.320
Neal Sáles-Griffin: and and no matter, it doesn't matter. We say this every time, and for some reason it still happens, no matter what but what I'm saying, and maybe there's someone not in the room right now I don't know. I think we're missing, probably at least 2 to me. That I want to make sure it hears this.

58
00:09:18.450 --> 00:09:19.510
Neal Sáles-Griffin: which is

59
00:09:19.790 --> 00:09:26.469
Neal Sáles-Griffin: all of a sudden. You have this like gust of $120,000 in your business bank account.

60
00:09:26.590 --> 00:09:31.330
Neal Sáles-Griffin: plus whatever you already had in there. And you look at that money. And you're like, yeah, like.

61
00:09:31.420 --> 00:09:33.670
Neal Sáles-Griffin: I just need to drop a bunch of this into

62
00:09:33.750 --> 00:09:37.840
Neal Sáles-Griffin: marketing. And I'm gonna make more money. And I'm gonna get more customers.

63
00:09:37.940 --> 00:09:46.099
Neal Sáles-Griffin: And it's a it's. It just blows me away. Because in my mind it's common sense that that's like, basically like grabbing a bag of money and lighting it on fire. But

64
00:09:46.360 --> 00:09:48.580
Neal Sáles-Griffin: there's people who don't know that who need to hear that.

65
00:09:48.810 --> 00:09:56.249
Neal Sáles-Griffin: So I'm just saying it out loud right now, if you think that now that you have this money, if you spend it on marketing, you're going to get sales. I got a bridge to sell it.

66
00:09:56.900 --> 00:10:00.129
Neal Sáles-Griffin: so I'll leave it at that for now. But I think Dominique seems to agree

67
00:10:01.340 --> 00:10:02.640
Neal Sáles-Griffin: absolutely.

68
00:10:02.988 --> 00:10:19.390
Dominique (CEO) & William Stone (COO)_Agent Lunar: It's like resonating anyone. Please talk. There's a few marketeers in the class, but especially before paid. I'm offering it up now. Please talk to me, because it's so sad to see ultimately these channels. I always say the businesses they want you to spend more. So

69
00:10:19.510 --> 00:10:22.176
Dominique (CEO) & William Stone (COO)_Agent Lunar: get very passionate. Thanks, Neil.

70
00:10:22.970 --> 00:10:47.290
Lilliana Robinson: I mean, we've just seen it time and time again. So this is the 1st time we're actually putting this warning before program even starts. So I'm hoping that it helps a lot. And it's not just marketing. That's just the biggest thing. There are a lot of things that people are wasting time and resources on chasing after customer segments that aren't going to come to fruition, wasting half of the program 6 weeks, 7 weeks, doing something that was not validated, and losing all of that time

71
00:10:47.290 --> 00:10:53.870
Lilliana Robinson: building a product that wasn't ever going to actually sell. All of these things are are pitfalls. We've seen time and time again.

72
00:10:53.870 --> 00:11:07.039
Neal Sáles-Griffin: Yeah. And one more anecdote on that front. Because that happened recently, Liliana, there was a company last year we had in our program. We were going through the jobs to be done stuff, and when it came down to present both during arena week and then, right after winter madness, both

73
00:11:07.070 --> 00:11:14.782
Neal Sáles-Griffin: moments it was very clear on our end. And to other founders like, Okay, the job isn't quite dialed in like we're not all the way there. We don't know.

74
00:11:15.110 --> 00:11:24.549
Neal Sáles-Griffin: and I get found. There's a choice, because we're not here to tell you what to do, so you can run with your assumption on what your gut tells you, even though the data isn't showing that there's anything there.

75
00:11:24.850 --> 00:11:28.450
Neal Sáles-Griffin: we're skeptical of it. We're concerned about it because we don't think it's going to work

76
00:11:29.060 --> 00:11:30.769
Neal Sáles-Griffin: the companies who do that

77
00:11:30.820 --> 00:11:34.780
Neal Sáles-Griffin: within 3 to 6 months after the program I'm sitting down with. And they're like

78
00:11:35.400 --> 00:11:47.679
Neal Sáles-Griffin: we need to find a way to adjust our strategy because it didn't work. And I'm like, okay, like I, I'm getting bored of of having that very predictable conversation with those founders. So just thought I would call that out because it continues to happen.

79
00:11:48.540 --> 00:11:53.650
Neal Sáles-Griffin: No, that was I was actually thinking about visual labs. But yes, spine help was a good example, too. Yeah.

80
00:11:53.650 --> 00:11:55.869
Lilliana Robinson: We're just calling people out, all right.

81
00:11:56.730 --> 00:11:59.839
Lilliana Robinson: so we'll just move on.

82
00:12:00.250 --> 00:12:05.039
Neal Sáles-Griffin: I mean they shut their company down, I mean, hey? They learned. They know they'll they'll own it.

83
00:12:05.250 --> 00:12:18.090
Lilliana Robinson: It happens. And and we'll always still help you guys, we actually like to try and like repurpose our founders. If you shut down to like other portfolio companies and stuff. So text search for life. It's it's totally fine.

84
00:12:18.930 --> 00:12:43.910
Lilliana Robinson: And 3rd one, I believe that the majority of customers live in urban areas. This kind of goes into like the personas thing that I tried to shoot down when we were doing jobs to be done. A lot of people come in, especially people with like Mba backgrounds. No offense. But you've been trained on these like personas and these demographics and a lot of those are just assumptions. You really do need data. And you need to validate these things. Even if

85
00:12:43.910 --> 00:12:57.160
Lilliana Robinson: it's coming from which I'll get to it might be a bias from experience that you already have, but you still need to validate it in this specific circumstance, even if you have experience, and might be able to draw from that to make a pretty

86
00:12:57.530 --> 00:13:04.240
Lilliana Robinson: educated hypothesis. Every hypothesis is still that they're not proven until you validate it.

87
00:13:04.970 --> 00:13:07.710
Lilliana Robinson: So what can we do next? Based on these hypotheses?

88
00:13:08.573 --> 00:13:14.529
Lilliana Robinson: So that's the question that you should ask yourself. So I added, this slide. Does anyone know who this is?

89
00:13:17.310 --> 00:13:18.320
Monica Roy (Seascape Clinical): Darwin.

90
00:13:18.320 --> 00:13:18.990
Lilliana Robinson: Yeah.

91
00:13:19.100 --> 00:13:27.569
Lilliana Robinson: So we have a lot of scientists on the call. So I won't belabor the point because you guys already know this. But for those who don't. Darwin like father of evolution.

92
00:13:27.570 --> 00:13:52.269
Lilliana Robinson: He also is one of the best examples of early scientific method. And that's what we're going to talk about today, the scientific method. That's why we're talking about hypotheses and experimentation. All of that. It's kind of odd because we're in startups business entrepreneurship. But the scientific method transfers greatly over to building a startup. So Darwin made an observation in the Galapagos Islands. He saw these birds on the different islands with different kinds

93
00:13:52.270 --> 00:14:02.019
Lilliana Robinson: of beaks, and he observed, and those beaks, and made the hypothesis. Hmm! I wonder if the beaks are being

94
00:14:02.190 --> 00:14:23.860
Lilliana Robinson: developed over time based on the different food varieties that exist on each of the islands, so like the ones that have nuts, have a much sharper beak. The ones where they were eating worms have, like a less sharp beak. You can see the difference on the picture. And then he designed an experiment to go out and test that over many generations of birds, which he did, and then he was able to prove evolution.

95
00:14:24.162 --> 00:14:34.129
Lilliana Robinson: and kind of like survival of the fittest, that whole concept. So the point here is not the evolution piece. It's that he used the scientific method. He observed this about the birds.

96
00:14:34.140 --> 00:14:55.039
Lilliana Robinson: A lot of people who don't understand the scientific method, think that he observed that about the birds, and therefore concluded evolution. But that's not how it works, and that's not how it works in the startup, either. You're observing a lot of things. You're gonna observe your customers behavior. You're going to observe trends in the market, and you cannot just skip to drawing conclusions. You have to make an

97
00:14:55.720 --> 00:15:09.130
Lilliana Robinson: assumption or a hypothesis, and then design an experiment to prove or disprove that, and then you draw the conclusion. So there are really like 4 steps there. You can't just go from step one to 4, or else you're going to make a lot of mistakes and a lot of unfounded conclusions.

98
00:15:10.110 --> 00:15:33.470
Lilliana Robinson: So speaking of the scientific method, this is the same thing. So you understand the before and after of the hypotheses. So because I observed that individuals ages 45 to 60, use Facebook a lot. And our social media reports confirm this demographic. I believe that meaning our hypothesis, we're using the words, I believe, because that reminds us that it's not true.

99
00:15:34.060 --> 00:15:59.690
Lilliana Robinson: like not true. Yet it's an assumption. So I believe that. Or I assume that Facebook is the best channel to acquire people 45 to 60 years old, who are my target customers. And therefore this is the experiment I'm going to run ads on different social media platforms to test whether Facebook outperforms other platforms on acquiring customers. And then the 4th step here would be the conclusion you can draw.

100
00:16:00.200 --> 00:16:07.900
Lilliana Robinson: Based on this experiment. I am therefore able to conclude that Facebook is the best channel or is not the best channel.

101
00:16:07.940 --> 00:16:16.240
Lilliana Robinson: and then you can say for certain that statement. So that's a scientific method here. Does anyone have questions about any of this?

102
00:16:16.360 --> 00:16:18.139
Lilliana Robinson: It's pretty straightforward. But

103
00:16:18.310 --> 00:16:20.040
Lilliana Robinson: pause me if you need me to

104
00:16:22.850 --> 00:16:23.630
Lilliana Robinson: cool.

105
00:16:23.760 --> 00:16:38.129
Lilliana Robinson: So here's a little framework. This is from Ryan Cooter, who is a mentor to Neil. He's a long time managing director here at techstars. He has run techstars anywhere for the longest time, and now he's running techstars. San Diego.

106
00:16:38.604 --> 00:16:43.630
Lilliana Robinson: And he made this for you guys. So this is a framework. We have our observations.

107
00:16:43.670 --> 00:17:02.779
Lilliana Robinson: we have our hypotheses, which are the assumptions we're making. So it's important that when you're thinking about your assumptions like, what are the observations that you made that make made you think that assumption so like for Dave, you obviously have a reason that you are making the assumption that people are going to change from tabletop to computer games when you make their game online.

108
00:17:03.030 --> 00:17:30.970
Lilliana Robinson: I'm curious for you to brainstorm what observations you made or have had in your life that have led you to have those assumptions cause that's important to lead into and figure out where you're getting that then it's either proven or unproven. Most of your assumptions right now are going to be unproven. Very few of you have already run sufficient experimentation on your assumptions for any of them to be proven. But maybe you have. Maybe a couple of them are especially for the smaller ones, like which

109
00:17:30.970 --> 00:17:46.371
Lilliana Robinson: marketing channel to use and then we're gonna get into prioritization because you definitely want to rank your assumptions by importance so that you know what to experiment with first, st and then you have your experiments. So that's how you design to validate it. So pretty straightforward stuff here.

110
00:17:46.938 --> 00:18:11.769
Lilliana Robinson: But this is a skill that I think a lot of people neglect. We always try and build experiments with our founders, but we never sit down to try and like teach this for you guys to understand the whole method yourself. And it's really important because we can't just walk every single one of you through 10 experiments for the entire program. And we're definitely not going to be with you after program. But this is gonna be for the next 7 to 8 years of you building this business and seeing it to fruition.

111
00:18:11.770 --> 00:18:23.070
Lilliana Robinson: it will be making assumptions and validating those assumptions that is like the crux of making a business. That's the whole bread and butter of it. And I feel like it's not talked about enough because a lot of people.

112
00:18:23.070 --> 00:18:35.710
Lilliana Robinson: especially in the Bay area. I'm just gonna say it. They go off of unvalidated assumptions. And that's why we see a lot of people crash and burn. They'll raise millions and millions and then crash and burn because they were going off of assumptions that were unproven the whole time.

113
00:18:36.850 --> 00:18:37.810
Lilliana Robinson: So

114
00:18:38.610 --> 00:18:52.519
Lilliana Robinson: let's go into hypotheses first, st and then we'll go into experiments. So what is a hypothesis, a good, well formed hypothesis or assumption that I'd like you guys to make is 3 things. It's testable.

115
00:18:52.760 --> 00:19:08.470
Lilliana Robinson: testable meaning like you can design an experiment that would tell you yes or no. It's not like a vague statement, or something that you couldn't physically test to get an answer for kind of like the unknown questions of the universe. We're not going to be able to test that

116
00:19:08.470 --> 00:19:35.029
Lilliana Robinson: precise, precise meaning. It has a yes or no answer, so it either is true or it's not. There's not a gray area. So we're not going to test it and then come out and say, maybe it's true. Precise is mostly in the way that you're framing the assumption. You want to frame it in a way that is a yes or no, very similar to what we talked about with the key results yesterday, and the last one is discrete, and for those who are like more scientific or mathematic

117
00:19:35.710 --> 00:19:37.830
Lilliana Robinson: discrete is much more.

118
00:19:37.870 --> 00:19:47.859
Lilliana Robinson: it's much less about the precise yes or no. It's that we're validating. Only one variable at a time. So discrete is that we're isolating one unknown.

119
00:19:47.890 --> 00:20:00.190
Lilliana Robinson: And we're not putting a bunch of unknowns together, because that actually muddles it. And you're not sure what's true or what's not true. So very similar to this example with the Facebook, they're isolating one

120
00:20:00.220 --> 00:20:30.090
Lilliana Robinson: unknown. Their unknown is Facebook. They want to know if Facebook is the best channel. They're not like wondering. Oh, is all of social media good or which social media one is the best they're just seeing. Yes or no is Facebook the best? And that's kind of how you can be discreet. And that's going to give you the most successful results so testable, precise, and discrete. We want to isolate a variable one unknown. We want it to be a yes or no answer, and we want to be able to design a test to get that yes or no answer.

121
00:20:31.170 --> 00:20:35.320
Lilliana Robinson: There are different types of hypotheses like I've kind of alluded to already.

122
00:20:35.930 --> 00:21:00.810
Lilliana Robinson: if they can be high importance for your business, low importance, they can also be proven or unproven. But I want to call your attention to this top area. This is kind of what we were talking about at the very beginning. A lot of you unmuted, and gave me your foundational assumptions, which are much more risky for your business, because by definition they're foundational, which customer segment you're going after is this problem even a problem or the problem I should solve? Is it a validated problem?

123
00:21:00.810 --> 00:21:07.320
Lilliana Robinson: Is this solution the correct solution to the problem which jobs to be done can help validate.

124
00:21:07.610 --> 00:21:32.149
Lilliana Robinson: All of those different things are foundational, functional are some of the examples I shared. Product sales hiring economic assumptions, marketing assumptions which channel you're going down. Yeah. Hiring is a good one, too. We don't really think about that, but assumptions that hiring more people will help with said problem. That is an assumption I see people make during program. So so much.

125
00:21:32.230 --> 00:21:42.930
Lilliana Robinson: you guys will get the 120 K, and you'll say, I need to hire these 2 contractors. I need a designer for this, and I need a dev for that. And I need this 3rd person for this thing, a customer success.

126
00:21:43.428 --> 00:22:03.350
Lilliana Robinson: person. And that's going to solve all my problems. That's a heavy assumption. So I want you guys to see hiring as also assumptions that you need to validate. You need to be sure that adding a job or hiring someone right now is going to give you the value that you're willing to invest in it, if not more value. That is an assumption that you should test

127
00:22:04.230 --> 00:22:07.890
Lilliana Robinson: the same way. You would test a product assumption or a sales assumption.

128
00:22:08.170 --> 00:22:22.190
Lilliana Robinson: So there, there are 2 different kinds. It doesn't really matter which kind. It doesn't make something more or less important, because something might be more relevant in the functional aspect than the foundational. It does make things more risky if they're a foundational assumption, though.

129
00:22:23.090 --> 00:22:42.680
Lilliana Robinson: So a couple of examples, these are more functional. So product. We believe that the majority of small business owners in urban areas are actively seeking solutions to streamline their social media management. So this is. This is kind of foundational, too, because it sounds like their whole product is not going to be able to serve their customer segment. If this is not true.

130
00:22:43.052 --> 00:23:00.799
Lilliana Robinson: So that's a little bit foundational marketing. We believe that investing in social media marketing will result in more qualified leads per dollar than email marketing. So that's the one we saw before hiring. We believe that using a new recruiting software will double the qualified candidates for technical roles. This is a huge deal

131
00:23:01.240 --> 00:23:04.910
Lilliana Robinson: we don't want to pour a bunch of money in if we're not sure that that's true.

132
00:23:05.750 --> 00:23:08.319
Lilliana Robinson: So, reflecting on observation based

133
00:23:08.350 --> 00:23:18.379
Lilliana Robinson: on hypothesis. So this is kind of what I already said. It's important to figure out what observations you are making, so that you can recognize your own biases that are lending

134
00:23:18.520 --> 00:23:31.050
Lilliana Robinson: yourself to come up with the assumptions and hypotheses that you are. So a lot of people neglect like what led to that assumption. I want you guys to brainstorm what led to that assumption? Because it's very important. There might be

135
00:23:31.500 --> 00:23:56.230
Lilliana Robinson: clues in there that lead you to know whether this is going to be proven or unproven. Some of these you're going to realize once you brainstorm, are quite unfounded, and other ones you might be even more sure about. Once you think about the observations that led you to it, remember that the observations will never prove it. So, even if you have a bunch of data that you observed before. If you haven't run a test, it's still unproven, but it might be a more well founded hypothesis.

136
00:23:56.620 --> 00:24:02.060
Lilliana Robinson: So it's important, because what you observed in the past can explain why you believe something is true.

137
00:24:02.430 --> 00:24:08.399
Lilliana Robinson: So we're going to take 10 min. I'm not going to put you in breakout rooms. We're just going to do it. Here.

138
00:24:09.520 --> 00:24:36.219
Lilliana Robinson: We'll take closer to like 8 min. I want you guys to take a pen and paper or a Google Doc or a Google sheet and come up with at least 10 assumptions about your business include assumptions about your customer segment. All of the foundational things about your product, about your problem, space, about your solution, and also about the things you're doing functionally in your company, the marketing, the hiring, the financials, all of that. What do you believe to be true?

139
00:24:36.310 --> 00:24:48.090
Lilliana Robinson: And then, for each hypothesis, write down the observation that made you believe it's true. So the thing that you saw that led you to make that assumption, even if it's something as foundational as the whole reason you started the business

140
00:24:48.400 --> 00:24:52.570
Lilliana Robinson: getting to the root of why you have that hypothesis, and then for each one

141
00:24:52.770 --> 00:25:09.310
Lilliana Robinson: note whether each one is proven or disproven. Already most of yours are unproven. Sorry, most of yours will already be unproven. You will not have proven any, but if you have already run an experiment, that's fine. So write that down. Take about 8 min.

142
00:25:09.470 --> 00:25:11.439
Lilliana Robinson: and then I'll have a few of you share.

143
00:25:19.910 --> 00:25:22.210
Lilliana Robinson: I'm going to go back to this for you guys.

144
00:28:12.280 --> 00:28:16.560
Lilliana Robinson: Okay, take about 2 more minutes, and then we're gonna share

145
00:30:42.620 --> 00:30:44.329
Lilliana Robinson: alright. That's time.

146
00:30:44.810 --> 00:30:47.740
Lilliana Robinson: Does anyone want to share one or 2?

147
00:31:00.070 --> 00:31:01.330
Lilliana Robinson: Yeah, sure.

148
00:31:01.720 --> 00:31:10.149
Çoruh | CogniScope: Yeah, so this is one of the fundamental ones. We said, like, understanding players are the key for developing great games.

149
00:31:12.610 --> 00:31:13.470
Lilliana Robinson: And what

150
00:31:14.900 --> 00:31:16.919
Lilliana Robinson: what is the assumption you're making.

151
00:31:16.920 --> 00:31:22.960
Çoruh | CogniScope: Assumption is that that to develop great games developers should understand what player wants.

152
00:31:23.670 --> 00:31:28.089
Lilliana Robinson: Okay, I see. And then, did you, brainstorm? What observations you made that.

153
00:31:28.090 --> 00:31:32.280
Çoruh | CogniScope: Yeah, like, we are all gamers, and we do love

154
00:31:32.410 --> 00:31:33.540
Çoruh | CogniScope: what we

155
00:31:33.840 --> 00:31:36.530
Çoruh | CogniScope: like. Imagine to play so

156
00:31:37.313 --> 00:31:41.210
Çoruh | CogniScope: like to look ourselves and our among our friends.

157
00:31:41.940 --> 00:31:42.980
Çoruh | CogniScope: We always

158
00:31:43.020 --> 00:31:44.639
Çoruh | CogniScope: think that.

159
00:31:45.550 --> 00:31:47.979
Çoruh | CogniScope: These studios cannot

160
00:31:47.990 --> 00:31:51.210
Çoruh | CogniScope: develop great games because they do not

161
00:31:51.880 --> 00:31:55.620
Çoruh | CogniScope: understand what we think about it, like what we want from that game.

162
00:31:56.657 --> 00:31:59.860
Çoruh | CogniScope: Like kind of a self self observation.

163
00:32:01.155 --> 00:32:01.810
Çoruh | CogniScope: But

164
00:32:01.950 --> 00:32:04.120
Çoruh | CogniScope: like I think, the

165
00:32:06.500 --> 00:32:13.769
Çoruh | CogniScope: The experiment should be to talk with great game producers and understand

166
00:32:13.950 --> 00:32:19.769
Çoruh | CogniScope: the key aspects on, how did they made their games that successful? Maybe.

167
00:32:19.770 --> 00:32:35.800
Lilliana Robinson: I'm I'm glad that you gave that example because a lot of you guys and all the founders that I've ever worked with a lot of you guys are solving problems that you've seen yourself in your own personal life, either with yourself or a loved one. Not everybody, but a lot of founders do that, and

168
00:32:35.800 --> 00:32:51.349
Lilliana Robinson: assumptions are really huge for that scenario, because a lot of founders see a problem or experience problem assume that that problem has certain things true about it, whether it's that everyone feels that problem or everyone experiences it in the exact way that

169
00:32:51.350 --> 00:32:53.659
Lilliana Robinson: the founder has all of those things.

170
00:32:53.969 --> 00:33:09.459
Lilliana Robinson: And you need to validate all of that. So I completely agree. I think there are a lot of assumptions made when it's from your personal experience. But it also could be that they're all correct, which is amazing. So I'm very happy and excited to see you validate those

171
00:33:10.050 --> 00:33:10.860
Lilliana Robinson: flow.

172
00:33:13.544 --> 00:33:40.909
Florence Ettlin: Yeah. So one of ours is that short term rental management companies struggle the most with handling guest inquiries at pre booking rather than pre-arrival, and during the stay and the way we came to that hypothesis or assumption is that we observed literally companies that we reached out to saying, Oh, wait! You don't tackle pre booking, then maybe not for us. So, and the experiment for us will be during jobs to be done. To really get to the bottom of that.

173
00:33:41.660 --> 00:33:52.800
Lilliana Robinson: Yeah, these are really good examples. So Truro gave an example of like, I had this personal experience. So I made this assumption, I need to validate it. I know. Cause I talked to you guys about this a little bit more, Flo.

174
00:33:53.230 --> 00:33:54.810
Lilliana Robinson: You had

175
00:33:54.950 --> 00:34:11.390
Lilliana Robinson: 2, I think 2 or 3 customers ask for something that you weren't offering. And so you're making the assumption that the rest of your initial customer segment would feel the same way. And so before building product. I remember you said this before going and building product. You want to validate that

176
00:34:11.389 --> 00:34:29.300
Lilliana Robinson: for you. It's going to be using jobs to be done. There's also experiments you can design around that. I think that's great. So that's something that everyone here probably will experience. You're gonna see? Like 2, 3 of your customers ask for a certain thing, or want a certain thing, you cannot immediately assume that the rest of the segment wants that.

177
00:34:29.300 --> 00:34:42.040
Lilliana Robinson: So what a few things could happen, they could be a sub segment that you decide. You want to pivot and serve, or they could be representative of your whole segment, or they could just be a minority that you don't want to serve, and you need to like.

178
00:34:42.110 --> 00:34:53.909
Lilliana Robinson: Kick them out of your segment and keep serving the main segment. So, like all of those things could happen, and each of those are completely different directions. So if you just picked one blindly, I mean, you have like a 1 3rd chance of of being correct. So

179
00:34:53.980 --> 00:34:58.690
Lilliana Robinson: yeah, I'm really glad you want to validate that before building product for that. That ask of theirs

180
00:35:00.020 --> 00:35:05.139
Lilliana Robinson: cool. Okay, so we'll have, we'll be able to share more in the next exercise. But those were 2 great examples.

181
00:35:05.280 --> 00:35:13.760
Lilliana Robinson: So here are some more things to think about. So look at the list that you made, and for the ones that you wrote have been proven.

182
00:35:13.760 --> 00:35:35.430
Lilliana Robinson: think about. Why do you think it's proven. What data specifically, do you have to prove this hypothesis? And do you need additional observations to prove this hypothesis? I would say, not only what data, but do you have enough data, or is it just like one thing that's proving this? And then for your unproven hypotheses on your sheet? Why do you think it's unproven?

183
00:35:35.570 --> 00:35:47.460
Lilliana Robinson: Where are the gaps like? What are the things you need to test to make that proven? And what data do you need to prove this hypothesis. So those questions are key to designing an experiment to prove it. So we'll get into that in a second

184
00:35:49.050 --> 00:35:52.290
Lilliana Robinson: some examples. Here's an unproven hypothesis.

185
00:35:52.400 --> 00:35:59.460
Lilliana Robinson: The same one we've been talking about investing in social media marketing will result in more qualified leads per dollar than email marketing.

186
00:35:59.630 --> 00:36:15.750
Lilliana Robinson: Why, it's unproven. Might be. We haven't yet run parallel campaigns to compare the effectiveness of both channels. So it would be unproven. Therefore, even if you have a marketing background, and you've seen this be true before. It's unproven for this specific scenario with this specific startup

187
00:36:16.050 --> 00:36:40.319
Lilliana Robinson: gaps, lack of comparative data between social media and email marketing campaigns so same thing and then needed data. So an experiment that they could run is a B testing between the 2 campaigns, measuring the number and quality of leads generated from each, comparing that, and then drawing a conclusion that would make this a proven hypothesis, which would then, as you can probably imagine, unlock a lot of doors for them to then invest in this route.

188
00:36:40.850 --> 00:36:41.690
Lilliana Robinson: Dave.

189
00:36:45.200 --> 00:36:57.400
Dave Scott: So I have an hypothesis that, you know. Can I get a million subscribers? You know, that would would buy my product right? And so how? How can I prove that? Like, you know, I mean.

190
00:36:57.490 --> 00:37:01.790
Dave Scott: until I get a million subscribers, how can I prove that I can get a million subscribers.

191
00:37:02.280 --> 00:37:03.620
Lilliana Robinson: I would probably

192
00:37:03.800 --> 00:37:27.470
Lilliana Robinson: I would probably roll that back to like the foundational assumptions that are in there. So yeah, a lot of us are making assumptions that the business is going to work right. And the way you test that the business is going to work is you try and make the business work, and it's a great experiment, and for some of you it'll work, and some of you. It won't. But for yours, Dave saying you can get a million subscribers is really like the 1st one you said. People will convert to my platform.

193
00:37:27.660 --> 00:37:29.160
Lilliana Robinson: I will be able to get

194
00:37:29.220 --> 00:37:31.949
Lilliana Robinson: X amount of users per month.

195
00:37:31.960 --> 00:37:51.409
Lilliana Robinson: or something like. First, st you should make that assumption time bound. So like I can get a million users in the next 2 years, and then your assumptions really, that you wanted to attest? Are you're acquiring this amount of customers. Does it look like you're actually gaining that are they switching over like. You see how there are a lot of mini assumptions that.

196
00:37:51.410 --> 00:37:57.139
Dave Scott: I guess my hypothesis was different. My cop, my cop, my my hypothesis, was, there are enough

197
00:37:58.250 --> 00:38:02.489
Dave Scott: modern Ttrpg players to grow to a million subscribers.

198
00:38:02.490 --> 00:38:03.950
Lilliana Robinson: Oh, interesting!

199
00:38:04.010 --> 00:38:16.720
Lilliana Robinson: Well, that's a very testable hypothesis. Remember that the hypothesis needs to be testable and then precise discrete. That's extremely testable, right? You just need the data on how many of those players exist?

200
00:38:16.730 --> 00:38:18.740
Lilliana Robinson: You're just saying exist in the world.

201
00:38:18.890 --> 00:38:19.560
Dave Scott: Yeah.

202
00:38:19.820 --> 00:38:22.619
Lilliana Robinson: Yeah, so you could gather data for that

203
00:38:23.810 --> 00:38:26.490
Lilliana Robinson: and test that like, you could run an experiment where you just

204
00:38:26.610 --> 00:38:29.630
Lilliana Robinson: collect the data of how many exist in the world.

205
00:38:30.210 --> 00:38:31.970
Dave Scott: Yeah, I mean, like a like a

206
00:38:32.200 --> 00:38:34.239
Dave Scott: a panel survey or something.

207
00:38:35.080 --> 00:38:40.089
Lilliana Robinson: No, there. There are ways to purchase data on different segments and things we can work together on that.

208
00:38:40.200 --> 00:38:54.060
Lilliana Robinson: That one is actually a really easy one to run. You don't even have to run an experiment that one's much more like data gathering. That's why it says needed data. You don't always have to run an experiment if there's like data out there that could validate, especially when you have assumptions about the market in general.

209
00:38:55.430 --> 00:39:03.759
Lilliana Robinson: So we could. We could like purchase data to figure out how many Ttrpg players exist in America, for instance?

210
00:39:06.654 --> 00:39:16.039
Lilliana Robinson: Or we could do it ourselves, Dave. So I can think of a lot of ways that we could like you said kind of like survey people or figure out the amount of people that exist in in this country.

211
00:39:16.700 --> 00:39:41.709
Lilliana Robinson: So challenging your hypotheses, I added, this slide for you guys, this is a huge like, does everyone pause and pay attention to this one slide. It's extremely important investment bias I have created this term. So your assumptions will have this investment bias, meaning some of you have already made a big bet based on an unproven hypothesis. You may be inclined to assume that it's true.

212
00:39:41.710 --> 00:39:48.859
Lilliana Robinson: So a lot of you have already cause you guys have been working on your businesses, you might have already sunk a lot of money into something

213
00:39:48.880 --> 00:40:14.030
Lilliana Robinson: probably a product that you have right now that is not validated. You might have sunk money into a hire that you don't know. If you actually need. You might have been paying a bunch of money into ads or marketing before validating that. That's what you needed to do. That's okay. Most people come into techstars. Having made unproven, unvalidated big bets and lost a lot of capital time resources. Effort. Into that.

214
00:40:14.380 --> 00:40:28.909
Lilliana Robinson: That's fine. A lot of people start thinking about their assumptions and then want to post hoc, justify meaning after the fact. Come up with a justification for why, what they did was actually true, but it was never proven.

215
00:40:28.910 --> 00:40:57.410
Lilliana Robinson: So, going in and making a big bet on an unproven hypothesis. Make sure that you don't have this bias where it's kind of like what George was talking about the confirmation bias like you just are trying to validate it because you already made the bet there. It's okay to have been wrong. It's okay to have lost a bunch of money or time or resources. That's fine. Most probably more than half of you have already made this mistake. That's why we have this workshop right now. It's totally fine. So make sure that in your assumptions you're not having a bias.

216
00:40:57.530 --> 00:41:10.579
Lilliana Robinson: what? I'm calling an investment bias where you've already invested a lot into something. And so you assume that it's true or proven. Be honest with yourself. Is it actually true or proven, or am I post hoc justifying it? Because I already made the action?

217
00:41:10.770 --> 00:41:19.560
Lilliana Robinson: So another caveat here. We're going to have a whole workshop in week, 6 about how to spend or not spend your money wisely.

218
00:41:20.110 --> 00:41:29.320
Lilliana Robinson: Neil and I didn't want to have you guys wait until week 6, because a lot of people make a lot of mistakes between weeks one and 6, and then tell us they wish that workshop was at the beginning.

219
00:41:30.100 --> 00:41:35.869
Lilliana Robinson: I don't think that it's good to put that workshop at the beginning, because you guys need to have gone through mentor madness 1st to see that.

220
00:41:36.090 --> 00:41:39.360
Lilliana Robinson: However, I am putting this warning bell. Here

221
00:41:39.950 --> 00:41:57.130
Lilliana Robinson: pause. All big bets based on unproven assumptions. You will think yourself down the road. So from week one to week 6, when we have that workshop, just be extremely conservative with any sort of big bets you're making with your capital, with your time, with your resources.

222
00:41:57.670 --> 00:42:11.970
Lilliana Robinson: Make sure that anything you're doing you're doing based on proven assumptions, or you're doing it to validate an assumption. Do not do anything that isn't unproven. You will regret it, and then you will be so sad when you hear. Hear the workshop in week 6.

223
00:42:12.220 --> 00:42:40.690
Lilliana Robinson: I'm just saying that right now another one another bet is giving away equity to advisors, to random people. People do that between weeks one and 6 don't do that. You will regret that. Come, talk to me and, Neil, if that's something that you want to do. So this is your warning bell. We'll talk more about it in 6 weeks. But just don't do this for now, and if you feel like you need to come, talk to us before you make that decision, or else you're going to regret it. Once we talk more about it in in mid program.

224
00:42:41.640 --> 00:42:42.590
Lilliana Robinson: Morgan.

225
00:42:44.650 --> 00:43:01.619
Morgan Stanton: Yeah, I have a quick question. So like the validation process itself will will take time right of ours. So obviously, you want it to be like a quick validation like, do you have like an optimal like this should happen in like week, like in a in a month, like what? I I guess that might depend a little bit on the goals. But like.

226
00:43:01.740 --> 00:43:07.299
Morgan Stanton: how do you find that balance of like I need to get to this answer quickly, because you want to move your business forward.

227
00:43:07.590 --> 00:43:18.798
Lilliana Robinson: Yeah, at techstars it. We like to experiment quickly. So like the whole month to growth, month is really for validating assumptions and experimentation. And then, like growth, hacking and stuff

228
00:43:19.790 --> 00:43:26.763
Lilliana Robinson: the fastest turnaround possible, the better. So like Brad, who is on our Texar Chicago team, and you'll meet him next week.

229
00:43:27.351 --> 00:43:52.070
Lilliana Robinson: Now he runs global sourcing. He is really really good at helping founders with experimentation. He tries to run them as fast as possible. How many experiments. Can you do at once? How many assumptions can you be validating at once? He teaches the founders that it should be like a hurried experience like you want to validate these things as quickly as possible? And to answer your question, Morgan, it does. It does depend right. Cause. If you need to do an experiment where you're sending something out and

230
00:43:52.070 --> 00:44:04.580
Lilliana Robinson: seeing how many customers sign up and things you do need to give it time. But if it's something more like you can gather data and validate it. You could do it like multiple today. So it really depends on what kind of experiment you need to set up for that

231
00:44:04.812 --> 00:44:27.089
Lilliana Robinson: but the faster the better. That's this is something that we do want to speed run. We want to do it well, but we want to like validate as many of these things as quickly as possible, because what I have on the screen here we don't want to start making bets on things that are unproven. But we also don't want to pause all of our progress. Right? So it's like it's a 2 sided coin that the answer to it is to just quickly validate as many things as possible as quickly as possible.

232
00:44:28.340 --> 00:44:43.660
Lilliana Robinson: This is going to be something that you guys probably won't have time to do until we get to the mid program. Most of you guys will be like validating assumptions via mentors in mentor madness or not at all or through jobs to be done

233
00:44:43.690 --> 00:44:50.999
Lilliana Robinson: until we finish and we get to mid program. You probably won't be able to run like full experiments that are outside of those 3 things.

234
00:44:52.750 --> 00:45:11.689
Lilliana Robinson: Here's some more things to consider, to challenge your hypotheses, reflect on implicit bias that could be impacting these hypotheses. So one implicit bias is the investment bias that I just talked about, but also just your own personal experiences, especially if it's a an issue that you've already experienced yourself or a loved one has?

235
00:45:12.510 --> 00:45:20.780
Lilliana Robinson: Do you have an implicit bias on these assumptions? Are you assuming them to be even more true because it comes from your experience a lot of people? The answer is, yes.

236
00:45:20.840 --> 00:45:24.869
Lilliana Robinson: Have I taken in other perspectives to develop these hypotheses?

237
00:45:24.900 --> 00:45:41.520
Lilliana Robinson: So a perspective bias make sure that you're, you know, getting outside of your own head, and picturing the rest of the world, too. That's going to make it even more true. What are my past hypotheses did they lead to a positive or negative outcome? So consider your history and things that you've already proven or disproven

238
00:45:41.700 --> 00:45:59.180
Lilliana Robinson: the outcome. Have I approached this thinking about what I want to find out rather than trying to prove something that I believe to be true. This one is huge, this is like confirmation bias. Is it something that you want to find out? Or do you just want this thing to be true? Probably because we've already invested a lot of time in, or we feel

239
00:45:59.410 --> 00:46:17.459
Lilliana Robinson: personally invested in it. Being true, a lot of people want their idea of the problem space to be true because they've personally invested their heart and soul into solving that problem. I'd like us to detach a little bit and be much more invested in solving the truest form of the problem. That's what's going to make you successful.

240
00:46:17.710 --> 00:46:20.480
Lilliana Robinson: One of the companies Neil was referencing earlier.

241
00:46:20.730 --> 00:46:40.640
Lilliana Robinson: The founder clearly had a personal bias, a personal tie to wanting to solve a very specific problem when the market and the customer segment clearly needed something completely different. And it's almost like he forced it. He was trying to shove a square peg in a round hole, because that's the problem he wanted to solve, and it didn't work

242
00:46:40.690 --> 00:46:47.399
Lilliana Robinson: it. It never does so trying to like, detach yourself and the things you want to be true versus what's actually true.

243
00:46:49.518 --> 00:46:55.230
Lilliana Robinson: I'm going to skip this and keep moving forward. So now I want. I'm going to put you guys in breakout rooms for

244
00:46:55.590 --> 00:47:16.720
Lilliana Robinson: like 6 min. I want you to take 3 assumptions each and like and challenge each other's. Why do you think the hypothesis is unproven? What observations led up to the hypothesis, and what data might be missing and also start brainstorming? What experiments could I probably do to validate this assumption, so help each other start to come up with that.

245
00:47:17.460 --> 00:47:22.949
Lilliana Robinson: I'm gonna pop you guys into breakout rooms right now, and I'll let you know when 6 min is up.

246
00:47:29.730 --> 00:47:31.740
Lilliana Robinson: Does anyone have any questions?

247
00:48:02.140 --> 00:48:03.180
Lilliana Robinson: Alright.

248
00:48:08.640 --> 00:48:09.919
Lilliana Robinson: see you guys in a bit

249
00:49:55.580 --> 00:49:58.629
Lilliana Robinson: alright. Does anyone want to share, how they're.

250
00:49:58.870 --> 00:50:00.189
Lilliana Robinson: how their group went.

251
00:50:07.870 --> 00:50:09.269
Monica Roy (Seascape Clinical): Yeah, I can go.

252
00:50:09.982 --> 00:50:17.809
Monica Roy (Seascape Clinical): So one of my hypotheses was that I believe that conference sponsorship

253
00:50:17.820 --> 00:50:23.129
Monica Roy (Seascape Clinical): is like industry. Conference sponsorship is an effective way to get leads and increase revenue.

254
00:50:23.280 --> 00:50:34.000
Monica Roy (Seascape Clinical): and it was based on observations. I've had as a conference attendee that lots of clinical trial tech vendors either had a booth or a speaking slot.

255
00:50:34.420 --> 00:50:38.860
Monica Roy (Seascape Clinical): and I would say, that's unproven, because it's not like I've

256
00:50:39.040 --> 00:50:44.689
Monica Roy (Seascape Clinical): sponsored a series of conferences. But my partner was Dave, and he said, when he was at Amazon.

257
00:50:45.022 --> 00:50:54.100
Monica Roy (Seascape Clinical): his experience was that they would lead to leads, and you know, more activity in the pipeline. So it was in. It was interesting to get that feedback from him.

258
00:50:54.160 --> 00:50:58.049
Monica Roy (Seascape Clinical): but I think the only way to really prove it is to actually

259
00:50:58.100 --> 00:51:01.869
Monica Roy (Seascape Clinical): get out there, and, you know. Make the measurement myself.

260
00:51:03.020 --> 00:51:09.940
Lilliana Robinson: Yeah, I definitely. But it sounds like it's very well founded. So what could be like an experiment that you

261
00:51:09.950 --> 00:51:11.520
Lilliana Robinson: design for that.

262
00:51:14.910 --> 00:51:25.839
Monica Roy (Seascape Clinical): would probably have to. And I kind of put this in my financial model, but, like you know, spend money on sponsorships, but break it down into different kinds, like

263
00:51:25.860 --> 00:51:29.659
Monica Roy (Seascape Clinical): a very low, level sponsorship, where they just put your logo on. Stuff.

264
00:51:29.790 --> 00:51:34.900
Monica Roy (Seascape Clinical): another one where you speak, another one where you have a booth, and then see if there's actually

265
00:51:34.910 --> 00:51:40.020
Monica Roy (Seascape Clinical): an uptick in the number of discovery calls booked in a given month.

266
00:51:40.020 --> 00:51:40.600
Lilliana Robinson: Yeah.

267
00:51:40.780 --> 00:51:41.590
Lilliana Robinson: and you remember.

268
00:51:41.590 --> 00:51:45.390
Dave Scott: I'm actually convinced that we can actually get her a speaking slot for free.

269
00:51:46.020 --> 00:51:47.100
Lilliana Robinson: I love that David.

270
00:51:47.100 --> 00:51:49.750
Dave Scott: Only that I'd work with her on proposals.

271
00:51:50.150 --> 00:51:58.670
Dave Scott: I'm positive we can do. I do it all the time for my industry. But, like Monica. Let's do that. Let's live in the next 90 days. Let's get you a speaking slot, so you can prove this thing out.

272
00:51:58.890 --> 00:51:59.850
Lilliana Robinson: I love that

273
00:51:59.980 --> 00:52:17.360
Lilliana Robinson: scrappiness. Remember, Monica, to keep the testing discrete so like you would only test one at a time, or else you wouldn't know like which one is making an uptick, so like do the Low Level first, st and see like, can I spend only a little, or do free and see, and then move up from there instead of doing them all at once.

274
00:52:17.390 --> 00:52:19.729
Lilliana Robinson: But yeah, that's a great example.

275
00:52:20.060 --> 00:52:20.600
Lilliana Robinson: So

276
00:52:21.320 --> 00:52:24.090
Lilliana Robinson: you guys are, gonna do this next one on your own like

277
00:52:24.360 --> 00:52:26.989
Lilliana Robinson: after this workshop. But

278
00:52:27.050 --> 00:52:47.129
Lilliana Robinson: we're gonna rank. The hypotheses that you have based on importance. So as you can probably imagine, high importance is that it's critical and timely to prove or disprove meaning like you're spending money on on this or wasting time on this or like you're gonna be mission critical. If this is not true.

279
00:52:47.766 --> 00:53:09.819
Lilliana Robinson: Right now, and it has a high impact on the company. That's high importance. That's like you might even need to validate this during the 1st half of program, even though we're doing other things. That's how high importance this is. You should have only a few of those, because it's not just the your riskiest ones. It's that it's risky and timely. Then we have medium importance, important, but not critical.

280
00:53:09.940 --> 00:53:20.769
Lilliana Robinson: So for this I mean by critical is much more like timeliness critical. Of course, if it's important, it's going to be critical to your business. But if it's not

281
00:53:20.870 --> 00:53:24.739
Lilliana Robinson: absolutely burning your business right now. It's medium.

282
00:53:24.780 --> 00:53:48.900
Lilliana Robinson: And those are the ones you're gonna test. Once we get to growth month. And then there's low importance, hypotheses, nonessential and low impact on company growth still important to recognize. And once you're chugging along and you have some repeatability, you will get to those to enhance the business, but not probably during techstars. So that's how I would view this. We're not going to do it right now. But remember, high medium low.

283
00:53:49.650 --> 00:53:54.310
Lilliana Robinson: You're going to go on your own with your 10 hypotheses. You wrote down in the 1st experiment.

284
00:53:54.700 --> 00:53:56.570
Lilliana Robinson: or the 1st exercise

285
00:53:56.580 --> 00:54:02.940
Lilliana Robinson: and rank all of them. So you're going to rank them from one to 11 being the lowest 10 being the highest.

286
00:54:03.440 --> 00:54:10.130
Lilliana Robinson: If that's too vague. I I don't really like the one to 10 scale. I just like to put things into buckets, so you can also just rank them high medium low.

287
00:54:10.200 --> 00:54:23.559
Lilliana Robinson: If that's easier for you. If you feel like there's a a gradient. You can do one to 10. But it's really important, because you want to focus on the top 3 unproven hypotheses to design experiments, for during program. I would recommend doing that during growth month.

288
00:54:24.364 --> 00:54:43.999
Lilliana Robinson: And remember that the 10 you wrote down here are not just the 10 that are final. You probably want to go back with your team or by yourself, and write down all of your hypotheses. That you can think of all of your assumptions. Just keep a running, Doc, and then rank them while you're doing that. If they're proven or unproven and then level of importance.

289
00:54:44.850 --> 00:54:46.809
Lilliana Robinson: So let's go into experiments.

290
00:54:48.410 --> 00:55:08.639
Lilliana Robinson: So an experiment is, what actions will you take to obtain evidence to support our hypothesis? So to test our hypothesis, we will blank. That's your experiment. There's also evidence. So like I was saying, you can run an experiment, or you can gather evidence evidence what data will be collected to prove or disprove your hypothesis.

291
00:55:08.830 --> 00:55:23.709
Lilliana Robinson: We will know this hypothesis is true when blank, so there are 2 different sides here, and you can see one is much more Async. And one is sync. So like an experiment, you need to run over a certain amount of time. But evidence you can just devote time during one day to gather

292
00:55:25.330 --> 00:55:48.540
Lilliana Robinson: common types of experiments. These are the top 3. There were many more listed. But you can look up all the different types of experiments you can do. This is from the lean entrepreneur but these were my top 3 favorites. I put them on this slide together, a B testing that is like the most kind of like bread and butter form of testing. You guys have probably heard of it. You're you take 2 versions of something

293
00:55:48.540 --> 00:56:03.070
Lilliana Robinson: you compare them to see which performs better. It's usually in terms of user engagement or conversion. So like, you're putting something in front of users, and they pick one or the other, or you put like 2 sodas in front of a tester, and they pick one or the other. That's A. B testing

294
00:56:03.230 --> 00:56:06.590
Lilliana Robinson: concierge and Wizard of Oz are very similar.

295
00:56:07.040 --> 00:56:24.830
Lilliana Robinson: These are my favorite kinds of tests. They're very good at validating assumptions. The difference is for concierge. The the customer is aware that it is kind of like a low, level version of the product. And in Wizard of Oz you're the man behind the curtain, and they don't know that they're not using an actual product.

296
00:56:24.880 --> 00:56:29.109
Lilliana Robinson: So for concierge, you're providing a high touch manual service

297
00:56:29.380 --> 00:56:33.200
Lilliana Robinson: as a version of your product that you want to build in the future, to up.

298
00:56:33.520 --> 00:57:01.029
Lilliana Robinson: understand customer needs and gather insights before developing an automated solution. So you do it yourself before you spend that money on a product Wizard of Oz is the same thing. You're manually fulfilling it. But it's not going to be like high touch. You're not interacting with the customer. You have a front up, whether it's like a Google form or whatever it is. And you're doing the work on the back end. There's no true automation. You're fulfilling the product's purpose after the person buys to validate whether or not

299
00:57:01.310 --> 00:57:05.010
Lilliana Robinson: this product is going to be able to solve their needs. So

300
00:57:05.010 --> 00:57:29.999
Lilliana Robinson: an example for concierge is we had a company helping nursing homes, teach their elderly people to learn technology. It came out of Covid. A lot of people wanted their like grandparents or parents to be able to zoom with them and things like that. So in our 2022 cohort. We had a company doing that. They wanted to build a whole Saas platform that would like train the the elderly people and also be like

301
00:57:30.000 --> 00:57:48.560
Lilliana Robinson: a help desk platform. But instead of putting a bunch of money into build that product, they just sent a bunch of their teams like their founding team of 5. They just went to nursing homes, and they would spend, like all of Tuesday and all of Wednesday, providing this high touch manual service, they were able to understand the user's needs.

302
00:57:48.760 --> 00:58:11.960
Lilliana Robinson: They were able to talk to the nursing home residents, families, because those are the people who were going to pay for this service, and they were able to gather insights. So they were doing the concierge. They were giving the same service they're going to with their product. But they were doing it manually without paying to build the the development, because they wanted to gather the insights 1st and validate the assumption that this would be something that they wanted.

303
00:58:11.960 --> 00:58:29.177
Lilliana Robinson: It actually helped them validate their assumption because the original plan was to charge the nursing homes as like a perk. If you come here. We offer tech help. We have a tech desk. They realized through their concierge experiment that it was the families who actually cared the most because they had the most stake in the game of the

304
00:58:29.610 --> 00:58:35.390
Lilliana Robinson: nursing home residents being able to use technology. They didn't want to be isolated from their loved ones, so they were the ones willing to pay.

305
00:58:35.905 --> 00:58:39.519
Lilliana Robinson: I'll explain, Wizard of Oz in a second. But I want to take your questions, Neroad.

306
00:58:40.420 --> 00:59:03.309
Nirat Attri: Yeah, on the concierge model. Something which worries us because, like, that's also how we try to run our experiments. Is, how do you focus on transitioning like, what's the right point at which you decide that? By the way, now, I'm going to stop this and focus on the technological aspect of building this as versus, like doing more sales and like gathering more data, because that's where like the temptation

307
00:59:03.310 --> 00:59:11.089
Nirat Attri: lies. And then, of course, like just shutting everything down and just working on the Sas and like working on the technology aspect. I mean.

308
00:59:11.410 --> 00:59:14.899
Nirat Attri: is it sustainable in the long run? So there's like, are some questions.

309
00:59:15.090 --> 00:59:15.980
Lilliana Robinson: Yeah.

310
00:59:16.180 --> 00:59:38.899
Lilliana Robinson: So there, that was kind of like 2 questions, right? Like, when do you change? And then what do you do with the customers. So when do you change that? For, like a concierge experiment, I would say once you've completely validated it. So once you can say like beyond unreasonable doubt this is true like this is what we want to build. We want to spend all our money and time into building this, and I would say jobs to be done. It's going to help you with that

311
00:59:39.020 --> 00:59:50.710
Lilliana Robinson: when you know your job statement, and you're also testing that in the real field with this concierge service for that segment that you came out of jobs to be done with. You can say like, Yep, this is what we want to build.

312
00:59:50.760 --> 00:59:58.469
Lilliana Robinson: So, of course, like you'll, it'll always be like 99.9, because you'll never know it'll work for certain. But as certain as you can be without

313
00:59:58.710 --> 01:00:18.889
Lilliana Robinson: spending too much time, because there is such thing as like prolonging your your product growth. Right? You should get to it at some point, but as much as you can validate before then is good. And then what I've seen people do is a couple of things you can offer the new service to the people you're doing the concierge with, and say, like, Hey, we have this new product.

314
01:00:19.170 --> 01:00:42.749
Lilliana Robinson: you should continue servicing them while you're building like you shouldn't just let them drop off. You can try and have them convert, or some people just end up having a few that they keep doing concierge with while they're selling their new product. And then they slowly like drop off or convert, or you might just serve them for a while. So there are like many different things you can do, depending on your specific business, but I've seen all of them.

315
01:00:42.750 --> 01:00:53.279
Lilliana Robinson: I think, just making sure that you're respecting your reputation, like treating the customers well and still being there for their needs and trying to help them. Transition to your new app is the best way to do it.

316
01:00:54.370 --> 01:01:16.209
Nirat Attri: Can I ask one? Follow up question? I mean, like, yeah. So I think one thing that we receive as a feedback when we talk about this is, customers tend to be like they they get worried. Because again, the idea is they're used to having a service model. So they're saying that. By the way, if you grow and you technologize like we're not sure if you want to stay our customers because they see it as a risk

317
01:01:16.700 --> 01:01:20.060
Nirat Attri: to their own business. So they're like, if we become commoditized for you.

318
01:01:20.446 --> 01:01:26.650
Nirat Attri: Then there's a chance that you stop focusing on our business. So how do you navigate something like that.

319
01:01:27.200 --> 01:01:47.689
Lilliana Robinson: I I think we would have to work on it with you personally, specifically, to deal with those customers. But like in general, yeah, that's why a lot of people do kind of keep the the white glove approach with their 1st initial customers. If they're not going to move over. I know that happened with basecamp? So we can just work on it with you specifically and like, decide what's best for your group of customers.

320
01:01:48.150 --> 01:01:49.150
Lilliana Robinson: Annie.

321
01:01:51.690 --> 01:01:57.149
Annie Brown: Yeah. So I, I really like this idea of the Concierge and Wizard of Oz, because we've had

322
01:01:57.450 --> 01:02:11.400
Annie Brown: customers say, like, Oh, while you're building the software. Would you be interested in like consulting or like helping us build? And so we've actually turned that down because we're like, no, we've got to focus on the product. But now I'm thinking.

323
01:02:11.460 --> 01:02:24.459
Annie Brown: we can like kind of reframe that consulting work as as concierge work. My question is in that example that you had did the

324
01:02:25.230 --> 01:02:28.740
Annie Brown: Are they? Did that company charge

325
01:02:28.790 --> 01:02:36.450
Annie Brown: for those that hands on service? Or did they offer it as like a free pilot, and like framed it as like testing.

326
01:02:37.410 --> 01:02:39.580
Lilliana Robinson: It evolved. They started

327
01:02:39.650 --> 01:02:46.760
Lilliana Robinson: free in the nursing homes because they they were trying to get the nursing homes to charge. They were doing like free pilots with like 3 nursing homes.

328
01:02:46.760 --> 01:03:10.170
Lilliana Robinson: And then, when they realized they needed to charge the families, I believe they started charging the families for the service for the concierge service. So they were making money. And then they started making small like apps that they could send out so or small bits of software. They started making like a phishing training for like emails, because the old people started getting like fish, very commonly like during Covid so they let the

329
01:03:10.360 --> 01:03:18.180
Lilliana Robinson: families buy that for their loved ones. They just started making offerings more and more to the point that they got to

330
01:03:18.260 --> 01:03:23.609
Lilliana Robinson: offering their full service that they wanted to, but it was just them doing it manually, and they were charging.

331
01:03:23.760 --> 01:03:26.860
Lilliana Robinson: And then they were able to build a software.

332
01:03:26.970 --> 01:03:44.549
Lilliana Robinson: After that I would just caution you any one. You don't want it to be doing a bunch of free work, so make sure that you're doing the concierge as an experiment like you have a valid, an assumption you want to validate. So you go and get that hands on experience experience with the customers for a reason.

333
01:03:44.560 --> 01:04:01.220
Lilliana Robinson: And you know that you end it because you want to go, actually make it into a development a developed software. So it needs to be something that is very measured, that you're very aware of, that you're making sure is not a time suck that you're doing to gain specific insights for your business.

334
01:04:01.430 --> 01:04:11.739
Annie Brown: Yeah. And that way, it's also like hyper focused on testing the product. Because if you just label it consulting, it can be like so broad, and it's not helpful. So I think that's that's great.

335
01:04:12.020 --> 01:04:39.490
Lilliana Robinson: Yeah. And like, you might even want to do kind of like Wizard of Oz, where it's like, we have this product fill out this form here, and, like, you know, our our back end will like analyze it for you. But it's really just you guys. So that's what Wizard of Oz is I was gonna get to for you guys. A lot of my students at Northwestern will do Wizard of Oz to validate. They'll just send out a Google form and say, we do this thing for you like we match you with, a friend to study with, or something like that, we have an algorithm

336
01:04:39.490 --> 01:05:00.189
Lilliana Robinson: fill out this form. And really, it's just them going through a Google sheet. And they like match people and then send it. And so that's the Wizard of Oz, because you feel like you're signing up for this whole like, you know, service. But it's just the people doing it, and that's how you validate it. It doesn't make it any less valid for someone to pay you, because you're doing the same service. It's just that you're putting in a lot more work than if you built the software. Obviously

337
01:05:01.110 --> 01:05:06.800
Lilliana Robinson: cool. We have only a few minutes left. I'm gonna just show you the rest of the slides.

338
01:05:06.900 --> 01:05:25.250
Lilliana Robinson: You guys can fill in the last bit of this chart. You're gonna go on your own. Look at your hypotheses, rank them by importance, like we said. Then design experiments. Even if it's just a brainstorm you don't have to go full into, like all the details of the experiment, but just jot down for your future self. Here's something that could validate it.

339
01:05:25.580 --> 01:05:37.689
Lilliana Robinson: something I could design. So you're not going to do this now, but for each of your hypotheses, not just your top 3. All of them design an experiment that can prove or disprove the hypothesis.

340
01:05:39.582 --> 01:05:41.069
Lilliana Robinson: That's the end. Here

341
01:05:41.300 --> 01:06:05.750
Lilliana Robinson: we have about 2 min left for questions. But there's another feedback form here so that you guys can let the education team know what you thought of this workshop. I thought, this is really cool cause. This is not a skill that we have taught in the past, and I appreciate the really fruitful conversations we had here from you guys sharing. So just remember that Neil and I and the whole team will be here helping you guys experiment and test these assumptions and validate them. The whole program.

342
01:06:05.750 --> 01:06:16.010
Lilliana Robinson: And afterward, this is really like your whole journey of being a founder is just testing and validating assumptions and then making bets based on those so get really used to this skill.

343
01:06:16.050 --> 01:06:40.379
Lilliana Robinson: Go back through this list, keep writing down your assumptions, ranking them designing experiments, and we'll work together to do this. But it's really important that you're not doing things that are unproven. So I'm really glad that we had this opportunity to put this warning bell here. If anyone feels like they're going after something that's unproven, or that you may have already made a big bet. Don't worry. Send us a vox, or we can talk about it. I want to make sure nobody is, you know, making these

344
01:06:40.730 --> 01:06:46.160
Lilliana Robinson: tumultuous decisions. Between now and week 6. So let's try and get ahead of that. This program.

345
01:06:47.750 --> 01:06:49.480
Lilliana Robinson: I'll stick around for questions.

346
01:06:52.560 --> 01:06:53.840
Lilliana Robinson: Thanks. Everybody.

347
01:06:54.210 --> 01:06:54.720
Dominique (CEO) & William Stone (COO)_Agent Lunar: Right.

