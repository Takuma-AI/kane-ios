WEBVTT

1
00:00:04.220 --> 00:00:07.460
Thread Genius: We are recording. You guys can hear us and see us.

2
00:00:07.940 --> 00:00:12.069
Thread Genius: And I think, we'll get started. So, David, if you want to take it away.

3
00:00:12.270 --> 00:00:17.890
Thread Genius: Sounds good. You want to go to the 1st slide here, Matt, or next slide. Yeah.

4
00:00:18.466 --> 00:00:44.579
Thread Genius: Hey? Everyone for for those who I don't know I'm David Ambler. I'm a corporate startup partner at Paul Hastings. We have been working with techsars as a global legal partner for about 6 months, and we already are working with a lot of you already. As you. I won't give you a a commercial, but just generally we have a package with you all in terms of discounts and referrals and free corporations, etc.

5
00:00:44.580 --> 00:01:08.450
Thread Genius: The focus of today is obviously to talk about AI, and maybe at the outset. I just wanted to give you from a corporate perspective just a few slides, and then I'll turn over to Amir and Katie on the market. I promise I won't go through this deck. Yeah.

6
00:01:08.450 --> 00:01:18.410
Thread Genius: I know it's it's it's hard to keep up and Amir and I have done a bunch of big AI deals together. But I just thought it was interesting. This is only data.

7
00:01:18.460 --> 00:01:41.799
Thread Genius: AI and Ml, obviously some massive deals in here. But, as you can all see lots of activity, lots of huge deals. And I think that's dominated at the very early stage. What we're what we're seeing. And I'm sure what you all are feeling and seeing as well. But I mean it used to be that we would do these early stage deals, and they all sort of felt the same. We track the data, and it would go up or it would go down.

8
00:01:41.810 --> 00:01:56.560
Thread Genius: And AI is really skewed. Some of these deals, especially as we add commercial components to these, I mean Mira and I just did a 400 million dollar series, a financing with billions of dollars in commercial attached to it. So there's a lot more scale and complexity that are added to some of these deals.

9
00:01:56.820 --> 00:02:12.549
Thread Genius: I'm going to skip this slide just because it's interesting, but probably better to look at when you have more time. If you go to the next one, Matt, this is one where I just think it's interesting to focus on, obviously for those who are joining there. You all are interested in AI, and many of you have

10
00:02:12.620 --> 00:02:26.189
Thread Genius: native companies. But I mean, the data is clear. And this is Carta again. But the data is very clear, and we're seeing it, too, just in terms of negotiations that pre money evaluations for series. A companies are are just higher. Right? They just are.

11
00:02:26.190 --> 00:02:48.060
Thread Genius: And I think whether you're AI native or not, I think if you have some play on AI. We're just seeing it, that you get a better valuation. You get more term sheets, which is not stated in this data, more competition. How fast can we close Kai Vibe? And it really feels like the vibe, at least for me and my practice. And I've worked with hundreds of companies over the years and still do work for the investor side.

12
00:02:48.060 --> 00:03:11.359
Thread Genius: It really feels like the 2021 buy for AI companies in terms of like, how fast can we close. They tripled revenue in the last 4 weeks. We really want to get this done as soon as possible. And you can just see in the data, 40% boost for AI valuations for the seed, and about 25% boost for the series, a something that's not in the chart that I do want to mention. That's sort of a very live topic, would would

13
00:03:11.360 --> 00:03:34.870
Thread Genius: some folks I'm working with? I think there's a pedigree point in evaluations, too. I've noticed very anecdotally that if you either have a founder who wrote a famous research paper, or is an alum of either, you know the Hinton lab, or one of the, you know, very famous cutting edge research labs.

14
00:03:34.870 --> 00:04:03.199
Thread Genius: There's definitely a premium that investors pay to become a part of that. And then, you know, separately. If you don't have that, you have to show much stronger proof of concept in order to get over the funding hurdles for sure. Yeah. And Amir will get into a lot of the tech side of the AI in general. I just want to give folks kind of a little bit of a shot of what we're seeing. Let's go to the next slide, Matt.

15
00:04:04.300 --> 00:04:33.429
Thread Genius: I'd say, just to double click on kind of the series, a series C, even if you raise more cash. You're getting less dilution right? Obviously, that's a little bit as implied in a higher valuation. But you know, having some play here, you could raise in some cases a million or $2 more in seed, and 4 or 5 million dollars more in an A and you're still coming out better than have you done? Non AI. So just as you think about kind of how Vcs are looking at companies and paying valuations.

16
00:04:33.560 --> 00:04:50.880
Thread Genius: it's interesting. Don't just focus on pre-money valuations, especially if you're a founder. And you're thinking about, how much am I being diluted, etc, really focusing on kind of the catchphrase and dilution, and Carter has some modeling. But we've done a lot of modeling around this, too. So again, just an interesting data point, let's go to the next one, Matt.

17
00:04:51.230 --> 00:05:16.069
Thread Genius: This we have a separate topic. We've done a training on, and we are doing a training with some of your cohorts on safe. So I go too deep here, but, as you all know, safes are still by far and above the most popular way to raise, especially if you're raising under 2.5 million we talked about in our safe presentation. Kind of what happens when you raise above that short answer is, you're more diluted as a founder, the more you raise on safes just given the way they're structured.

18
00:05:16.230 --> 00:05:31.009
Thread Genius: There's been a frankly, a lot more, you know. Say philosophical pressure on our shapes, the right mechanism for the next 5 years of investing recently and again, we'll cover that, maybe with some of your cohort separately, Matt. Let's go to the next one, and then we can turn it over to me

19
00:05:31.100 --> 00:05:45.589
Thread Genius: again. I think this one's interesting just to kind of double click on in terms of dilution. I was just working with another Tech Stars Company 2 days ago on hey? I raised money in another jurisdiction. We're doing a flip which you know we're doing lots of flips for tech source companies right now.

20
00:05:45.660 --> 00:06:07.190
Thread Genius: You know how much dilution is. Too much dilution without saying too much detail. Turns out that their dilution didn't really track kind of the typical dilution. Let's just put it that they were getting diluted by more than 10% for under 250 K. Raise, which if folks have not been focused on dilution. And again, this is part of the reason. I think safes are getting more and more pressure.

21
00:06:07.190 --> 00:06:22.800
Thread Genius: It's just really hard to know how much you're being diluted, even if you're safe stacking at higher valuations every time. It's really hard to track. And I think even really smart people get confused about dilution. But again, I mean, Carta has modeling for this for free. We can also help you with this, but

22
00:06:22.800 --> 00:06:48.109
Thread Genius: focusing on kind of like, how much you're raising and what the ultimate dilution is. Even if you're an AI company is super important and unfortunately partners and track. AI versus non AI. But we certainly are seeing data there. That shows at least anecdotally that post money caps are higher for AI. So anyways nice to meet you all, at least wanted to give a plug for for some of the things we're seeing on the corporate side before turning it over to Katie and Amir, who are the real show. So nice to see you.

23
00:06:48.720 --> 00:06:50.400
Thread Genius: Yeah, thanks, David.

24
00:06:51.630 --> 00:07:14.109
Thread Genius: start. Intro. Yeah. Sure. Hello, everyone. My name is Katie Katsuki. I recently joined the Poly 6 team along with Amir Gabby, and we work in the tech transactions practice. And we focus on everything related to AI emerging tech it. We advise AI model developers. We advise companies who want to commercialize product counseling

25
00:07:14.280 --> 00:07:41.609
Thread Genius: copyright everything in between. We've worked on some deals and some financings and fundings, and we are excited to be here to share with you a little bit about our perspective throughout the last couple of years, watching the market, learning about the regulatory compliance considerations and seeing what's happening today, we're going to take a fine tune a little deeper, and and we'll go over some of the key topics that you should know about the current regulatory landscape here in the Us.

26
00:07:41.610 --> 00:07:48.919
Thread Genius: And we'll discuss some best practices. What we see in the market. With respect to building the application layer and fine tuning AI models.

27
00:07:49.340 --> 00:07:57.709
Thread Genius: Yeah, everything, she said. I lead the AI practice. And at Paul Hastings I think only things, I would add, is.

28
00:07:57.710 --> 00:08:21.409
Thread Genius: we really our practice has been somewhat unintentionally but exclusively been focused on AI since that Cambrian summer of 2022. We represented the folks behind stable diffusion which felt like broke the Internet for a while, and I've been involved in a number of the copyright AI litigation. So, as Katie said, we've been helping folks

29
00:08:21.410 --> 00:08:34.080
Thread Genius: think about how to build commercialize deploy deal with regulations of the models, but then also deal with some of the IP aspects as well. So without further ado, let's let's get to the meat of this.

30
00:08:34.179 --> 00:08:56.089
Thread Genius: Sure. So we figured it made sense to talk about the current landscape here with respect to AI regulation at a very high level, as some of you may know, the EU AI act is the world's 1st comprehensive legislation regulating AI, which is currently in effect, and over the next 2 years various additional obligations and compliance requirements will take will come into effect.

31
00:08:56.160 --> 00:09:24.540
Thread Genius: you know, based on the type of model and the scope of risk. So and and why does that matter for founders? Right as you're building and thinking I'm guessing no one on here is building a foundation model. If you are, we should have a separate conversation. But if you're building an application on top of one or many models. Right? Ultimately, you're going to deploy your application somewhere. And really, the frame here is to think about

32
00:09:24.540 --> 00:09:49.819
Thread Genius: in this fractured world where there are jurisdictions, who are either regulating somewhere. Others are thinking about regulating. You're releasing your product, you know, in some degrees, globally, and some it is going to be nationally. And then what we're seeing now is essentially bifurcation of the EU market from the Us. Market. The EU market definitely has some non-voluntary rules

33
00:09:49.820 --> 00:10:14.340
Thread Genius: that you will have to comply with if you're releasing your applications in that jurisdiction. Sorry just to interject to just on the corporate side. I mean, I get a lot of questions, and especially on the investor side of like, Hey, I'm just a 1 or 2 or 3 person company like, even if I did care. How can I care right like there's so much out there. And obviously we'll talk a little bit about that today. I mean, the reality is

34
00:10:14.340 --> 00:10:39.310
Thread Genius: when I'm on the investor side. These questions still get asked of like when you're building your company like, what? What's your like product roadmap like? Why is this stuff matter for me? I can tell you, investors are looking at this. They're thinking about it, not only just from a risk perspective, but like a scale perspective. And I've heard that more and more with investors of like is this a scalable business based on like the approach they're taking? Or there's some risks there that really are value driven

35
00:10:39.310 --> 00:11:01.500
Thread Genius: you don't want to do is get stuck in tech debt where you can't scale and deploy to different markets. Right. So to put it more bluntly, if if you build your primary proof of concept and you're ready to go to market, and it's not compliant with, let's say, the EU market, where it's not compliant with California's view on what you know, AI deployed technology should look like.

36
00:11:01.890 --> 00:11:13.780
Thread Genius: In effect, what we're seeing is, you know, people have to think about raising more cash to basically rebuild the product and make it compliant. So all to say, as you're thinking about how to build

37
00:11:13.780 --> 00:11:32.460
Thread Genius: the the product. Not that folks need to undergo a deep audit or have a you know. Phd, in, you know, European Parliamentary law on AI. But to understand generally directionally where you have to land we, we think, is a much smarter way to build.

38
00:11:33.190 --> 00:11:56.860
Thread Genius: Yeah, future proofing is a huge part of our, you know, regulatory compliance aspect, because we want to make sure that people are aware of what's coming down the pipe, both in the us and the EU, the Us. We have more of a patchwork framework approach similar to how privacy law developed. Here and there was an executive order that was rescinded. And now the current administration has sent out an Roi

39
00:11:56.860 --> 00:12:11.540
Thread Genius: for an AI Action plan. So we're monitoring that they have about 8,000 responses already. So we've been, you know, keeping up to date on the current and proposed bills throughout the States, and how they affect developers and employers of AI systems.

40
00:12:12.800 --> 00:12:35.310
Thread Genius: As Mayor mentioned, we are working on some of the current copyright litigations. There's almost 40 in the United States alone, primarily in the northern district of California and the southern district of New York. These fall into various buckets for AI developers. There's Codex cases which are some of the original cases. There's image generation. You have your Chatbots, your large language model.

41
00:12:35.580 --> 00:13:04.159
Thread Genius: you know. Open. AI is the defendant. Many of these anthropic. And what's interesting is, we're seeing plaintiffs. When the cases 1st started. Most of them were class action, individual plaintiffs. And now we're seeing some shift into publishers, media companies, and corporations bringing these cases as well. So what we haven't seen is so far, plaintiffs suing either platforms right? So we haven't seen.

42
00:13:04.430 --> 00:13:18.990
Thread Genius: Somewhat to my surprise, we haven't seen hugging face sued. We haven't seen Amazon Sue, for you know, bedrocker, sagemaker, we haven't seen seen any of the other, you know, major platforms and basically, these claims just to break it down real quick.

43
00:13:19.500 --> 00:13:28.829
Thread Genius: At a very high level. They fall into input views, input cases and output cases, which is to say, some of the claims involve

44
00:13:29.120 --> 00:13:34.040
Thread Genius: you. You use my copywriting works to train your model.

45
00:13:34.080 --> 00:13:58.330
Thread Genius: That's bad, and violates copyright. I disagree or output. Your the outputs of your models infringe my particular work. It's too close in the parlance we call that substantial similarity, and that one is a no one knows right? You have to determine that on a case by case basis can't really resolve those in anonymous way. It's it, you know. If if it is

46
00:13:58.330 --> 00:14:22.990
Thread Genius: substantially similar, then it may be a violation of copyright law, but you have to look at that on a case by case basis and just jumping in from the corporate perspective. I did a financing, and I'll be very generic here, because this is a public case. There's lots of confidential information. Maybe 8 months ago I did for the investors and investment into an AI native company. And we had our typical AI questions like, Where's your data set? Come from? It's behind a Paywall capture a snippet.

47
00:14:22.990 --> 00:14:41.680
Thread Genius: Is it all public? We ask all the questions that typically get asked in venture financing at the very early stage, and remember, venture investors are doing diligence, not M. And a style diligence. So they're not doing like months of diligence. This isn't really a risk shifting exercise so much as it is understanding the model and making sure they understand where the business is going.

48
00:14:41.680 --> 00:14:49.340
Thread Genius: But we got a lot of yes, it's all public. No, no, it's all fine. And then literally 3 weeks, or whatever was 4 weeks after we closed.

49
00:14:49.340 --> 00:15:14.060
Thread Genius: they got sued by a big company for saying, you basically use. I can't remember if it was inputs or outputs. But yeah, it probably was input but regardless, like, even in cases where there's diligence done on both sides, there's good law firms, and everything looks fine at the end of the day. It's up to you to understand a basic level like what you're doing here, because, regardless of what the lawyers say, or you've done financing.

50
00:15:14.110 --> 00:15:18.249
Thread Genius: you still have risk, and you'd understand, kind of how to how to drive a business.

51
00:15:18.370 --> 00:15:46.179
Thread Genius: and some of the work that we do. And the data points that we pull from these ongoing litigations is, what are the current data sets that are being litigated? What are the disclosures that were made by developers and helping companies when they release documents, or they release papers about their systems. So we advise on all sides of that there are also several litigations happening around the world. Canada, Netherlands, France, Germany. A lot of these cases follow similar fact patterns to the New York Times case

52
00:15:46.180 --> 00:15:55.580
Thread Genius: where various media publishers are suing for copyright infringement. That's the input case that Amir had mentioned. And we're seeing that happening in India, Korea as well.

53
00:15:56.890 --> 00:15:58.789
Thread Genius: Okay. So

54
00:15:59.080 --> 00:16:07.779
Thread Genius: we figured it made sense to talk a little bit about foundation models and sort of the basic foundations of foundation models before we dig a little deeper into

55
00:16:08.080 --> 00:16:30.759
Thread Genius: into the fine tuning. So foundation models are what you think of these large scale, general purpose models that are trained on large amounts of data sets. And that could be text that could be codes that could be images that could be videos. And so they are able to do various tasks. As you are all probably familiar with text summarization, translation code generation.

56
00:16:31.100 --> 00:16:40.620
Thread Genius: And so, yeah, I mean what we've seen, right? So 20, if 2022 and 2023 were the years of mode around the model.

57
00:16:40.620 --> 00:17:03.589
Thread Genius: We've seen that erode a bit in 2024, and certainly, I think, with the deepseq moment, and we'll talk to you about Deepseq in particular, more later. But as folks, the tech has moved towards reasoning models in, let's say December onward with a couple of interesting releases from anthropic Openai and and the deepseq. What we've seen is.

58
00:17:03.590 --> 00:17:15.089
Thread Genius: as you probably felt, the shift in value or perceived value moving from the model to the application. And so we, we have a little bit of a theory on that which goes back to

59
00:17:15.089 --> 00:17:25.679
Thread Genius: 2022, and our work with the foundation model developers, I think, at a high level and a philosophical level. When you're talking about the foundation model developers. And I think this still holds true.

60
00:17:25.680 --> 00:17:50.509
Thread Genius: They are still largely unapplied research labs. Right? They're trying to figure out agi and semantic reasoning. They don't really care frankly about all the downstream applications, which is really the level that you all are focused on, which is, how do we take this abstract idea of semantic reasoning engine? And how do we put it to work and create some notion of value? And so

61
00:17:50.510 --> 00:18:12.439
Thread Genius: I think naturally, you ex. You expect the application layer and the domain specific experts that to have a better sense of that. And I think that's where we're seeing deployment as as folks are more focused on. What is the Poc. How can we build an application that that you know customers actually want as opposed to something that just.

62
00:18:12.450 --> 00:18:22.539
Thread Genius: you know, shows potential. I think the other thing that we've seen, you know, in addition to to the reasoning models, is smaller models, there's been a shift

63
00:18:22.580 --> 00:18:46.190
Thread Genius: towards smaller bottles, as folks have frankly gotten the the bills from the hyperscalers and the cost of deploying these models. Right? So I would say, going back again historically, it's kind of laughable. When we talk about history. We talk 3 years span, but historically 2022, 23 folks were focused on the cost of training models, right?

64
00:18:46.290 --> 00:18:56.989
Thread Genius: Told us. You know, hundreds of millions. This huge amount. It's really expensive to train and pre train a large, very large language model. So. But folks, I think, on the deployment side

65
00:18:57.140 --> 00:19:11.560
Thread Genius: didn't really give so much thought on the cost of inferencing. And so what we've seen is essentially a tax on successful poc, right? As you roll out either an application or something that gets adopted at the enterprise. If you're not using the right

66
00:19:11.869 --> 00:19:35.679
Thread Genius: level of model right? If you're if you're using a model that's too capable, you're going to get hit with an inferencing tax, and people get the bill back and and they're asking, Where's my Roi? It's great that I can automate this task. But it was cheaper to actually run the task manually through people than it was to automate it. And so that's part of the shift for the demand for for smaller models. And

67
00:19:35.680 --> 00:19:42.940
Thread Genius: I think you know the bottom of this slide. We have some examples of of some of the recent models that have been released.

68
00:19:43.070 --> 00:19:59.929
Thread Genius: And the interesting point, too, about smaller models, is that some of the proposed legislation, including what was in the executive order, and some bills that were passed in California, tried to regulate model based on the that threshold, and that was sort of a parameter that was used. So as these models get smaller and smaller.

69
00:20:00.020 --> 00:20:25.699
Thread Genius: that data point becomes less applicable and almost becomes useless. So some of the new bills in California are using indicators such as, how many users do you have in the State of California to regulate as that model shifts. Yeah, as regulators have gotten more sophisticated, we're throwing out that kind of ridiculous notion of 10 to the 26 flops as a threshold

70
00:20:25.700 --> 00:20:31.500
Thread Genius: for highly capable models. Because exactly, we have a client which

71
00:20:31.865 --> 00:20:55.249
Thread Genius: their bailiwick is highly capable. Small models, right? But most of their models that they're developing are are under 40 v. 1 billion parameters. So you know you you would never hit that 10 to the 26 threshold. And and yet in benchmarks, you know, performance benchmarking. They're outperforming some of the very large models. Let me go to the next slide.

72
00:20:56.070 --> 00:20:58.299
Thread Genius: So fine tuning or

73
00:20:59.410 --> 00:21:08.150
Thread Genius: so. 1 point about foundation models is that they they learn by adjusting their internal parameters. And so these

74
00:21:08.730 --> 00:21:13.209
Thread Genius: they don't store the raw data like a database, but instead, they make inferences.

75
00:21:13.270 --> 00:21:41.210
Thread Genius: patterns, statistical correlations, abstractions from that data. And so what's left is the model weights, right? The parameters. And that's sort of the key point of foundation model. So ways in which foundation models can be adjusted to adapt to specific tasks and specific domains include fine tuning, so it specializes certain abilities for different practice areas, different industries. And so what we've seen is

76
00:21:41.310 --> 00:21:50.050
Thread Genius: fine tuning is achieved by freezing and further training and already trained model on a smaller data set. That's specific to a task. And so

77
00:21:50.140 --> 00:22:04.269
Thread Genius: you can compare this with Rav, which might take a model and just provide it with new information, with fine tuning. You're taking the models, internal weights. And you're further training it on new data. So it's sort of a

78
00:22:04.270 --> 00:22:27.009
Thread Genius: the, their distinctions between the 2 sense. Yeah. And we've seen with corporate deployments. We've seen people both fine tune and mix it with rack for digital context, I think for your perspective of of, you know, being domain experts and building the application layer. I think you all should be ready to think about the world in which companies, you know

79
00:22:27.710 --> 00:22:46.049
Thread Genius: come to you with endogenous and exogenous data. What I mean by that is exogenous data meaning, hey? We went to this data broker. We bought a bunch of data and then endogenous data, the data that they accumulated just in running their business. Now, sometimes that is just, you know.

80
00:22:46.050 --> 00:23:04.600
Thread Genius: systems, utility data. Sometimes it involves their own customer data. And what we've seen is that these companies aren't really sure how to handle these disparate buckets. The thing to just flag is again, as you're building into David's point, you know, not getting caught up in a weird tech stack.

81
00:23:04.600 --> 00:23:10.169
Thread Genius: is you, you know, with increased focus, not just on data privacy.

82
00:23:10.170 --> 00:23:34.170
Thread Genius: But there's there's there's notions of confidentiality. Right companies don't want to be caught in a in an embarrassing situation where they trained a model, whether it's through file, tuning or otherwise. And that model then begins to reveal unintentionally elements of that data. You know, it could be internally right, because there's no provision control.

83
00:23:34.170 --> 00:23:41.229
Thread Genius: or it could be externally to to 3rd party. So I think you know one of the themes that we see with.

84
00:23:41.550 --> 00:23:55.359
Thread Genius: you know the the corporate AI governance policies and deployments is this notion of how do we handle the different types and buckets the streams of data that we're dealing with.

85
00:23:56.940 --> 00:24:21.840
Thread Genius: So you know this, we can fly through this. I mean, this is a little bit sort of technical about the different types of ways to fine tune models. You know. Look, I think you all are technical smart enough to know that. You know certain situations are going to demand different techniques. I think. You know, the the state of the art on this is always moving. We're gonna talk

86
00:24:21.840 --> 00:24:33.929
Thread Genius: about. We're gonna we're gonna talk a little bit later in our presentation. About the sort of the new, the Mcp Protocol, the Open source protocol that anthropic is

87
00:24:33.930 --> 00:24:49.530
Thread Genius: promulgating. But suffice it to say that there is no consensus on what even fine tuning means, and I think you should be ready again to deal with disparate types of data in different contexts as you as you think about.

88
00:24:49.530 --> 00:25:14.090
Thread Genius: You know how AI works with your tech stack. Yeah, in your in the use case that you need it. For. So here are just some examples that we mentioned earlier, where we see industry, specific domains being trained. Healthcare finance. Bloomberg, Gpt is a great example. Yeah, I mean in finance, one anecdote is like we had a client basically say, Yeah, it's great that the very large language model can write a Haiku. But we're, you know.

89
00:25:14.090 --> 00:25:36.459
Thread Genius: a financial institution like we don't need to write haikus. We need to. You know, we needed to understand financial data and look for patterns there. So you know it sort of go it. It sort of supports this notion of smaller, more purpose. Driven models as opposed to, you know, an all you can eat buffet that has a little bit of everything.

90
00:25:37.010 --> 00:26:03.629
Thread Genius: And in the legal industry we see this a lot research contract analysis, specific use case, you know, within various practice groups have developed their own fine tune. So we see this across various. Yeah. No one likes lawyers and some other, you know benefits of fine tuning. You can develop your custom tone. It's a lot more personalized, a lot more intentional to your business, to your brand.

91
00:26:04.050 --> 00:26:10.770
Thread Genius: It helps with language translations, and they can also be used with code generation as well.

92
00:26:11.090 --> 00:26:36.020
Thread Genius: And this is a good visual that I liked from from anthropic. So one thing that we spend a lot of time on is seeing what's out there in the market with fine tuning. So there's fine tuning available through Apis, and there's also offerings that you can self host and have it on prem. And so we monitor this because these terms of use change almost daily for a lot of the large providers, Microsoft, Openai, Amazon.

93
00:26:36.020 --> 00:26:45.145
Thread Genius: on just to name a few, and we note the difference in offerings and the product offerings that they have, if you want to build upon their existing

94
00:26:46.020 --> 00:27:04.860
Thread Genius: their existing models. And so some things that we look out for is ownership right? Who owns the fine tune, who owns the outputs. Most often the user owns the input and the output. But different providers have different variations about who owns the fine tune. And so one thing that we advise on is what type of

95
00:27:05.030 --> 00:27:29.500
Thread Genius: use case that you have. What's the better provider for for you? Cost also. Some are token based, some bill for inference. And so, as you could see, these are just a very high, level snippet of 6 providers. There's a lot more out there. But something that we're constantly learning about and focusing on is, you know, what's being offered out there, and how how this has changed even in the last

96
00:27:29.710 --> 00:27:52.359
Thread Genius: couple of yeah. And again, I I think choice matters right. We're seeing 2 different ways to approach this. Some. Sometimes, you know, very early stage companies are saying, we we need to prove prove the model. And we still need to prove the concept. And so we're gonna stick with one model developer. We're going to find some through their Apis and reduce our upfront costs.

97
00:27:52.410 --> 00:28:17.059
Thread Genius: which is great. But you know, you're gonna suffer on performance. You're gonna suffer on cost throughput and then also, you kind of, you know, have to manage the risk of going showing up to a customer that says, well, it's great that you are with model provider A, but we have a preference for model Provider B, and you got to figure out about whether you know you've built enough

98
00:28:17.080 --> 00:28:23.469
Thread Genius: interoperability in your system to work with different model providers. And, by the way.

99
00:28:23.670 --> 00:28:33.460
Thread Genius: let's I'm gonna go back to it again. Let's not forget the deep. Seek moment. In December, I mean, we've been fielding a number of calls from

100
00:28:33.740 --> 00:28:58.670
Thread Genius: anywhere, from startups to frankly, very, very, very large tech companies that you might not expect who have asked us? So you know, legally and otherwise. Do you see any issues with us, you know, building on top of deep seek, or you know, or or hosting deep seek on our platform. So you know, I think we we've been.

101
00:28:58.670 --> 00:29:09.649
Thread Genius: And the reason, though, you know, can't bury the lead on that. The the reason people are asking that is because the inferencing costs are like

102
00:29:09.790 --> 00:29:38.650
Thread Genius: 20%. Yeah, about 20% of what? What? You know, the rack rates are for some of the larger models. We have a quick question here, and we can address at the end. We are going to be going off reporting at the end for the final 10 or 15Â min. Just so the policy team?

103
00:29:39.970 --> 00:29:46.280
Thread Genius: Yeah, why don't we say the questions for for the end? Do I mention liquid?

104
00:29:46.390 --> 00:29:51.640
Thread Genius: So yeah. So look, I mean, I think 1 1.

105
00:29:51.930 --> 00:30:16.469
Thread Genius: So the other. One way of viewing the world is, we're going to hook up to the Api. We have to do less work. On setup because we we, our engineering dollars, should be spent elsewhere. However, you know, depending on what domain and what meaning, what industry, you're going to try and serve. Some of these are highly regulated. So insurance banking healthcare, highly regulated industries

106
00:30:16.470 --> 00:30:22.880
Thread Genius: turns out that you know the highly regulated industries tend to want on Prem, you know, tech.

107
00:30:22.880 --> 00:30:47.569
Thread Genius: And that's because it already passes through their security audits. And whatnot. Right? You're not, gonna you know, as a small startup asking, you know Big Bank or someone else to to, you know, fit or change their security posture to match your, you know, potential offerings, the tail leg of the dog. It's very unlikely to happen.

108
00:30:47.570 --> 00:30:56.299
Thread Genius: and so flipping it the other way. Creating a product that can be easily tested by some regulated industries

109
00:30:56.300 --> 00:31:08.015
Thread Genius: is why, folks have have started looking at some of the open source alternatives. And and you know this, this chart shows a number of companies with on prem deployments.

110
00:31:08.490 --> 00:31:30.220
Thread Genius: we actually represent all of those except for Deep seat but you know, it's a it's a different view of you know, inferencing closer to the data which solves, checks the box on privacy, confidentiality, and also tends to to have lower inferencing costs as well. Yeah.

111
00:31:32.850 --> 00:31:47.510
Thread Genius: So, and we touched on some of these. I mean, one other thing that we will observe is we'll call the llama phenomenon, which surprises a bit in 2023 and 2024, when

112
00:31:47.868 --> 00:32:10.821
Thread Genius: very large, Us. Companies started looking at Meta as a blue chip. American tech company, which if you're like sort of my age, cohort or older. You can still think of of Meta as a as a social media company and not an American Blue Chip Tech Company. But obviously given revenues and size and scale, it is the point being these very

113
00:32:11.180 --> 00:32:22.139
Thread Genius: you know, venerable fortune. 500 companies we're deciding. Well, do we want to build our future? You know, product, roadmap on AI with

114
00:32:22.140 --> 00:32:42.365
Thread Genius: some of the traditional, some of the, you know, open AI's of the world and electronics of the world? Or do we want to build it ourselves? With with the help of open source? Models. We want them heavy fine tuners in essence and then a lot of folks pivoted towards experimenting with llama.

115
00:32:42.810 --> 00:33:07.759
Thread Genius: Now we can talk sort of later about some of the the challenges and pitfalls with that. But that's definitely been. Look, we've we've all heard about, you know, the land chain wrappers and and others who are just sort of building an application layer a very thin application layer on top of on top of some some foundation models. But you know the the use of open source

116
00:33:07.760 --> 00:33:37.159
Thread Genius: has some benefits and also has some challenges. Right? When you're trying to push down product terms. When you have people willing to be paying customers. You may or may not be able to guarantee those outputs, and and to provide things like indemnities the same way that you would be getting from some of the Apis and the large model developers. So it's really a matrix that you have to look at. When when deciding some of this. Yeah.

117
00:33:38.670 --> 00:33:41.830
Thread Genius: to talk about the Western annotation games. Yeah.

118
00:33:42.433 --> 00:34:06.820
Thread Genius: So, as I mentioned earlier, there, when deep sea communities on the scene, sorry at a high level, deep. Seek right? So for folks, I mean, we've seen it in the media. But deep seek basically released a reasoning. Well, they released 2 models, they released the reasoning model, which is good. They released their sort of more vanilla model as well, which basically reported and early showed on early benchmarks

119
00:34:06.820 --> 00:34:19.309
Thread Genius: to have high, you know, to be highly capable and similar to state of the art, very large language models, and was purportedly trained at a fraction of the cost and and sort of the question was.

120
00:34:19.310 --> 00:34:42.829
Thread Genius: Well, you know, how did they get there? And and so, so, without getting into some of the controversial sides of it. We know a couple of things we know one, that they were coding and and tuning the model at below the cuda layer of in in the video chips which so Cuda is is the, you know, the the software, as you know.

121
00:34:43.164 --> 00:35:00.880
Thread Genius: That you can basically tweak and calibrate the gpus they were. There's actually a layer below that which no one bothers with in the West because we have access to compute. They didn't have access to compute, so they were forced to take a different engineering approach, and they unlocked some really powerful

122
00:35:00.890 --> 00:35:19.068
Thread Genius: performance by doing that. But but but you know, Deep Seat, just like any other model developer which is releasing models in China, basically has to be licensed. And one of the ways one of the requirements of the licensure is basically that it has to be

123
00:35:19.810 --> 00:35:44.009
Thread Genius: how do we say? I mean, culturally aware, it has to pass some some tests, and one of those tests are being slides can't be critical of in certain ways of the Ccp. Or you know, you're gonna have a hard time in a DC model pulling out you know, conversations about Tiananmen Square. And and so Katie will talk a little bit about, you know, obviously just taking that model as it was trained.

124
00:35:44.010 --> 00:35:58.200
Thread Genius: porting it to the Us. Market and then deploying them in an enterprise scale is going to cause some problems. So we had some solutions. So perplexity and Amazon Microsoft also provides an offering so to sort of

125
00:35:58.900 --> 00:36:28.649
Thread Genius: get around that they were able to open source it. So it's available here under us markets. And they did a few modifications of complexity. Being a legal search engine, they sort of addressed some of the censorship and other, and the goal concerns that folks had about the use of Dxc. Using filtering out personal identifier information and revising some of the censorship provisions there, and Amazon offers it as well

126
00:36:28.650 --> 00:36:31.389
Thread Genius: through bedrock, and it's also

127
00:36:31.890 --> 00:36:43.399
Thread Genius: being able to provide a much cheaper option here. But it gives people more people access to what they probably would not have access to. Yeah.

128
00:36:43.870 --> 00:36:44.850
Thread Genius: okay.

129
00:36:45.220 --> 00:37:05.751
Thread Genius: so we've talked about, I think most of these, but just a few, a few considerations that we have. When you know, advising if you're going to be fine tuning, are there any contractual restrictions? Technical debt which we discussed about, you know, choosing your model provider data leaks, which is another thing we had mentioned.

130
00:37:06.360 --> 00:37:34.680
Thread Genius: this is something that we deal with a lot when we work in specific, you know, we're providing guidance for how the legal, you know group is going to use a fine tune, or you know their system. What's their instance? Who has access to what data do we provide? How do we train that? And making sure that the marketing team, you know, can access that instance. So we've sort of seen all of this sort of play out in the last couple of years, training data, privacy as well, and certain techniques that can help.

131
00:37:34.680 --> 00:37:50.450
Thread Genius: you know, address some of these known defects overfitting, for example, and of course, making sure that the training data is, you know, matches what- what you use we use it for, and sort of addressing issues of bias.

132
00:37:50.450 --> 00:38:11.970
Thread Genius: Can you go back to the last slide? Just just kind of up level for a second on like Llama's license, right? Like, if you if you're competing with llama like your startup like, what does that mean? Like? What? What? How much focus is there on that? Well, I mean, like Llama, for example, doesn't allow you to train competing models right? So so early on

133
00:38:11.970 --> 00:38:28.879
Thread Genius: one of the right. So- so the whole industry is data constraint, right? So they're scaling laws. And some very famous paper that observed that the more data you tend to throw at these models more data to compute the better performance you get on that level?

134
00:38:29.182 --> 00:38:53.419
Thread Genius: So so folks said, Okay, well, great. So why don't we use a model? Why don't we create an on-prem model where we can make more efficient and just create a I think the technical term is shit on of data. In order to synthetic data to then train our own models. For maybe, you know, maybe we're building a computing model although the Llo license, you know, handed that off in the past. The 1st

135
00:38:53.737 --> 00:39:02.613
Thread Genius: license was research only license. So you couldn't even use it for commercial purposes. The second license was commercial, except they had this prohibition on using it for

136
00:39:03.372 --> 00:39:29.119
Thread Genius: competing model. The 3rd version of license, you know, took a half step back from that, but still has some prohibitions. And so I think it's a really good point. If you if you're building on llama, yeah, you're more free to do a lot of things but you're not getting indemnity passed down to your customers, and there are restrictions on some of the ways in which you can use those outputs. That's just sort of one example, right? Everyone has

137
00:39:29.120 --> 00:39:55.230
Thread Genius: has has their own license that they attach to their open source projects. Some are mit licenses, which is great and kind of do whatever you want. Some are a little bit more complex, and and there are some variants of the creative commons licenses flying around. That's why love is known as like an open access, not really open source license, because of some of those restrictions, including some of the user restrictions as well that they have

138
00:39:55.995 --> 00:40:23.430
Thread Genius: great should dive into the application layer. Yeah. So you know, we're gonna move through this pretty quickly. Yeah, because, like, you know, you all are focused on building the usable side, the application. And so so really, you know, sitting in front of, or being the front end of that engine? You know you all are tasked with being the domain specific expert and go to the next slide.

139
00:40:24.704 --> 00:40:34.645
Thread Genius: You know some of the some of the things to think about that we see right is picking the right engine. And so you know, one of one of the things. One of the

140
00:40:35.010 --> 00:41:01.659
Thread Genius: principal concepts that's in vogue right now is not committing to any one model right? Being able to build your architecture that you can choose from multiple models. You know, or even using sort of mixture of experts, type, architecture, and others where you know the the projects and the tasks are broken down, and even within that framework you can have a model, discriminate, determine where to send

141
00:41:02.039 --> 00:41:20.240
Thread Genius: the inferencing, or where to send. You know the the prompt based on the type of question. We talked a little bit about the characteristics at a high level being open source versus proprietary. You know, David mentioned the importance of infrastructure and deployment. Being able

142
00:41:20.240 --> 00:41:43.489
Thread Genius: to to be scalable is continues to be an important principle here. But then also data, right? These things consume enormous amounts of data and data is going to be something you're gonna have to contend with whether it's just context to make the model run better, or whether it's data to be consumed as as inputs or prompts in order to give usable.

143
00:41:44.127 --> 00:42:12.219
Thread Genius: You know, outputs and inferences. Yeah, let's go to the Mcp. Probably. Yeah, I think so. So. Look, Mcp is an idea that was promulgated by anthropic. And you can think of it essentially as a USB-c cord. It's basically a communication protocol layer that allows you bidirectionally to communicate with your context. And so what this graph really shows is in the Old World being 6 months ago.

144
00:42:12.310 --> 00:42:39.180
Thread Genius: you basically had to build proprietary connectors between the bottle and the different data inputs. And so you're wasting a lot of time on this infrastructure. It's kind of silly infrastructure layer to find folks that interrupting said, well, actually, why don't we just propose a standard? Have other folks, you know, join on? And you can we? We will do the legwork and then open source it so that folks can have this bidirectional

145
00:42:39.650 --> 00:42:46.239
Thread Genius: you know, communication with with their data. And I think that's that's

146
00:42:47.220 --> 00:43:08.620
Thread Genius: what what that really will do is it opens up the world for more capable models. And and it prevents the customers from getting bogged down to have to figure out how to connect their data to the models themselves. So I think that puts us at a pretty good stopping point. To then take some of the questions. Maybe we can

147
00:43:08.640 --> 00:43:17.789
Thread Genius: do a bit of a chat of house rules, and we can. We can do a little bit more forthcoming on on the answers. Great? Well, I'm gonna stop the reporting

