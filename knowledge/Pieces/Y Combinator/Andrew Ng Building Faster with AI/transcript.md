# Andrew Ng: Building Faster with AI

**Channel**: Y Combinator  
**Date**: 2025-07-10  
**URL**: https://www.youtube.com/watch?v=RNJCfif1dPY  
**Transcribed**: 2025-07-10

---

## Transcript

It's really great to see all of you.
What I want to do today since this is
this build at Startup School is
share of you some lessons
I've learned about building startups at AI Fund.
AI Funds Adventure Studio,
and we build an average of about one startup per month.
Because we co-founded startups,
we're in there writing code,
talking to customers, design on features,
that are in pricing,
and so we've done a lot of reps
of not just washing others build startups,
but actually being in the weeds,
building startups with entrepreneurs.
And what I want to do today is
share of you some of the lessons
I've learned building startups,
especially around this changing AI technology
and what it enables.
And it'll be focused on the theme of speed.
So it turns out that,
but those of you that want to build a startup,
I think a strong predictor for startups
also success is an execution speed.
And I actually have a lot of respect
for the entrepreneurs and executives
that can just do things really quickly.
And new AI technology is enabling startups
to go much faster.
So what I want to do is share of you
some of those best practices,
which are frankly changing every two to three months though,
to let you get that speed
that hopefully lets you have a higher odds of success.
Before diving to speed,
a lot of people ask me,
hey, Andrew, where are the opportunities for startups?
So this is what I think of as a AI stack,
where at the lowest level
are the semiconductor companies,
then the clouds,
I have my scalars build on top of that,
a lot of the AI foundation,
all the companies build on top of that.
And even though a lot of the PR excitement and hype
has been on these technology layers,
it turns out that almost by definition,
the biggest opportunities have to be
at the application layer
because we actually need the applications
to generate even more revenue
so that they can afford to pay the foundation, cloud,
and semiconductor technology layers.
So for whatever reason,
media and social media tends not to talk
about the application layer as much,
but for those of you who think you're building startups,
almost by definition,
the biggest opportunities have to be there,
although of course,
the opportunities are all layers of the set.
One of the things that's changed a lot over the last year,
and in terms of AI tech trends,
if you ask me,
what's the most important tech trend in AI?
I would say is the rise of agent to AI.
And about a year and a half ago,
when I started to go around and give talks
to try to convince people
that AI agents might be a thing,
I did not realize that around last summer,
a bunch of marketers would get a hold of this term
and use it as a sticker and slap it on everything in sight,
which made it almost lose some of its meaning.
But I'll share with you from a technical perspective,
why I think agentic AI is exciting and important,
and also opens up a lot more startup opportunities.
So it turns out that the way a lot of us use LOMs
is to prompt it to have it during an output.
And the way we have an LOM output something
is as if you're going to a human,
or in this case, an AI,
and asking it to please type on an essay for you
by writing from the first word to the last word,
all in one go, whatever, using backspace.
And humans, we don't do our best writing,
being forced to type in this linear order.
And it turns out neither does AI,
but despite the difficulty of being forced to write in this linear way,
our LOMs do surprisingly well.
With agentic workflows,
we can go to AI system and ask it
to please first write an essay online,
then do some web research if it needs to,
and fetch some web pages to put in their own context,
then write the first draft,
then read the first draft in critique,
and revise it, and so on.
And so we end up with this iterative workflow
where your model does some thinking and some research,
does some revision, goes back to do more thinking,
and by going around this loop many times,
it is slower, but it delivers a much better work product.
So for a lot of projects,
AI fund has worked on everything from pulling out complex compliance documents
to medical diagnosis,
to reasoning about complex legal documents.
We found that these agentic workflows
are really a huge difference within it working,
versus not working.
But a lot of the work that needs to be done,
a lot of available businesses to be built still,
will be taking workflows,
existing or new workflows,
and figuring out how to implement them
into these types of agentic workflows.
So just to update the picture for the AI stack,
what has emerged over the last year
is a new agentic orchestration layer
that helps application builders orchestrate
or coordinate a lot of calls to the technology layers underneath.
And the good news is the orchestration layer
has made it even easier to build applications.
But I think the basic conclusion
that the application layer has to be
the most valuable layer of the stack still holds true.
With a bias or focus on the application layer,
let me now dive into some of the best practices of learning,
but how startups can move faster.
It turns out that at AI fund,
we only focus on working on concrete ideas.
So to me, a concrete idea, a concrete product idea,
is one that's specified enough detail
that an engineer can go and build it.
So for example, if you say,
let's use AI's optimized healthcare assets,
you know, that's actually not a concrete idea.
It's too vague.
If you tell me it's a very software,
do you use AI's optimized healthcare assets?
Different engineers would do totally different things.
And because it's not concrete,
you can't build it quickly and you don't have speed.
In contrast, if you had a concrete idea,
like let's write software to let hospitals,
let patients with MRI machine slots online to optimize usage,
I don't know if this is a good or a bad concrete idea.
That's actually business already doing this.
But it is concrete and that means
engineers can build it quickly.
If it's a good idea, you find out
there's not a good idea, you will find out
by having concrete ideas by as you speed.
Or someone would say, let's use AF email personal productivity.
Too many interpretations of that, that's not concrete.
But if someone says,
could you build an app,
Gmail integrates the automation that uses,
let's use the right prompt,
so it's my filter and tag email,
is that it's concrete?
I could go build that this afternoon.
So concrete is by as you speed.
And the deceptive thing for a lot of entrepreneurs
is the vague ideas tend to get a lot of kudos.
If you go and tell all your friends,
we should use AI to optimize the use of healthcare assets.
Everyone will say that's a great idea.
But it's actually not a great idea,
at least in a sense of being something you can build.
It turns out when you're vague,
you're almost always right.
But when you're concrete,
you may be right or wrong.
Either way, it's fine.
We can discover that much more fast,
which is what's important for us to start up.
In terms of executing concrete ideas,
I find that AI fun,
I ask my team to focus on concrete ideas
because a concrete idea gives clear direction.
And the team can run really fast,
to build it and either validate it, prove it out,
or falsify it and conclude it doesn't work.
Either way, it's fine.
So let's do that quickly.
And it turns out that finding good concrete ideas
usually requires someone,
could be you, could be a subject matter expert.
Think about a problem for a long time.
So for example, actually before you're starting Coursera,
I spent years thinking about online education,
talking to the users,
holding my own intuitions
about what would make a good tech platform.
And then after that long process,
I think YC sometimes calls it wondering the idea maze.
But after thinking about it for a long time,
you find that the guts of people
that thought about this for a long time
can be very good about rapidly making decisions.
As in, after you've thought about this,
talk to customers and stuff for a long time,
if you're also this expert,
should I build this feature or that feature,
you know, the gut,
which is an instantaneous decision,
can be actually a surprisingly good proxy.
It can be surprisingly good mechanism for making decisions.
And I know I work on AI,
you might think I'll say,
oh, we need data.
And of course, I love data.
The turns out getting data for a lot of starters
is just slow mechanism for making decisions.
And subject matter expert with good guts
is often a much better mechanism
for making a speedy decision.
And then one other thing,
but many successful starters,
at any moment in time,
you're pursuing one very clear hypothesis.
They're building out and trying to sell
a value of all-spot.
And a startup doesn't have resources to hedge
and try 10 things at the same time.
So pick one, go for it.
And if data tells you to use faith in that idea,
that's totally fine.
Just pivot on the dime to pursue
a totally different concrete idea.
So that's what often feels like an AI fund.
We're pursuing one thing doggily with determination
until the world tells us we were wrong,
then change and pursue a totally different thing
with equal determination and equal dogginess.
And one of the pads in our scene,
if every piece of new data calls you to the pivot,
it probably means you're starting off
from too weak a base of knowledge, right?
Every time you talk to a customer,
you totally change your mind.
Part of it means you don't know enough about that sector yet
to have a really high quality concrete idea
and finding someone to start about a subject
for longer may get drawn to better power.
In order to go faster,
the other thing often think about
is the built feedback loop,
which is rapidly changing when it comes to
how we build with AI coding assistance.
So when you're building a lot of applications,
one of the biggest risks is customer acceptance, right?
A lot of startups struggle,
not because we can't build whatever we want to build,
but because we build something and it turns out nobody cares.
And so for a lot of the way I build startups,
especially applications,
less so deep tech, less so technology startups,
but definitely application startups
is often built software.
So this is an engineering toss.
And then we will get feedback from users
and just a product management toss.
And then we'll go back, you know,
then basically use the feedback,
we'll tweak our views on what to build,
go back to write more software,
and we go around this loop many, many times,
iterate toward product market fit.
And it turns out that with AI coding assistance,
which Andre talked about as well,
rapid engineering is becoming possible in a way
that just was not possible.
It's becoming much more feasible.
So the speed of engineering is going up rapidly
and the cost of engineering is also going down rapidly.
This changes the mechanisms
by which we drive startups around this loop.
When I think about the software I did I do,
I may be put into two major buckets.
Sometimes I've built quick and dirty prototypes
to test an idea,
and say, we'll build a new customer service chat ball.
Let's build an AI to process legal documents or whatever.
But the quick and dirty prototype
to see if we think it works.
The other type of software I do is,
write, maintain production software,
maintain legacy software,
but these massive production-ready code bases.
Depending on which analyst report you trust,
it's been hard to find very rigorous data on this.
You know, when writing production quality code,
maybe we're 30% to 50% faster with AI assistance,
hard to find a rigorous number.
Maybe, these falls more to me.
But in terms of building quick and dirty prototypes,
we're not 50% faster.
I think we're easily 10 times faster,
maybe much more than 10 times faster.
And there are a few reasons for this.
When you're building standalone prototypes,
this less integration with legacy software infrastructure
and legacy data needed.
Also, the requirements for reliability,
even scalability, even security are much lower.
And I know I'm not supposed to tell people
to write insecure code, right?
Feels like the wrong thing to say.
But I routinely go to my team
and say, go ahead, write insecure code.
Because if this software is only gonna run on your laptop,
and if you don't plan to maliciously hack your own laptop,
it's fine to have insecure code, right?
But of course, after it seems to be working,
please do make a secure before you ship it to someone else.
And you know, like a leaking PII, a leaking send data
that is very damaging.
So before you ship it, make a secure and scalable,
but they were just assessing it, it's fine.
And so I find increasingly,
starters will systematically pursue innovations
by building 20 prototypes to see what works, right?
Because I know that there's some ads in AI,
a lot of proof of concepts don't make into production.
But I think by driving the cost of a proof of concept
low enough, it's actually fine.
I've lost a proof of concepts.
Don't see the light of day.
And I know that the mantra,
move fast and break things got a bad rep,
because it broke things.
And some teams took away from this
that you should not move fast,
but I think that's a mistake.
I tend to tell my teams to move fast and be responsible.
And I think they actually lost their ways
to move really quickly while still being responsible.
And in terms of the AI assistance coding landscape,
I think was it three, four years ago,
code autocomplete, right?
Popularized by GitHub, co-pilot.
And then there was a cursor,
wind-served generation of AI-enabled IDEs.
We're in great use, wind-served cursor, quite a lot.
And then starting on our six, seven months ago,
decided to be this new generation
of high-dagentic coding assistance,
including, like you're using,
oh, fee a lot for coding.
Cloud code is fantastic.
Since Cloud 4 release has become,
and I've been getting a few months,
I may use something different,
but the tools are evolving really rapidly.
But I think Cloud code X,
this is a new generation of highly-agentic coding assistance
that is making developer productivity keep on growing.
And the interesting thing is,
if you're even half a generation or one generation behind,
it actually makes a big difference
compared to if you're on top of the latest tools.
And I find my team is taking really different approaches
to software engineering now compared to even fee
or six months ago.
One surprising thing is,
we're used to thinking of code
as this really valuable artifact,
because it's so hard to create.
But because the cost of software engineering is going down,
code is much less of a valuable artifact as it used to.
Some on teams where,
we've completely rebuilt the code base
three times the last month, right?
Because it's not that hard anymore
to just completely rebuild the code base.
Pick a new data schema is fine,
because the cost of doing that has plummeted.
Some of you may have heard of Jeff Bezos' terminology
of a two-aidor versus a one-way door.
A two-aidor's decision they can make.
If you change your mind,
come back out, you know,
reverse it relatively cheap.
Whereas the one-way door is,
you make a decision and you change your mind
is very costly, very difficult to reverse.
So choosing the software architecture
of your text stack,
used to be a one-way door.
Once you build on top of a certain text stack,
you know, set the database schema really hard to change it.
So that used to be a one-way door.
I don't want to say it's totally a two-way door,
but I find that my team will more often
build on a certain text stack a week later, change your mind.
Let's throw the code base away
and redo it from scratch on the new text stack.
I don't want to over-hype it.
We don't do that all the time.
There are still costs to redoing that.
But I find my team is often rethinking
what is a one-way door and what's now a two-way door
because the cost of software engineering is so much lower now.
And maybe going a little bit beyond software engineering,
I feel like it's actually a good time
to empower everyone to build a AI.
Over the last year,
a bunch of people have advised others not to learn to code
on the browser AI will automate it.
I think we'll look back on this
as some of the worst career advice ever given.
Because as better tools make software engineering easier,
more people should do it, not fewer.
So when many decades ago,
the world moved from punch cards to keyboard and terminal
that made coding easier.
When we moved from assemblies,
high-level languages like Cobalt,
there are actually people arguing back then
that now we have Cobalt,
we don't need programmers anymore,
that people actually don't pay for it to that effect.
But of course, there was wrong
and program languages made easier to code
and more people learn to code.
Check sentences, IDs,
IDs, AI coding assistance.
And as coding becomes easier,
more people should learn to code.
I have a controversial opinion,
which is, I think it's actually time
for everyone of every job role to learn to code.
And in fact, on my team,
my CFO, my head of talent,
my recruiters, my friend desk person,
all of them knew how to code
and I should see all of them performing better
at all of their job functions because they can code.
And I think I'm pretty little bit ahead of her
for most businesses are not there yet.
But in the future,
I think when part everyone to code,
a lot of people can be more productive.
I want to share of you one lesson I learned as well
on why we should have people learn to do this,
which is when a teacher in Genese VIF everyone on Coursera,
we needed to generate background art like this
using mid-Journey.
And one of my team members knew art history.
And so he could prompt mid-Journey
with the genre, the palette,
the artistic inspiration had a very good control
over the images he generated.
So we end up using all of Tommy's generous images.
Whereas in Contrast, I don't know art history.
And so when I prompt your image generation,
I could write, please make pretty pictures
of robots for me, right?
And I could never have the control
that my collaborates are good.
And so I couldn't generate as good images as he could.
And I think with computers,
one of the most important skills of the future
is the ability to tell a computer exactly what you want
so that it will do it for you.
And there will be people that have that deeper understanding
of computers that will be able to command the computer
to get the outcome you want.
And learning to code, not that you need to write the code
yourself, steer AI to code for you.
It seems like it will remain the best way
to do that for a long time.
With software engineering becoming much faster,
the other interesting dynamic I'm seeing
is that the product management work,
getting user feedback, deciding what features to build,
that is increasingly the bottleneck.
And so I'm seeing very interesting dynamics
in multiple teams over the last year.
A lot more of my teams have started
to complain that the bottleneck on product engineering
and design because the engineers have gotten so much faster.
Some machine trends I'm seeing.
Three, four, five years ago, Silicon Valley
used to have these slightly suspicious rules of thumb,
but nonetheless, rules of thumb
will have 1 p.m. to four engineers
or 1 p.m. to seven engineers.
It was this like p.m. product manager
at the engineering ratio, right?
I think we're going to solve for those typical
as ever, 1 p.m. to six, seven engineers.
And with engineers becoming much faster,
I don't see product management work
designing what to build, becoming faster
at the same speed that engineering is.
I'm seeing this ratio shift.
So literally yesterday, one of my teams came to me
and for the first time, when we're planning
to hit comfort project, this team proposed to me
not have 1 p.m. to four engineers,
but have 1 p.m. to 0.5 engineers.
So the team I should propose to me,
I still know that this is a good idea.
For the first time in my life,
I saw your managers propose to me,
having twice as many p.m. as engineers
was a very interesting dynamic.
I still don't know if this proposed
I heard yesterday is a good idea,
but I think it's a sign of where the world's going.
And I find there's a p.m.s that can code
or engineers with some product instincts
often end up doing better.
The other thing that found important
for startup leaders is because engineering
is going so fast, if you're good tactics,
we're getting rapid feedback to shape your perspective
on what to build faster.
That helps you get faster as well.
So I'm going to go through a portfolio of tactics
for getting product feedback to keep shaping
what you will decide to build.
And we're going to go through a list of the faster,
maybe less accurate, the slower more accurate tactics.
So the fastest tactic for getting feedback is
look at the product yourself and just go by your gut.
And if you're subject matter to exhibit,
this is actually surprisingly good.
If you know what you're doing,
loop is slower is go ask the friends of teammates
to get feedback to play your product and get feedback.
Loop is slower is ask the attend strangers for feedback.
It turns out when I built products,
one of the most important skills I think I learned
was how to send to the coffee shop,
how to sit in there.
When I travel, I often send the hotel lobby.
It turns out lent to spot places to buy foot traffic
and very respectfully, you know,
grab strangers and ask them for feedback
on whatever I'm building.
This used to be easier was less known
when people recognize you as a little bit more awkward.
But I found that I've actually sat with teams
at the hotel lobby, very high foot traffic.
And you know, very respectfully,
off strangers, hey, we're building the same,
do you mind taking a look?
Oh, and I actually learned in a coffee shop
that a lot of people working,
a lot of people don't want to be working.
So we give them, excuse me, distracted.
They're very happy to do that too.
But I've actually kind of made tons of product decisions
in a hotel lobby and a coffee shop
who collaborates with just just like that.
St. Prototype 100 testers, if you have access,
the logic goes to users.
St. Prototype to more users.
And these are, these get to be slow and slow tactics.
And I know Silicon Valley, you know,
we like to talk about A, B testing.
Of course, I do a ton of A, B testing.
But contrary to what many people think,
A, B testing is not one of the slowest tactics
in my menu because it's just slow to ship it.
It depends on how many users you have, right?
So and then the other thing is as you use anything,
but the first tactic, some teams will look at the data
to make a decision.
But the missing piece is, when I A, B test something,
I don't just use the, without the A, B test,
to pick product A or product B.
My team will often sit down and look here for the data
to hone our instincts, to speed up, to improve the rate.
I wish we were able to use the first tactic,
to make high quality decisions.
I often sit down and think, gee,
I thought, you know, this product name
will work better than their product name.
Clearly, my mental model that uses wrong,
so we sit down and think, to update our mental model,
using all of that data, to improve the quality of our guts
on how to make product decisions faster.
That turns out to be really important.
All right.
So talk about concrete ideas, speed up engineering,
speed up product feedback.
This is one last thing we'll touch on,
which is a scene that, understanding AI,
actually makes you go faster.
And here's why, as an AI person,
maybe I'm biased to be pro AI,
but I want to share you why.
So it turns out that when it comes to mature technology,
like mobile, you know, many people have had smartphones
for a long time, we kind of know what a mobile app can do, right?
So many people, including non-technical people,
have good instincts about what a mobile app can do.
If you look at mature job roles,
like sales marketing, HR legal,
they're all really important, all really difficult.
But, you know, there are enough marketers
that have done marketing for long enough,
and the marketing tactics haven't changed that much
in the last year, so there are a lot of people
that are really good at marketing.
And it's really important, really hard,
but that knowledge is relatively diffused
because, you know, the knowledge of how to do HR,
like it hasn't changed dramatically, you know,
in the last six months.
But AI is the emerging technology,
and so the knowledge of how to do AI really well
is not widespread.
And so teams that actually get it,
they understand AI, do have an advantage
over teams that don't.
Whereas if you need an HR problem,
you can find someone, you know,
that knows how to do it well, probably.
But if an AI problem,
you're doing how to actually do that,
could put your head of other companies.
So things like, what accuracy can you get
for a customer service chat box?
You know, should you problem a fine team
using a journey workflow?
How do you get a voice out to a low latency?
And a lot of these decisions
that if you make the right technical decision,
you can like solve the problem in a couple days,
they make the wrong technical decision,
you could chase a blind alley for three months, right?
And one thing I'm a bit surprised by,
it turns out if you have, you know,
two possible architecture decisions,
it's one bit of information.
It feels like if you don't know the right answer,
at most, you're twice as slow, right?
One bit, you know, try both of you,
it feels like one bit of information,
that it most by your two xp'd up.
And I think in some, their echo says that is true.
But what I see in practice,
if you flip the wrong bit, you're not twice as slow,
you spend like 10 times longer chasing a blind alley,
which is why I think going into this right technical
adjustment, it really makes startups
go so much faster.
The other reason why I find,
staying on top of AI really hopeful for startups
is over the last two years,
we have just had a ton of wonderful Genei tools
or Genei building blocks, right?
Partial list.
But prompting agentic workflows,
evals, Godrails, rad, voice that,
asynchronous, loss of ETL, embeddings,
fine tuning, graph DB, how to integrate a computer,
use mcb-reasing models.
There's a long and wonderful list of building blocks
that you can quickly combine to build software
that no one on the planet could have built even a year ago.
And this creates a lot of new opportunities
for startups to build new things.
So when I learned about these building blocks,
this is actually a picture that I had in mind.
If you own one building block,
like you have a basic white building block,
you know, you can build some cool stuff,
maybe you know how to prompt,
see you have one building block,
you know, you build some amazing stuff.
But if you get a second building block,
like you also know how to build chat box,
see you have a white Lego break and a black Lego break.
You can build something more interesting.
If you acquire a blue building break as well,
you can build something even more interesting.
Get few red building breaks, maybe a little yellow one,
more interesting.
Get more building breaks, get more building breaks,
and very rapidly, the number of things
you can combine them into,
grows, kind of carbonate or early or grows exponentially.
And so knowing all these wonderful building blocks,
let's you combine them in a much richer combination.
One thing that deep learning that I do,
so I actually take a lot of deep learning that I
show causes myself, you know,
to this work of great part.
We work with, I think, like,
finish all the leading AI companies in the world,
and so the, and, and, and try to hand out building blocks.
But when I look at the deep learning that AI
caused catalog, this is actually what I see.
And whenever I take these causes to learn these building blocks,
I feel like I'm getting new things that can combine
to form, kind of, combinatorial,
the exponentially more software applications
that were not possible just one or two years ago.
So just to wrap up, this is my last slide.
Then you'll take questions if, if you'll have any.
I find that there are many things in that for startup,
not just speed, but when I look at the startups
that AI fund is building,
I find that the management team's ability
to execute as speed is highly correlated
with this odds of success.
And some things with learning to get your speed
is, you know, work on concrete ideas.
It's going to be good concrete ideas.
I find that as an executive, I'm judge
on the speed and quality of my decisions,
both do matter, the speed absolutely matters.
Rapid entering with AI coding assistance
makes you go much faster,
but that shifts the bottleneck to getting user feedback
on the product decisions.
And so having a portfolio of tactics
to go get rapid feedback.
And if you haven't learned to go to coffee shop
and talk to strangers, it's not easy,
but then just be respectful, right?
Just be respectful of people.
That's actually a very valuable skill
for entrepreneurs to have, I think.
And I think also, say on top of the AI technology,
buys you speed.
All right, with that, let me thank you very much.
I think I have a quick question.
As AI advances, do you think it's more important
for humans to develop the tools
or learn how to use the tools better?
Like, how can we position ourselves
to remain essential in a world
where, you know, intelligence is becoming democratized?
I feel like AGI has been over height.
And so for a long time,
there'll be a lot of things that humans can do
that AI cannot.
And I think in the future,
the people that are most powerful
are the people that can make computers
do exactly what you want it to do.
And so I think, seeing on top of the tools,
some of us will build tools sometimes,
but there will be a lot of other tools
that others will build that we can just use.
But so people that know how to use AI
to get computers to do what you want it to do
will be much more powerful.
Not where about people running out of things to do,
but people that can use AI will be much more powerful
than people that don't.
Hey, so well, first of all, thank you so much.
I have a huge respect for you
and I think that your inspiration for a lot of us,
my question is about the future of compute.
So as we move towards more powerful AI,
where do you think that compute is setting?
I mean, we see people saying,
let's ship GPUs to space,
some people talking about nuclear power, data centers.
What do you think about it?
There's some kind of debate we're going to say
in response to the last question about kind of AGI,
about maybe I'll answer this and a little bit of the last question.
So turns out there's one framework you can use,
but deciding what's hype and what's not hype.
I think over the last two years,
there's been a handful of companies
that hyped up certain things,
but promotional PR fundraising influence purposes.
And because AI was so new,
handful of companies got away with saying almost anything
without anyone fact checking them
because the technology was not understood.
So one of my mental filters is
there's certain hype narratives
that make these businesses look more powerful,
that's been amplified.
And so for example, this idea that AI is so powerful,
we might accidentally to human extinction.
That's just ridiculous,
but it is a hype narrative
that made certain businesses look more powerful
and it got around top
and actually helped certain businesses fund raising goals.
AI is so powerful, soon no one will even have a job anymore.
Just not true, right?
But again, that made these business look more powerful,
got hyped up.
Or we are so powerful,
so when the high narrative,
we're so powerful that by training a new model,
we will casually wipe out thousands of startups.
That's just not true.
Yes, Jasper, Brandon Trouville,
a small number of companies got wiped out,
but it's not that easy to casually wipe out thousands
of startups.
AI needs so much electricity,
only nuclear power is good enough for that.
That wind, solar, stuff, this is not true.
So I think a lot of this GPUs in space,
you know, I don't know,
is like, go for it.
I think we have a lot of room to run still
for terrestrial GPUs.
Yeah, but I think some of these hype narratives
have been amplified that I think are a distortion
of what actually will be done.
There's a lot of hype in AI and how,
and nobody's really certain about how we're going to
be building the future with it.
But what are some of the most dangerous biases
or over hyped narratives that you've seen people talk about
or get poisoned by that they end up running with
that we should try to avoid or be more aware of
and allow us to have a more realistic view
as we are building this future.
So I think the dangerous AI narrative has been over hyped.
AI is a fantastic tool,
but like any other powerful tool like electricity,
lots of ways to use it for beneficial purposes,
also some ways to use it in harmful ways.
I find myself not using the term AI safety that much,
not because I think we should build dangerous things,
but because I think safety is not a function of technology,
it's a function of how we apply it.
So like electric motor,
the maker of electric motor can't guarantee
that no one will ever use it for an unsafe downstream task.
Like using electric motor,
we use to build a Dallas machine,
electric vehicle,
can use to build a smart bomb,
but the electric motor manufacturer can't control
how we use downstream.
So safety is not a function of electric motor
as a function of how you apply it.
And I think the same thing for AI,
AI is neither safe nor unsafe.
It is how you apply it that makes it safer unsafe.
So instead of thinking about AI safety,
I often think about responsible AI,
because it is how we use it responsibly, hopefully,
or irresponsibly the determines
that are not what we build with AI technology
as it being harmful or beneficial.
And I feel like sometimes that the really weird corner cases
they get hyped up in the news,
I think just one or two days ago,
there was a Wall Street Journal article
about AI losing control of AI or something.
And I feel like that article
took a corner case experiments run in the lab
and sensationalized it in a way
that I think was really disproportionate
relative to the lab experiment those being run.
And unfortunately, technology is hard enough to understand
that many people don't know better.
And so these high narratives do keep on getting amplified.
And I feel like this has been used
as a weapon against open source software as well,
which is really unfortunate.
Thank you for a work.
I think your impact is remarkable.
My question is, as aspiring founders,
how should we be thinking about business
in the world where anything can be disrupted in a day?
Whatever great mode product or feature you have
can be replicated with vibe code in competitors
in basically hours.
It turns out when you start a business,
there are a lot of things to worry about.
The number thing I worry about
is are you building a product that uses love?
It turns out that when you build a business,
there are lots of things to think about.
There goes market, channel, competitors, technology,
mode, all that is important.
But if I were to have a singular focus on one thing,
it is, are you building a product
that users really want?
Until you solve that, it's very difficult
to build a valuable business.
After you solve that, the other questions do come to play.
Do you have a channel to get to customers?
What is pricing?
Long-term, what is your mode?
I find that modes tend to be over-hyped.
Actually, I find that more businesses
tend to start up with a product
and then evolve eventually into a mode.
But consumer products brand is somewhat more defensible.
And if you have a lot of momentum,
it comes harder to catch you.
But enterprise products, sometimes,
if you have a, maybe mode is more of a consideration
of the channels that are hard to get to enterprises.
So I think, sorry, when AI fund looks at businesses,
we actually wind up doing a fairly complex analysis
of these factors and writing to the six-page narrative
memo to analyze it before we decide whether or not
to go see it or not.
And I think all of these things are important.
But I feel like at this moment in time,
the number of opportunities, meaning
the amount of stuff that is possible
that no one's built yet in the world,
seems much greater than the number of people
with the skills to build them.
So definitely at the application layer,
it feels like there's a lot of white space
for new things you can build
that no one else seems to be working on.
And I would say, you know, focus on building a product
that people want that people love
and then figure out the rest of it along the way.
Although this is important to figure a long way.
Hi, Professor.
Thank you for your wonderful speech.
I'm an underwear researcher from Stanford.
And I think your metaphor in your speech
is very interesting.
You said the current AI tools are like bricks
and can be built upon accumulation.
However, so far it is difficult to see
the accumulative functional expansion
of the integration of AI tools
because they open a line on the stacking
of functions based on intent distribution
and are accompanied by dynamic problems of tokens
and time overhead.
So which is different from static engineering?
So what do you think will be the perspective
of a possible agent to accumulation effect in the future?
But hey, just some quick remarks to that, right?
You mentioned agent OM token cost.
My most common advice to developers
is to first approximation.
Just don't worry about how much tokens cost.
Only a small number of startups are lucky enough
to have users use so much of your product
that the cost of tokens becomes a problem.
It can become a problem.
I've definitely been on a bunch of teams
where the cost uses like a product
and we started to look at all, right?
Genii bills and it was definitely climbing
in a way that really became a problem.
But it's actually really difficult to get to point
where your token usage costs other problem
and for the teams I'm on where we were lucky enough
that users made our token cost a problem.
We often had entry solutions
to then bend the curves and bring it back down
through prompting, fine tuning,
USDSPI to optimize or whatever.
And then what I'm seeing is that
I'm seeing a lot of agent workflows
that actually integrate a lot of different steps.
So for example, if you build a customer's service chatbot
will often have to use prompting,
maybe optimize some of the results
through your DSPI, the emails, the God Rails,
maybe the customer's service chatbot
needs rack apart the way to get information
to feedback to the user.
So I actually do see these things grow.
But one tip for many of you as well is
I will often architect my software
to make switching between different building block providers
relatively easy.
So for example, I have a lot of products
that I build on top of LOMs,
but sometimes the point is specific product
and ask me which OM are we using.
I honestly don't know because we'll build up emails
and where does the new model that's released
will quickly run emails to see
if the new model is better than the O1.
And then you'll just switch to the new model
of the new model that's better on emails.
And so the model we use week by week,
you know, sometimes our engines will change it
without even bothering to tell me
because it either showed the new model works better.
So it turns out that switching costs
for foundation models is relatively low
and we often architect our software.
Oh, AI suite is open sourcing
that my friends that I worked on
to make switching easier.
Switching costs for the orchestration platforms
is a little bit harder,
but I find that preserving that flexibility
in your choice of building blocks
often less you go faster,
even as you're building more and more things
on top of each other.
So hope that helps.
Thank you so much.
In the world of education in AI,
there are two paradigms mostly.
So one is AI can make teachers more productive
or automating grading and automating homeworks.
But another score of thought is that
there'll be personal tutors for every student.
So every student can have one tutor
that gets feedback from an AI
and gets personal questions from them.
So how do you see these two paradigms converge?
And how would education look like in the next five years?
I think everyone feels like a change is coming in at tech.
Like don't think the disruption is here yet.
I think a lot of people are experimenting
at different things.
So Coursera has Coursera coach,
which actually works really well.
Deep learning data is more focused on teaching AI.
Also, some built-in chatbos.
A lot of teams experience of water grading.
Oh, there's an avatar of me
on the Deep learning data website that you can talk to
if you want.
Deep learning data AI slash avatar.
And then I think for some things
like language learning with, you know, speak, dolingo,
that has become clearer
some of the ways that I would transform it.
But the broader educational landscape,
the exact ways that AI would transform it,
I see a lot of experimentation.
I think what key relearning
which I've been doing some work with
is doing this is very promising for K-12 education.
But I think what I'm seeing
is frankly tons of experimentation.
But the final end state is still not clear.
I do think education will be hyper-personalized.
But that workflow is an avatar,
is a text chatbot, what's the workflow?
I think I feel like the hype from a couple of years ago
that with AI soon, and we're all so easy, that was hype.
The reality is work is complex, right?
Teachers, students, people do really complex workflows.
And for the next decade,
we'll be looking at the work that needs to be done
and figuring out how to map it to agente workflows.
And education is one of the sectors
where this mapping is still underway,
but it's not your mature enough
to the point where the end state is clear.
So I think I think we should all just keep working on it.
All right, all right, thank you so much, Andrew.
Thank you.
Hey, my question is, I think AI
has a lot of great potential for good,
but there's also a lot of potential
for bad consequences as well,
such as exacerbate an economic inequality
and things like that.
And I think a lot of our startups here,
while they'll be doing a lot of great things,
will also be just five or a true of their product,
be contributing to some of those negative consequences.
So I was curious,
how do you think, you know, us as AI builders
should kind of balance our product building
with also the potential societal downsides
of some AI products?
And essentially, how can we both move fast
and be responsible as you mentioned in your talk?
Looking at your heart,
and if fundamentally what you're building,
if you don't think it'll make people
writ large better off, don't do it, right?
I know it sounds simple,
but that's actually really hard to do in the moment.
But AI fun, we've killed multiple projects,
not on financial grounds, but on ethical grounds,
where they're multiple projects.
We looked at the economic cases, very solid,
but we said, you know what,
we don't want this to exist in the world
and we just killed them on that basis.
So I hope more people will do that.
And then I worry about bringing everyone with us.
One interesting thing I'm seeing is
people in all sorts of drug roles
that are not engineering are much more productive
if they know AI than if they don't.
And so for example, on my marketing team,
my marketers, they know how to code.
Frankly, they were running circles
around the ones that don't.
So then everyone learned to code,
and then they got better.
But I feel like trying to bring everyone with us,
to make sure everyone is empowered to build with AI,
that'll be an important part of what all of us do, I think.
I'm one of your big fans,
and thank you for your online courses.
Your courses make the deep learning
like much more accessible to the world.
And my question is also about education.
As AI becomes more powerful and widespread,
there's seem to be a growing gap between
what can it actually do and what people perceive it.
So what do you think about like,
is it important to educate the general public
about deep learning stuff
and not only like educate those technical people
and make people understand more
what really what AI really do and how it works?
I think the knowledge will diffuse.
Even in AI, we want to empower everyone to build with AI.
So we're working on it, many of us are working on it.
I'll just tell you what I think is the main date.
I think there are maybe two dangers.
One is if you don't bring people with us fast enough,
I hope we'll solve that.
There's one other danger, which is,
it turns out that if you look at the mobile ecosystem,
mobile phones, it's actually not that interesting.
And one of the reasons is there are two gatekeepers
and Jordan and iOS,
and then let's say that you do certain things,
you're not allowed to try certain things on mobile.
And I think this, you know, campus innovators,
these dangers of AI have been used by certain businesses
that are trying to shut down open source
because there are a number of businesses
that would love to be a gatekeeper
to large scale foundation models.
So I think piping up dangers,
suppose it falls dangers of AI,
in order to get regulators to pass laws
like the proposal SB 1047 in California,
which thank goodness we shut down
where the put in place really burdens
some regulatory requirements
that don't make anyone safer
but would make it really difficult
for tears to release open source
and open weight software.
So one of the dangers to inequality as well
is if these regulatory, you know,
awful regulatory approaches,
and I've been in the room, right?
Some of these businesses set stuff to regulators
that was just not true.
So I think that some of these arguments,
the danger is if these regulatory proposals succeed
and end up siphoning regulations,
leaving us with a small number of gatekeepers
where everyone needs the permission
of a small number of companies
to find to you the model
from a certain way,
that's always siphoning innovation
and prevent the diffusion of this information
to let lots of startups, you know,
build whatever they want responsibly
but they're the freedom to innovate.
So I think so long as we prevent this line of attack
on open source open weight models from succeeding
and would make good progress with a fitness still there,
then I think eventually we get to the diffusion of knowledge
and we can hopefully then bring everyone with us
with this fight to protect open source
or we've been winning, but the fight is still on
and we still have to keep up that work to protect open source.
Thank you all very much,
as we want to proceed my will.
Thank you.

---

*Transcribed using OpenAI Whisper (base model)*
